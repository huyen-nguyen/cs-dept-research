"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Energy-Efficient Federated Learning in Internet of Drones Networks","J. Yao; X. Sun","Department of Computer Science, Texas Tech University, Lubbock, TX, USA; Department of Electrical and Computer Engineering, University of New Mexico, Albuquerque, NM, USA","2023 IEEE 24th International Conference on High Performance Switching and Routing (HPSR)","14 Jun 2023","2023","","","185","190","Internet of drones (IoD), where drones act as the Internet of things (IoT) devices, makes IoT networks much more flexible and responsive because of high mobility of drones. Machine learning (ML) techniques can be applied in IoD to facilitate multiple applications such as object tracking and traffic surveillance, where ML data samples are collected and analyzed in the edge servers at the ground base station (BS). However, aggregating all data samples incurs huge wireless network traffic and potential data privacy leakage. Federated learning (FL) is then proposed to address these challenges by performing local training in drones and aggregating model parameters at the BS without sharing raw data samples. The FL performance in IoD networks is greatly affected by limited drone batteries which power FL local training, wireless data transmission, and drones’ movements. This paper hence investigates the energy-efficient FL in IoD networks to optimize CPU frequencies of drones’ on-board computing units such that total energy consumption of all the drones in the FL process can be minimized, while satisfying the FL training time requirement. We formulate the problem as a non-linear programming problem and then design an algorithm with polynomial time complexity to derive the optimum solution. Extensive simulations are conducted to demonstrate the performance of our proposed algorithm.","2325-5609","978-1-6654-7640-9","10.1109/HPSR57248.2023.10147956","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10147956","Internet of Drones (IoD);federated learning;energy consumption;CPU control","Training;Time-frequency analysis;Energy consumption;Federated learning;Wireless networks;Programming;Energy efficiency","","","","","","19","IEEE","14 Jun 2023","","","IEEE","IEEE Conferences"
"Split Learning for Image Classification in Internet of Drones Networks","J. Yao","Department of Computer Science, Texas Tech University, Lubbock, TX, USA","2023 IEEE 24th International Conference on High Performance Switching and Routing (HPSR)","14 Jun 2023","2023","","","52","55","Internet of drones (IoD), where drones act as the Internet of things (IoT) devices, has attracted much attention for its application in traffic surveillance, object tracking, and disaster rescue. These applications rely heavily on machine learning (ML) techniques for image classification. The traditional method of training ML models in IoD networks involves sending all data to a centralized ground base station (BS), which can result in privacy and security concerns. Split learning, an approach that separates the training model into a client model and a server model, can mitigate these concerns by allowing the client model to reside in the drones while the server model is in the BS, preserving data privacy without sharing raw data. In this study, we explore the application of split learning in IoD networks and conduct simulations to evaluate our designed algorithm. Simulation results demonstrate that different separations do not significantly impact split learning accuracy, increasing the number of layers on clients can lead to longer training times, communication overhead is a significant bottleneck in split learning for IoD networks, client numbers do not significantly affect accuracy, and training time slightly increases with an increasing number of clients.","2325-5609","978-1-6654-7640-9","10.1109/HPSR57248.2023.10147979","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10147979","Internet of Drones (IoD);split learning;image classification","Training;Data privacy;Switches;Traffic control;Data models;Servers;Internet of Things","","","","","","12","IEEE","14 Jun 2023","","","IEEE","IEEE Conferences"
"Fabrication and Characterization of Flexible Solid-State MIM Supercapacitor with Inkjet-Printing of Stacked Ag NP and Polymer Dielectric Layers","M. R. Momota; T. Ferdous; T. Fujiwara; B. I. Morshed","Department of Computer Science, Texas Tech University, Lubbock, TX, USA; Department of Chemistry, The University of Memphis, Memphis, TN, USA; Department of Chemistry, The University of Memphis, Memphis, TN, USA; Department of Computer Science, Texas Tech University, Lubbock, TX, USA","2023 IEEE 16th Dallas Circuits and Systems Conference (DCAS)","25 May 2023","2023","","","1","5","Electrification of vehicles and energy-storage needs of renewables and robotic devices including wearables, implantables, drones, and wireless sensors demands energy storage solution that is not achievable with current battery technology. In this paper, we present the proof-of-concept results of inkjet-printed (IJP) flexible supercapacitors to store energy and power devices for wearables and implantable, as well as other energy storage needs. We have developed a patent-pending technique of stacked Metal-Insulator-Metal (MIM) parallel plate capacitors to form supercapacitor with high energy density. The flexible supercapacitor was fabricated on a thin 25 µm flexible polyimide (PI) film. We have used silver (Ag) nanoparticle (NP) ink for printing the conductive layer and a polymer Poly(4-vinylphenol) ink for printing dielectric layer to fabricate insulation between metal plates. The area of the supercapacitors was 100 mm2. Six coatings of PVP dielectric were printed for proper insulation between two successive metal layers. The measured capacitance of a single MIM capacitor was 996 pF. The measured capacitance of two-stacked MIM capacitors on the same footprint was 1.98 nF. The thickness of two stacked MIM capacitor was 22.3 µm. By stacking more MIM capacitors on the same footprint, supercapacitors can be fabricated that can achieve high energy storage capability within a small footprint, weight, and volume. This IJP supercapacitor, which is flexible yet solid-state with high-performance and safe for long-term use, can play a significant role in wearables, implantable, drones, renewable energy storage, and electric vehicles.","","979-8-3503-9918-9","10.1109/DCAS57389.2023.10130228","National Science Foundation(grant numbers:2105766); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10130228","Energy storage;Inkjet-printing;Solid-state supercapacitor;Stacked MIM capacitor","Printing;Silver;Insulation;Renewable energy sources;Wearable computers;Supercapacitors;Capacitance","flexible electronics;ink jet printing;MIM devices;nanoparticles;polymer films;supercapacitors","Ag/int;capacitance 1.98 nF;capacitance 996.0 pF;conductive layer;current battery technology;dielectric layer;energy storage;flexible polyimide film;flexible solid-state MIM supercapacitor;IJP flexible supercapacitors;inkjet-printed flexible supercapacitors;inkjet-printing;patent-pending technique;polymer dielectric layers;polymer poly(4-vinylphenol) ink;power devices;renewable energy storage;robotic devices;silver nanoparticle ink;size 22.3 mum;size 25.0 mum;stacked metal-insulator-metal parallel plate capacitors;wireless sensors demands energy storage solution","","","","24","IEEE","25 May 2023","","","IEEE","IEEE Conferences"
"Design and Packaging of a Custom Single-lead Electrocardiogram (ECG) Sensor Embedded with Wireless Transmission","M. Rahman; R. Hewitt; B. I. Morshed","Department of Computer Science, Texas Tech University, Lubbock, TX, USA; Department of Engineering Technology, University of Memphis, Memphis, TN, USA; Department of Computer Science, Texas Tech University, Lubbock, TX, USA","2023 IEEE 16th Dallas Circuits and Systems Conference (DCAS)","25 May 2023","2023","","","1","4","An embedded device is developed to acquire an electrocardiogram (ECG) signal by using only a single lead. The device is capable to capture the ECG signal of a human from the wrist or heart region and transmit data through Bluetooth low energy (BLE). The device includes a custom printed circuit board (PCB) for ECG acquisition. The custom PCB implements the AD 8232 chip for ECG calculation. The PCB is embedded with a commercial nRF52840 development board. Analog data from custom PCB is converted to the digital domain by using a 10-bit analog to digital converter (ADC) and transmitted through Bluetooth 5.3 both supplied by the development board. nRF52840 is 32 bit ARM® Cortex™ processing unit. The BLE data is transmitted by a baud rate of 115.2 kbps. The PCB is 2-layer (32 mm X 50 mm X 4 mm). The device is powered by a 350 mA-hr LiPo battery and consumes approximately 0.58 mA of current. With this current consumption, the battery would last around 603 hours. Finally, all parts are packaged in a box to make it compact and functional for body-worn uses. The device is a low-cost, low-weight design suitable for mobile health (mHealth) applications.","","979-8-3503-9918-9","10.1109/DCAS57389.2023.10130229","National Science Foundation(grant numbers:2105766); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10130229","electrocardiogram;printed circuit board;body-worn;mobile health","Wrist;Wireless communication;Wireless sensor networks;Tracking;Printed circuits;Electrocardiography;Packaging","","","","","","8","IEEE","25 May 2023","","","IEEE","IEEE Conferences"
"STUNNER: Radar Echo Extrapolation Model Based on Spatiotemporal Fusion Neural Network","W. Fang; L. Pang; V. S. Sheng; Q. Wang","School of Computer and Software, Engineering Research Center of Digital Forensics, Ministry of Education, Nanjing University of Information Science and Technology, Nanjing, China; School of Computer and Software, Engineering Research Center of Digital Forensics, Ministry of Education, Nanjing University of Information Science and Technology, Nanjing, China; Department of Computer Science, Texas Tech University, Lubbock, TX, USA; Meteorological Cadre Training Institute, China Meteorological Administration, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","5 May 2023","2023","61","","1","14","Radar echo extrapolation based on deep learning is an important method for conducting precipitation nowcasting. Radar echo sequence data have spatiotemporal correlations and nonrigid movements of the radar echo. According to the characteristics of radar data, this study proposes a new spatiotemporal fusion neural network called STUNNER. STUNNER implements a two-stream spatiotemporal fusion strategy to extract and fuse spatial and temporal signals. Specifically, it uses a novel cross-network embedding method to achieve efficient spatiotemporal fusion; the fusion integrates a temporal differencing network (TDN) and a spatiotemporal trajectory network (STTN). The TDN models the high-order nonstationarity of the radar sequence to learn the motion trend. The STTN optimizes the convolution operator in a spatiotemporal long short-term memory model to extract the transient variations from the radar images. We compare STUNNER with the other five models on two public datasets. On the radar echo extrapolation task, STUNNER achieves the best performance.","1558-0644","","10.1109/TGRS.2023.3268187","National Natural Science Foundation of China(grant numbers:42075007,41975100); Bei Ji Ge Open Project of Nanjing Institute of Meteorological Science and Technology Innovation(grant numbers:BJG202306); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10103916","Dynamic convolution;high-order nonstationarity;precipitation nowcasting;radar echo extrapolation;two-stream spatiotemporal fusion","Radar;Radar imaging;Market research;Extrapolation;Convolution;Transient analysis;Spaceborne radar","convolutional neural nets;deep learning (artificial intelligence);extrapolation;feature extraction;image fusion;learning (artificial intelligence);neural nets;radar imaging;radar signal processing;recurrent neural nets;spatiotemporal phenomena","efficient spatiotemporal fusion;novel cross-network embedding method;radar data;radar echo extrapolation model;radar echo extrapolation task;radar images;radar sequence;sequence data;spatiotemporal correlations;spatiotemporal fusion neural network;spatiotemporal trajectory network;STUNNER;temporal differencing network;two-stream spatiotemporal fusion strategy","","","","31","IEEE","18 Apr 2023","","","IEEE","IEEE Journals"
"A Survey on BERT and Its Applications","S. Aftan; H. Shah","Department of Computer Science, Texas Tech University, Lubbock, TX, USA; Department and College of Computer Science, King Khalid University, Abha, Saudi Arabia","2023 20th Learning and Technology Conference (L&T)","11 Apr 2023","2023","","","161","166","A recently developed language representation model named Bidirectional Encoder Representation from Transformers (BERT) is based on an advanced trained deep learning approach that has achieved excellent results in many complex tasks, the same as classification, Natural Language Processing (NLP), prediction, etc. This survey paper mainly adopts the summary of BERT, its multiple types, and its latest developments and applications in various computer science and engineering fields. Furthermore, it puts forward BERT's problems and attractive future research trends in a different area with multiple datasets. From the findings, overall, the BERT and their recent types have achieved more accurate, fast, and optimal results in solving most complex problems than typical Machine and Deep Learning methods.","","979-8-3503-0030-7","10.1109/LT58159.2023.10092289","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10092289","BERT;Machine Learning;Natural Language Processing model;bidirectional encoder","Deep learning;Computer science;Text mining;Text analysis;Bit error rate;Predictive models;Transformers","deep learning (artificial intelligence);learning (artificial intelligence);natural language processing;pattern classification;sentiment analysis","attractive future research trends;Bidirectional Encoder Representation;computer science;forward BERT's problems;Natural Language Processing;recent types;recently developed language representation model;solving most complex problems","","","","36","IEEE","11 Apr 2023","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning Assisted Client Selection in Non-orthogonal Multiple Access based Federated Learning","R. Albelaihi; A. Alasandagutti; L. Yu; J. Yao; X. Sun","University of New Mexico, Albuquerque, NM, USA; University of New Mexico, Albuquerque, NM, USA; University of New Mexico, Albuquerque, NM, USA; Department of Computer Science, Texas Tech University, Lubbock, TX, USA; University of New Mexico, Albuquerque, NM, USA","IEEE Internet of Things Journal","","2023","PP","99","1","1","To reap the benefit of big data generated by the massive number of Internet of Things (IoT) devices while preserving data privacy, federated learning (FL) has been proposed to enable IoT devices to train machine learning models locally. That is, instead of sharing the local data sets, different clients in terms of IoT devices only need to upload their local models to a centralized FL server. Client selection in FL is critical to maximize the number of qualified clients, who can successfully upload their local models to the FL server before the predefined deadline. Normally, client selection is coupled with wireless resource management owing to the fact that different clients need to share the same spectrum to upload their local models. The existing solutions of joint optimizing client selection and resource management are designed based on frequency division multiple access (FDMA) or time division multiple access (TDMA), which do not consider the dynamics of the clients and lead to low bandwidth utilization. In this paper, we propose the Non-Orthogonal Multiple Access (NOMA) based resource allocation for client selection in FL to dynamically and jointly optimize client selection for each global iteration as well as the transmission power of each selected client in each time slot within a global iteration. We design the Deep Reinforcement lEarning based client selection in nonorthogonAl Multiple access based Federated Learning (DREAMFL) algorithm to solve the problem. Extensive simulations are conducted to demonstrate that DREAM-FL can select more qualified clients and has higher model accuracy than FDMA and TDMA-based solutions.","2327-4662","","10.1109/JIOT.2023.3264463","National Science Foundation(grant numbers:CNS-2148178); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10092890","Federated Learning;client selection;deep reinforcement learning;NOMA","Resource management;Bandwidth;Wireless communication;Data models;Internet of Things;Training;Servers","","","","","","","IEEE","5 Apr 2023","","","IEEE","IEEE Early Access Articles"
"Load Optimization Scheduling of Chip Mounter Based on Hybrid Adaptive Optimization Algorithm","X. Yan; H. Zuo; C. Hu; W. Gong; V. S. Sheng","School of Computer Science, China University of Geosciences, Wuhan, China; School of Computer Science, China University of Geosciences, Wuhan, China; School of Computer Science, China University of Geosciences, Wuhan, China; School of Computer Science, China University of Geosciences, Wuhan, China; Department of Computer Science, Texas Tech University, Lubbock, TX, USA","Complex System Modeling and Simulation","9 Mar 2023","2023","3","1","1","11","A chip mounter is the core equipment in the production line of the surface-mount technology, which is responsible for finishing the mount operation. It is the most complex and time-consuming stage in the production process. Therefore, it is of great significance to optimize the load balance and mounting efficiency of the chip mounter and improve the mounting efficiency of the production line. In this study, according to the specific type of chip mounter in the actual production line of a company, a maximum and minimum model is established to minimize the maximum cycle time of the chip mounter in the production line. The production efficiency of the production line can be improved by optimizing the workload scheduling of each chip mounter. On this basis, a hybrid adaptive optimization algorithm is proposed to solve the load scheduling problem of the mounter. The hybrid algorithm is a hybrid of an adaptive genetic algorithm and the improved ant colony algorithm. It combines the advantages of the two algorithms and improves their global search ability and convergence speed. The experimental results show that the proposed hybrid optimization algorithm has a good optimization effect and convergence in the load scheduling problem of chip mounters.","2096-9929","","10.23919/CSMS.2022.0026","National Natural Science Foundation of China(grant numbers:U1911205,62073300,62076225); National Key Research and Development Program of China(grant numbers:2021YFB3301602); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10065396","Surface Mount Technology (SMT);chip mounter;load optimization scheduling;adaptive genetic algorithm;ant colony algorithm","Adaptation models;Surface mount technology;Production;Scheduling;Hybrid power systems;Task analysis;Optimization","ant colony optimisation;assembling;genetic algorithms;resource allocation;scheduling;search problems;surface mount technology","adaptive genetic algorithm;chip mounter;convergence speed;global search ability;hybrid adaptive optimization algorithm;improved ant colony algorithm;load balance optimization;load optimization scheduling;load scheduling problem;production efficiency;production line;production process;surface-mount technology;workload scheduling","","","","40","","9 Mar 2023","","","TUP","TUP Journals"
"Fabrication and Characterization of Inkjet Printed Flexible Dry ECG Electrodes","M. M. R. Momota; B. I. Morshed; T. Ferdous; T. Fujiwara","Department of Computer Science, Texas Tech University, Lubbock, TX, USA; Department of Computer Science, Texas Tech University, Lubbock, TX, USA; Department of Chemistry, University of Memphis, Memphis, TN, USA; Department of Chemistry, University of Memphis, Memphis, TN, USA","IEEE Sensors Journal","31 Mar 2023","2023","23","7","7917","7928","Commercial gel ECG electrodes have a limited lifetime of operation and metal dry electrodes are rigid and noisy. In this study, we developed flexible dry ECG electrodes in different shapes and sizes using an inkjet printing (IJP) technology. As an additive manufacturing technique, IJP provides the flexibility to personalize the shape and size of electrodes with rapid prototyping. IJP ECG electrodes were fabricated on flexible polyimide substrates using silver nanoparticle inks for the conductive layer and a custom polymer ink of poly(4-vinylphenol) (PVP) for the insulator layer. The maximum and minimum diameters of the electrodes were 21 and 9 mm, respectively. The thickness of IJP electrodes was 2 ± 0.5  $\mu \text{m}$ . Various tests were performed for feasibility, including changing electrode position, and subject’s activity such as resting, standing, and walking using IJP ECG electrodes simultaneous with clinical standard gel electrodes, which provides an accurate comparison. Compared to the gel electrode, the circular shape IJP electrode was more promising than rectangular and square shapes as it showed more coherence to the gel electrode. IJP electrodes with different diameters were compared along with the gel electrode and the result demonstrated that the IJP electrode with a diameter of 21, 19, and 16 mm shows almost similar performance as the gel electrode. Considering ECG signal quality, IJP electrode with a diameter of 19 mm was the optimal size. These IJP dry electrodes can be usable in ECG wearable as it is flexible, reusable, and can be used for a long time.","1558-1748","","10.1109/JSEN.2023.3250103","National Science Foundation (NSF)(grant numbers:CNS-1932281); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10064144","Dry electrodes;ECG monitoring;flexible;inkjet printing (IJP);silver nanoparticle ink;wearable sensors","Electrodes;Electrocardiography;Ink;Silver;Printing;Monitoring;Shape","biomedical electrodes;electrocardiography;ink jet printing;medical signal processing;nanoparticles;polymer films;rapid prototyping (industrial);silver;three-dimensional printing","additive manufacturing technique;circular shape IJP electrode;clinical standard gel electrodes;commercial gel ECG electrodes;conductive layer;ECG signal quality;electrode position;flexible polyimide substrates;IJP ECG electrodes;inkjet printed flexible dry ECG electrodes;inkjet printing technology;insulator layer;metal dry electrodes;poly(4-vinylphenol);polymer ink;rapid prototyping;resting;silver nanoparticle inks;size 16.0 mm;size 19.0 mm;size 21.0 mm;size 9.0 mm;standing;walking","","","","43","IEEE","8 Mar 2023","","","IEEE","IEEE Journals"
"Feature-Level Deeper Self-Attention Network With Contrastive Learning for Sequential Recommendation","Y. Hao; T. Zhang; P. Zhao; Y. Liu; V. S. Sheng; J. Xu; G. Liu; X. Zhou","Institute of Artificial Intelligence, School of Computer Science and Technology, Soochow University, Suzhou, China; Huawei Technologies Co., Ltd, Nanjing, China; Institute of Artificial Intelligence, School of Computer Science and Technology, Soochow University, Suzhou, China; Rutgers University, New Brunswick, NJ, USA; Department of Computer Science, Texas Tech University, Lubbock, TX, USA; Institute of Artificial Intelligence, School of Computer Science and Technology, Soochow University, Suzhou, China; Macquarie University, Sydney, Australia; Hong Kong University of Science and Technology, Hong kong, China","IEEE Transactions on Knowledge and Data Engineering","","2023","PP","99","1","13","Sequential recommendation, which aims to recommend next item that the user will likely interact in a near future, has become essential in various Internet applications. Existing methods usually consider the transition patterns between items, but ignore the transition patterns between features of items. We argue that only the item-level sequences cannot reveal the full sequential patterns, while explicit and implicit feature-level sequences can help extract the full sequential patterns. Meanwhile, the item-level sequential recommendation also suffers from limited supervised signal issues. In this paper, we propose a novel model Feature-level Deeper Self-Attention Network with Contrastive Learning (FDSA-CL) for sequential recommendation. Specifically, FDSA-CL first integrates various heterogeneous features of items into feature-level sequences with different weights through a vanilla attention mechanism. After that, FDSA-CL applies separated self-attention blocks on item-level sequences and feature-level sequences, respectively, to model item transition patterns and feature transition patterns. Moreover, we propose contrastive learning and item feature recommendation tasks to capture the embedding commonality and further utilize the beneficial interaction among the two levels, so as to alleviate the sparsity of the supervised signal and extract the most critical information. Finally, we jointly optimize the above tasks. We evaluate the proposed model using two real-world datasets and experimental results show that our model significantly outperforms the state-of-the-art approaches.","1558-2191","","10.1109/TKDE.2023.3250463","NSFC(grant numbers:61876117,62176175); Universities of Jiangsu Province(grant numbers:21KJA520004); Suzhou Science and Technology Development Program(grant numbers:SYC2022139); Postgraduate Research and Practice Innovation Program of Jiangsu Province(grant numbers:KYCX-223198); Priority Academic Program Development of Jiangsu Higher Education Institutions; Exploratory Self-selected Project of the State Key Laboratory of Software Development Environment; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10059216","Contrastive learning;feature-level patterns;item-level patterns;self-attention network;sequential recommendation","Task analysis;Behavioral sciences;Feature extraction;Markov processes;Data mining;Long short term memory;Self-supervised learning","","","","","","","IEEE","6 Mar 2023","","","IEEE","IEEE Early Access Articles"
"Modality Specific CBAM-VGGNet Model for the Classification of Breast Histopathology Images via Transfer Learning","A. Ijaz; B. Raza; I. Kiran; A. Waheed; A. Raza; H. Shah; S. Aftan","Department of Computer Science, COMSATS University Islamabad (CUI), Islamabad, Pakistan; Department of Computer Science, COMSATS University Islamabad (CUI), Islamabad, Pakistan; Department of Computer Science, COMSATS University Islamabad (CUI), Islamabad, Pakistan; Department of Computer Science, Women University Swabi, Swabi, Pakistan; Department of Physics, COMSATS University Islamabad (CUI), Islamabad, Pakistan; Department of Computer Science, College of Computer Science, King Khalid University, Abha, Saudi Arabia; Department of Computer Science, Texas Tech University, Lubbock, TX, USA","IEEE Access","22 Feb 2023","2023","11","","15750","15762","Histopathology images are very distinctive, one image may contain thousands of objects. Transferring features from natural images to histopathology images may not provide impressive outcomes. In this study, we have proposed a novel modality specific CBAM-VGGNet model for classifying H and E stained breast histopathology images. Instead of using pre-trained models on ImageNet, we have trained VGG16 and VGG19 models on the same domain cancerous histopathology datasets, which are then used as fixed feature extractors. We have added the GAP layer and Convolutional block attention module (CBAM) after the first convolutional layer of convolutional blocks. CBAM is an effective module for neural networks to focus on relevant features. We have implemented the VGG16 and VGG19 in a novel way following the configuration of state-of-the-art models with our own concatenated layers. The addition of the GAP layer in VGGNet has reduced the number of parameters, requiring less computational power. Both models are ensembled using the averaging ensemble technique. Features are extracted from the final ensembled model and then passed to the feed-forward neural network. A hybrid pre-processing technique is proposed that first uses a median filter and then contrasts limited adaptive histogram equalization (CLAHE). The median filter removes the highly significant noise and is directly related to image quality. CLAHE improves the local contrast present in an image and boosts the weak boundary edges in each image pixel. The proposed CBAM ensemble model has outperformed state-of-the-art models with an accuracy of 98.96% and 97.95% F1-score on 400X data of the BreakHis dataset.","2169-3536","","10.1109/ACCESS.2023.3245023","Deanship of Scientific Research at King Khalid University through Large Groups(grant numbers:RGP.2/212/1443); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10044107","Breast cancer;CBAM;CLAHE;classification;deep learning;histopathology;transfer learning","Histopathology;Feature extraction;Transfer learning;Breast cancer;Convolutional neural networks;Computational modeling;Deep learning","cancer;convolutional neural nets;feature extraction;image classification;image enhancement;learning (artificial intelligence);median filters;medical image processing","averaging ensemble technique;boundary edges;BreakHis dataset;CBAM ensemble model;CLAHE;concatenated layers;contrasts limited adaptive histogram equalization;Convolutional block attention module;convolutional blocks;convolutional layer;domain cancerous histopathology datasets;E stained breast histopathology images;F1-score;feed-forward neural network;final ensembled model;fixed feature extractors;GAP layer;H stained breast histopathology images;hybrid pre-processing technique;image pixel;image quality;ImageNet;less computational power;local contrast improvement;median filter;modality specific CBAM-VGGNet model;natural images;pre-trained models;same domain cancerous histopathology datasets;transfer learning;VGG16 models;VGG19 models","","","","64","CCBY","14 Feb 2023","","","IEEE","IEEE Journals"
"Data Distribution for Heterogeneous Storage Systems","J. Zhou; Y. Chen; M. Zheng; W. Wang","Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Department of Computer Science, Texas Tech University, Lubbock, TX, USA; Department of Electrical and Computer Engineering, Iowa State University, Ames, IA, USA; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Computers","9 May 2023","2023","72","6","1747","1762","The exponential growth of data in many science and engineering domains poses significant challenges to storage systems. Data distribution is a critical component in large-scale distributed storage systems and plays a vital role in placing petabytes of data and beyond, among tens to hundreds of thousands of storage devices. Meantime, heterogeneous storage systems, such as those having devices with hard disk drives (HDDs) and storage class memories (SCMs), have become increasingly popular for massive data storage due to their distinct and complement characteristics. This paper presents a new data distribution algorithm called SUORA (Scalable and Uniform storage via Optimally-adaptive and Random number Addressing) specifically for heterogeneous devices to maximize the benefits of them. SUORA provides a fully symmetric, highly efficient methodology to distribute data across a hybrid and tiered storage cluster. It divides heterogeneous devices into different buckets and segments, and adopts pseudo-random functions to map data onto them with the balanced consideration of capacity, performance and life-time. By analyzing hotness and access patterns, SUORA gradually moves hot data from HDDs to SCMs to optimize the throughput, and moves cold data reversely for load balance. It combines data replication with migration to significantly reduce movement overhead while making data placement more adaptive to different workloads. Extensive evaluations on simulation and Sheepdog storage system show that, with considering distinct characteristics of various devices thoroughly, SUORA improves the overall performance efficiency of heterogeneous storage systems.","1557-9956","","10.1109/TC.2022.3223302","Strategic Priority Research Program of Chinese Academy of Sciences(grant numbers:XDC02010900); National Science Foundation(grant numbers:CCF-1718336,OAC-1835892,CNS-1817094); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9954893","Parallel/distributed file systems;data distribution;data placement;heterogeneous storage;data replication","Performance evaluation;Clustering algorithms;Nonvolatile memory;Costs;Throughput;Hash functions;Servers","disc drives;distributed algorithms;hard discs;resource allocation;storage management","cold data;data distribution algorithm;data placement;data replication;heterogeneous devices;heterogeneous storage systems;hot data;large-scale distributed storage systems;map data;massive data storage;storage class memories;storage cluster;storage devices;SUORA","","","","38","IEEE","18 Nov 2022","","","IEEE","IEEE Journals"
"QoS-Aware Machine Learning Task Offloading and Power Control in Internet of Drones","J. Yao; N. Ansari","Department of Computer Science, Texas Tech University, Lubbock, TX, USA; Helen and John C. Hartmann Department of Electrical and Computer Engineering, Advanced Networking Laboratory, New Jersey Institute of Technology, Newark, NJ, USA","IEEE Internet of Things Journal","23 Mar 2023","2023","10","7","6100","6110","Internet of Drones (IoD), where drones act as the Internet of Things (IoT) devices, provides multiple services, such as object recognition, traffic monitoring, and disaster rescue. The service response time is limited by drones’ onboard computing power, hence greatly affecting the Quality of Service (QoS). Machine learning [(ML), e.g., image processing] task offloading to the fog node attached to the ground base station can help reduce workload from drones and hence decrease service time. The communication latency between drones and the fog node during task offloading also affects the service time and hence the communication efficiency needs to be considered. Therefore, in this work, we consider the joint optimization of ML task offloading and power control in IoD to determine each drone’s number of offloaded images and wireless transmission power. We analyze the energy model of the commonly used convolutional neural network (CNN) for object recognition, and formulate the joint optimization problem as a mixed-integer nonlinear programming (MINLP) problem to minimize drones’ average service time constrained by drones’ energy budgets. An approximation algorithm with low computational complexity is then designed to address the problem and its performances are compared with the lower bounds and demonstrated via extensive simulations.","2327-4662","","10.1109/JIOT.2022.3222968","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9953950","Computation offloading;convolutional neural network (CNN);fog computing;Internet of Drones (IoD);Internet of Things (IoT);machine learning (ML);power control;Quality of Service (QoS)","Drones;Task analysis;Wireless communication;Power control;Object recognition;Internet of Things;Quality of service","computational complexity;convolutional neural nets;integer programming;Internet of Things;learning (artificial intelligence);nonlinear programming;object recognition;optimisation;power control;quality of service;telecommunication power management","drone;drones;fog node;ML task offloading;object recognition;offloaded images;power control;QoS-aware machine;service response time;service time;wireless transmission power","","","","46","IEEE","16 Nov 2022","","","IEEE","IEEE Journals"
"Short Text Topic Learning Using Heterogeneous Information Network","Q. Wang; C. Zhu; Y. Zhang; H. Zhong; J. Zhong; V. S. Sheng","School of Computer Science and Technology, Anhui University, Hefei, Anhui, China; School of Computer Science and Technology, Anhui University, Hefei, Anhui, China; School of Computer Science and Technology, Anhui University, Hefei, Anhui, China; School of Computer Science and Technology, Anhui University, Hefei, Anhui, China; School of Internet, Anhui University, Hefei, Anhui, China; Department of Computer Science, Texas Tech University, Lubbock, TX, USA","IEEE Transactions on Knowledge and Data Engineering","3 Apr 2023","2023","35","5","5269","5281","With the explosive growth of short texts on users’ interests and preferences, learning discriminative and coherent latent topics from short texts is a critical and significative work, since many practical applications, such as e-commerce and recommendations, require semantic understandings that short texts convey explicitly and implicitly. However, existing short text topic learning methods face the challenge of fully capturing semantically related co-occurrence phrases. Therefore, this paper proposes a novel Heterogeneous Information Network-based Short Text Topic learning approach (HIN-ShoTT) in terms of parts of speech, without depending on any auxiliary information. Specifically, HIN-ShoTT can be decomposed into three phases: ${{i}}$i) seeking semantic relations among words with different parts of speech, where HIN-ShoTT models multiple explicit and implicit semantic relations among words based on a Heterogeneous Information Network (HIN) in terms of parts of speech; ${{ii}}$ii) extracting co-occurrence phrases and filtering noises, where HIN-ShoTT defines parts-of-speech meta structures to guide co-occurrence phrase extraction and a self-adapting threshold filtering module is proposed for discarding noises; and ${{iii}}$iii) inferring topics, where HIN-ShoTT directly models the generative process of co-occurrence phrases to make topic learning effective with the abundant corpus-level information. Our experimental results on three real-world datasets not only show that HIN-ShoTT performs well, but also demonstrate that it is feasible to incorporate HIN into short text topic learning for accuracy improvement.","1558-2191","","10.1109/TKDE.2022.3147766","National Key Research and Development Plan of China(grant numbers:2019YFB1704101); National Natural Science Foundation of China(grant numbers:61872002,62006003,U1936220); Natural Science Foundation of Anhui Province of China(grant numbers:2008085QF307); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9699033","Short texts;topic learning;heterogeneous information network;parts of speech;meta structure;natural language processing","Semantics;Periodic structures;Optical wavelength conversion;Grammar;Peer-to-peer computing;Speech processing;Electronic mail","information networks;learning (artificial intelligence);natural language processing;sentiment analysis;text analysis","co-occurrence phrase extraction;coherent latent topics;discriminative topics;Heterogeneous Information Network;HIN-ShoTT directly models;implicit semantic relations;novelHeterogeneousInformationNetwork-basedShortTextTopic learning approach;parts-of-speech meta structures;semantic understandings;semantically related co-occurrence phrases;short text topic learning methods;short texts","","2","","48","IEEE","1 Feb 2022","","","IEEE","IEEE Journals"
"A Graph Convolution Neural Network Based Method for Insider Threat Detection","K. Fei; J. Zhou; L. Su; W. Wang; Y. Chen; F. Zhang","School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Department of Computer Science, Texas Tech University, Lubbock, USA; Information Technology Center China Mobile, Beijing, China","2022 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Big Data & Cloud Computing, Sustainable Computing & Communications, Social Computing & Networking (ISPA/BDCloud/SocialCom/SustainCom)","23 Mar 2023","2022","","","66","73","In this research, we propose Log2Graph, a new insider threat detection method based on graph convolution neural network (GCN). This method first retrieves the corresponding logs and features from log files through feature extraction. Specifically, we use an auxiliary feature of anomaly index to describe relationship between entities, such as users and hosts, instead of establish complex connections between them. Second, these logs and features are augmented through a combination of oversampling and downsampling, to prepare for the next-stage supervised learning process. Third, we use three elaborated rules to construct the graph of each user by connecting the logs according to chronological and logical relationship. At last, the graph convolution neural network constructed is used to detect insider threats. Our validation and evaluation results confirm that Log2Graph can greatly improve the performance of detecting insider threats compared against baseline and existing methods.","","978-1-6654-6497-0","10.1109/ISPA-BDCloud-SocialCom-SustainCom57177.2022.00016","National Science Foundation(grant numbers:OAC-1835892,CNS-1817094,CNS-1939140); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10070663","Insider threat detection;cluster security;graph construction;graph convolution neural network","Knowledge engineering;Convolution;Neural networks;Supervised learning;Feature extraction;Time measurement;Data models","computer network security;convolutional neural nets;feature extraction;graph theory;learning (artificial intelligence);neural nets;supervised learning","auxiliary feature;corresponding logs;feature extraction;graph convolution neural network;insider threat detection method;insider threats;log files;Log2Graph","","","","37","IEEE","23 Mar 2023","","","IEEE","IEEE Conferences"
"Can We Predict Consequences of Cyber Attacks?","P. Datta; A. S. Namin; K. S. Jones","Department of Computing Sciences, Coastal Carolina University, USA; Department of Computer Science, Texas Tech University, USA; Department of Psychological Sciences, Texas Tech University, USA","2022 21st IEEE International Conference on Machine Learning and Applications (ICMLA)","23 Mar 2023","2022","","","1047","1054","Threat modeling is a process by which security designers and researchers analyze the security of a system against known threats and vulnerabilities. There is a myriad of threat intelligence and vulnerability databases that security experts use to make important day-to-day decisions. Security experts and incident responders require the right set of skills and tools to recognize attack consequences and convey them to various stakeholders. In this paper, we used natural language processing (NLP) and deep learning to analyze text descriptions of cyberattacks and predict their consequences. This can be useful to quickly analyze new attacks discovered in the wild, help security practitioners take requisite actions, and convey attack consequences to stakeholders in a simple way. In this work, we predicted the multilabels (availability, access control, confidentiality, integrity, and other) corresponding to each text description in MITRE’s CWE dataset. We compared the performance of various CNN and LSTM deep neural networks in predicting these labels. The results indicate that it is possible to predict multilabels using a LSTM deep neural network with multiple output layers equal to the number of labels. LSTM performance was better when compared to CNN models.","","978-1-6654-6283-9","10.1109/ICMLA55696.2022.00174","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10069006","natural language processing;cybersecurity;mul-tilabel classification;deep learning","Deep learning;Threat modeling;Access control;Databases;Neural networks;Natural language processing;Stakeholders","computer crime;computer network security;convolutional neural nets;deep learning (artificial intelligence);natural language processing;recurrent neural nets;security of data","attack consequences;CNN models;cyber attacks;day-to-day decisions;deep learning;incident responders;known threats;LSTM deep neural network;predict consequences;security designers;security experts;security practitioners;stakeholders;text description;threat intelligence;threat modeling;vulnerability databases","","","","29","IEEE","23 Mar 2023","","","IEEE","IEEE Conferences"
"JobViewer: Graph-based Visualization for Monitoring High-Performance Computing System","T. Dang; N. V. T. Nguyen; J. Li; A. Sill; J. Hass; Y. Chen","Department of Computer Science, Texas Tech University, Lubbock, Texas, USA; Department of Computer Science, Texas Tech University, Lubbock, Texas, USA; Department of Computer Science, Texas Tech University, Lubbock, Texas, USA; High Performance Computing Center, Texas Tech University, Lubbock, Texas, USA; Dell Inc, Austin, Texas, USA; Department of Computer Science, Texas Tech University, Lubbock, Texas, USA","2022 IEEE/ACM International Conference on Big Data Computing, Applications and Technologies (BDCAT)","13 Mar 2023","2022","","","110","119","Visualization aims to strengthen data exploration and analysis, especially for complex and high-dimensional data. High-performance computing (HPC) systems are typically large and complicated instruments that generate massive performance and operation time series. Monitoring HPC systems’ performance is a daunting task for HPC admins and researchers due to their dynamic natures. This work proposes a visual design using the bipartite graph’s idea to visualize HPC clusters’ structure, metrics, and job scheduling data. We built a web-based prototype, called JobViewer, that integrates advanced methods in visualization and human-computer interaction (HCI) to demonstrate the benefits of visualization in real-time monitoring HPC centers. We also showed real use cases and a user study to validate the efficiency and highlight the current approach’s drawbacks.","","978-1-6654-6090-3","10.1109/BDCAT56447.2022.00021","Texas Tech University; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10062112","High-Performance Computing Cente;Job Schedulin;Radar Chart;Time-Series Data Analysi;Multidimensional Data Visualizatio","Human computer interaction;Visualization;High performance computing;Source coding;Time series analysis;Data visualization;Bipartite graph","data analysis;data visualisation;graph theory;human computer interaction;parallel processing","bipartite graph;data analysis;data exploration;data visualization;HCI;high-dimensional data;high-performance computing system;HPC clusters;HPC systems;human-computer interaction;JobViewer;real-time monitoring HPC centers;visual design;Web-based prototype","","","","37","IEEE","13 Mar 2023","","","IEEE","IEEE Conferences"
"HUSS: A Heuristic Method for Understanding the Semantic Structure of Spreadsheets","X. Wu; H. Chen; C. Bu; S. Ji; Z. Zhang; V. S. Sheng","Key Laboratory of Knowledge Engineering with Big Data (the Ministry of Education of China), Hefei University of Technology, China; Key Laboratory of Knowledge Engineering with Big Data (the Ministry of Education of China), Hefei University of Technology, China; Key Laboratory of Knowledge Engineering with Big Data (the Ministry of Education of China), Hefei University of Technology, China; Key Laboratory of Knowledge Engineering with Big Data (the Ministry of Education of China), Hefei University of Technology, China; Key Laboratory of Knowledge Engineering with Big Data (the Ministry of Education of China), Hefei University of Technology, China; Department of Computer Science, Texas Tech University, Lubbock, TX, USA","2022 IEEE International Conference on Knowledge Graph (ICKG)","6 Feb 2023","2022","","","329","336","Spreadsheets contain a lot of valuable data and have many practical applications. The key technology of these practical applications is how to make machines understand the semantic structure of spreadsheets, e.g., identifying cell function types and discovering relationships between cell pairs. Most existing methods for understanding the semantic structure of spreadsheets do not make use of the semantic information of cells. A few studies do, but they ignore the layout structure information of spreadsheets, which affects the performance of cell function classification and the discovery of different relationship types of cell pairs. In this paper, we propose a Heuristic algorithm for Understanding the Semantic Structure of spreadsheets (HUSS). Specifically, based on the existing cell function classification model [11], we propose five types of heuristic rules to extract four different types of cell pairs, based on the cell style and spatial location information. Our experimental results on two real-world datasets demonstrate that the proposed method HUSS can effectively understand the semantic structure of spreadsheets and outperforms corresponding baselines.","","978-1-6654-5101-7","10.1109/ICKG55886.2022.00049","National Natural Science Foundation of China(grant numbers:62120106008,61806065,62076085,91746209,62076087); Fundamental Research Funds for the Central Universities(grant numbers:JZ2020HGQA0186); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10030026","Spreadsheet semantic structure;information extraction;heuristics","Analytical models;Heuristic algorithms;Semantics;Layout;Feature extraction;Classification algorithms;Data mining","data mining;knowledge based systems;pattern classification;spreadsheet programs","cell function classification model;cell function types;cell pairs;cell style;heuristic method;heuristic rules;HUSS;layout structure information;relationship types;semantic structure;spatial location information;spreadsheets;tabular data analysis","","","","28","IEEE","6 Feb 2023","","","IEEE","IEEE Conferences"
"Projection Dual Averaging Based Second-order Online Learning","Z. Chen; H. Zhan; V. Sheng; A. Edwards; K. Zhang","Department of Computer Science, Xavier University of Louisiana; Department of Computer Science, Texas Tech University; Department of Computer Science, Texas Tech University; Department of Computer Science, Xavier University of Louisiana; Department of Computer Science, Xavier University of Louisiana","2022 IEEE International Conference on Data Mining (ICDM)","1 Feb 2023","2022","","","51","60","Most existing online learning methods focus on mining ever-evolving streaming data based on the principle of first-order optimization. However, one drawback of these methods is the slow convergence rate in each iteration, resulting in sub-optimal solutions and deteriorated performance. Second-order methods, while are able to provide faster convergence, have been under-studied due to the high cost of computing the curvature information. To address this problem, in this paper, we develop a second-order projection dual averaging based online learning (SPDA) method to effectively handle high-throughput streaming data. By fully exploiting the regularized dual averaging optimization, the second-order information, and an optimal projection operator, SPDA converges fast with fairly optimal solutions. Two speed-up versions of SPDA, i.e., SPDA-diag and SPDA-sketch, are developed via the diagonal operator and Hessian sketch, respectively. Theoretical derivations on the regret bound of SPDA establish a solid convergence guarantee for this method. Extensive experiments demonstrate the efficacy of the proposed algorithms on large-scale online learning tasks, such as online binary and multi-class classification and online anomaly detection, shedding light on their potential wide applications.","2374-8486","978-1-6654-5099-7","10.1109/ICDM54844.2022.00015","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10027654","projection dual averaging;second-order online learning;Hessian sketch;regret bound;data streams","Learning systems;Costs;Sensitivity analysis;Solids;Classification algorithms;Computational efficiency;Task analysis","gradient methods;learning (artificial intelligence);pattern classification","fairly optimal solutions;first-order optimization;large-scale online learning tasks;learning method;mining ever-evolving;optimal projection operator;projection dual averaging based second-order online learning;regularized dual averaging optimization;second-order information;second-order methods;second-order projection dual averaging;solid convergence guarantee;SPDA-diag;SPDA-sketch;streaming data;sub-optimal solutions","","","","31","IEEE","1 Feb 2023","","","IEEE","IEEE Conferences"
"Solar Irradiance Prediction Using Transformer-based Machine Learning Models","A. Demir; L. F. Gutiérrez; A. S. Namin; S. Bayne","Department of Electrical and Computer Engineering, Texas Tech University; Department of Computer Science, Texas Tech University; Department of Computer Science, Texas Tech University; Department of Electrical and Computer Engineering, Texas Tech University","2022 IEEE International Conference on Big Data (Big Data)","26 Jan 2023","2022","","","2833","2840","This paper presents a study of irradiance prediction using a transformer-based machine learning model for the photovoltaic (PV) renewable energy system. We explore the forecast of irradiance using ten years of data from the Texas Mesonet Data Archive at the Reese Center in Lubbock, Texas. After training our transformer model with 90% of the data, we show that it can fit the irradiance trend successfully, while the testing phase with the remaining 10% of the dataset indicates that the transformer can predict irradiance trends that align with the observed values. Additionally, we borrow the concept of rolling LSTMs to generate a rolling transformer in order to extrapolate values of irradiance even when the observed values are not available. Our extrapolation results show that the transformer can extrapolate irradiance values accurately in the short-term, but it is less precise in the long-term. To remedy this, we aim to explore more thoroughly the hyperparameter configuration of our model in order to move towards our goal of including machine learning methods in the control of PV systems.","","978-1-6654-8045-1","10.1109/BigData55660.2022.10020615","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10020615","Forecasting;solar panels;renewable energy;deep learning;machine learning;transformer;time series","Training;Photovoltaic systems;Renewable energy sources;Machine learning;Predictive models;Big Data;Transformers","deep learning (artificial intelligence);recurrent neural nets;solar cells;solar power","hyperparameter configuration;photovoltaic renewable energy system;PV systems;rolling LSTM;rolling transformer;solar irradiance prediction;Texas Mesonet Data Archive;transformer-based machine learning model","","","","18","IEEE","26 Jan 2023","","","IEEE","IEEE Conferences"
"Generating Interpretable Features for Context-Aware Document Clustering: A Cybersecurity Case Study","L. F. Gutiérrez; A. S. Namin","Department of Computer Science, Texas Tech University, Lubbock, Texas, USA; Department of Computer Science, Texas Tech University, Lubbock, Texas, USA","2022 IEEE International Conference on Big Data (Big Data)","26 Jan 2023","2022","","","740","747","This paper extends the applications of ContextMiner, a framework that we proposed recently aiming to extract interpretable contextual features to conceptualize knowledge in security texts. We show that contextual features are suitable for building a document-level representation that can be used further in a downstream machine learning and natural language processing task: document clustering. Our results show that the intrinsic readability of contextual features is usable for analyzing the document clusters obtained in our experiments. Such analysis is performed through a density-based feature selection, cluster visualization, and statistical methods in order to unveil which contextual features better characterize each document cluster. Our findings suggest that statistical techniques alongside feature analysis can be utilized to discover meaningful commonalities among documents in a particular cluster without the need of querying each document manually.","","978-1-6654-8045-1","10.1109/BigData55660.2022.10021049","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10021049","natural language processing;machine learning;unsupervised learning;feature selection;text mining;document clustering","Statistical analysis;Buildings;Machine learning;Big Data;Feature extraction;Natural language processing;Task analysis","data mining;data visualisation;feature extraction;feature selection;learning (artificial intelligence);natural language processing;pattern clustering;security of data;statistical analysis;text analysis;ubiquitous computing","cluster visualization;context-aware document clustering;ContextMiner;cybersecurity;density-based feature selection;document clustering;document-level representation;downstream machine learning;feature analysis;interpretable contextual feature extraction;interpretable feature generation;natural language processing;statistical methods","","","","21","IEEE","26 Jan 2023","","","IEEE","IEEE Conferences"
"Using Transformers for Identification of Persuasion Principles in Phishing Emails","B. Karki; F. Abri; A. S. Namin; K. S. Jones","Department of Computer Science, Texas Tech University; Department of Computer Science, San Jose State University; Department of Computer Science, Texas Tech University; Department of Psychological Sciences, Texas Tech University","2022 IEEE International Conference on Big Data (Big Data)","26 Jan 2023","2022","","","2841","2848","It is important to learn about attackers and their attacking strategies so that better and more effective defense systems can be built. During the reconnaissance stage, attackers intend to probe potential targets through various techniques including social engineering attacks. Phishing through email is a well-known, cheap, easy, and surprisingly effective technique for obtaining the needed information. This type of attack targets individuals and thus utilizes weaknesses that might exist in each person. Given the uniqueness of each individual’s personality, attackers make sure the right persuasion principle technique is employed for each targeted individual. This paper describes efforts to build machine-learning transformers, the emerging technique in language modeling, with the goal of building classifiers that take into account different types of persuasion principles. More specifically, the paper describes efforts to build machine-learning transformers based on BERT, RoBERTa, and DistilBERT and captures their classification results. The results show that these transformers are accurate enough to build a classification of phishing emails with respect to persuasion techniques. Furthermore, we report that the RoBERTa model is able to train faster than BERT and DistilBERT models.","","978-1-6654-8045-1","10.1109/BigData55660.2022.10020452","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10020452","Persuasion principles;machine-learning transformers;phishing attacks","Phishing;Bit error rate;Buildings;Machine learning;Big Data;Reconnaissance;Transformers","computer crime;deep learning (artificial intelligence);natural language processing;pattern classification","attacking strategies;BERT;DistilBERT;effective defense systems;email;machine-learning transformers;persuasion principle technique;persuasion principles;phishing emails;potential targets;reconnaissance stage;RoBERTa;social engineering attacks;targeted individual","","","","18","IEEE","26 Jan 2023","","","IEEE","IEEE Conferences"
"Proximal Cost-sensitive Sparse Group Online Learning","Z. Chen; H. Zhan; V. Sheng; A. Edwards; K. Zhang","Department of Computer Science, Xavier University of Louisiana; Department of Computer Science, Texas Tech University; Department of Computer Science, Texas Tech University; Department of Computer Science, Xavier University of Louisiana; Department of Computer Science, Xavier University of Louisiana","2022 IEEE International Conference on Big Data (Big Data)","26 Jan 2023","2022","","","495","504","Effective streaming feature selection in dynamic on-line environments is essential in numerous applications. However, most existing methods evaluate high-dimensional features individually and ignore the potentially pertainable group structures of features. Moreover, the class imbalance underlying streaming data may further decrease the discriminative efficacy of the selected features, resulting in deteriorated classification performance. Motivated by this observation, we propose a proximal cost-sensitive sparse group online learning (PCSGOL) framework to handle imbalanced and high-dimensional streaming data. Specifically, we formulate this issue as a new cost-sensitive online optimization problem by leveraging the ℓ2-norm, ℓ1-norm, and group-wise sparsity constraints in the dual averaging regularization. The average weighted distance is also introduced in PCSGOL to achieve stable prediction results. We mathematically derive closed-form solutions to the optimization problems with four modified hinge loss functions, leading to four variants of PCSGOL. Extensive empirical studies on real-world streaming datasets demonstrate the effectiveness of our proposed method.","","978-1-6654-8045-1","10.1109/BigData55660.2022.10021084","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10021084","online learning;data streams;cost-sensitive learning;group sparsity;high dimension","Sensitivity analysis;Big Data;Fasteners;Feature extraction;Stability analysis;Complexity theory;Time factors","feature selection;gradient methods;learning (artificial intelligence);optimisation;pattern classification","class imbalance;cost-sensitive online optimization problem;deteriorated classification performance;discriminative efficacy;feature selection;group-wise sparsity constraints;high-dimensional features;high-dimensional streaming data;on-line environments;PCSGOL;proximal cost-sensitive sparse group online learning;real-world streaming datasets;stable prediction results;ℓ1-norm;ℓ2-norm","","","","31","IEEE","26 Jan 2023","","","IEEE","IEEE Conferences"
"Beat-by-Beat Classification of ECG Signals Using Machine Learning Algorithms to Detect PVC Beats for Real-time Predictive Cardiac Health Monitoring","I. H. Tsai; B. I. Morshed","Department of Computer Science, Texas Tech University, Lubbock, TX, USA; Department of Computer Science, Texas Tech University, Lubbock, TX, USA","2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)","2 Jan 2023","2022","","","1751","1754","A premature ventricular contraction (PVC) disrupt the normal heart rhythm and indicate underlying cardiac disease. We aim to detect these PVC beats from electrocardiogram (ECG/EKG) data by automatically classifying these ECG beats with high accuracy in real-time. In this study, we used MIT BIH Long-Term Electrocardiogram Database (ltdb) dataset from the PhysioNet database. We extract signal-specific features and signal-independent features and combine them for feature ranking. We use principal component analysis (PCA), elastic net regularization (ENR), univariate filter of constant, quasi constant and duplicate feature removal (CQCDFR) and analysis of variance test (ANOVA) for feature selection. We take the top 10 features for four methods and classify them separately. The machine learning model explored is the random forest classifier. In our analysis, elastic net regularization performed best in terms of accuracy in cardiac patients. We further use the feature with the best accuracy in four algorithms to test sensitivity, specificity, accuracy, precision, f1-score to evaluate statistics. The overall accuracy of elastic net regularization for classifying the highest first 8 feature data is 97.8%. The sensitivity was 94.7% and the specificity was 99.6%. The accuracy rate is 99.6%, and the F1 score is 97.1%. The method can accurately detect ECG beats and analyze categories for real-time cardiac monitoring for feedback to the use patient. Efficient feature selection minimizes the number of features used and reduces the power consumption of the monitoring device.","","978-1-6654-6819-0","10.1109/BIBM55620.2022.9995081","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9995081","Cardiac episodes;ECG classification;machine learning;unsupervised monitoring","Performance evaluation;Machine learning algorithms;Databases;Electrocardiography;Feature extraction;Real-time systems;Classification algorithms","diseases;electrocardiography;feature extraction;feature selection;filtering theory;medical signal detection;medical signal processing;patient monitoring;principal component analysis;random forests;signal classification","analysis of variance test (ANOVA);beat-by-beat classification;cardiac disease;cardiac patients;constant-quasiconstant-duplicate feature removal;ECG beats;ECG signals;efficient feature selection;elastic net regularization;electrocardiogram data;f1-score;feature data;feature ranking;machine learning algorithms;machine learning model;MIT BIH long-term electrocardiogram database dataset;normal heart rhythm;PhysioNet database;power consumption;premature ventricular contraction;principal component analysis;PVC beat detection;random forest classifier;real-time cardiac monitoring;real-time predictive cardiac health monitoring;signal-independent features;signal-specific features;univariate filter","","","","13","IEEE","2 Jan 2023","","","IEEE","IEEE Conferences"
"Hierarchical Graph Capsule Networks for Molecular Function Classification with Disentangled Representations","J. Zhang; Y. Lei; Y. Wang; C. Zhou; V. S. Sheng","School of Cyber Science and Engineering, Southeast University, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; Department of Computer Science, Texas Tech University, Lubbock, TX, USA","IEEE/ACM Transactions on Computational Biology and Bioinformatics","","2022","PP","99","1","11","In biochemistry, graph structures have been widely used for modeling compounds, proteins, functional interactions, etc. A common task that divides these graphs into different categories, known as graph classification, highly relies on the quality of the representations of graphs. With the advance in graph neural networks, message-passing-based methods are adopted to iteratively aggregate neighborhood information for better graph representations. These methods, though powerful, still suffer from some shortcomings. The first challenge is that pooling-based methods in graph neural networks may sometimes ignore the part-whole hierarchies naturally existing in graph structures. These part-whole relationships are usually valuable for many molecular function prediction tasks. The second challenge is that most existing methods do not take the heterogeneity embedded in graph representations into consideration. Disentangling the heterogeneity will increase the performance and interpretability of models. This paper proposes a graph capsule network for graph classification tasks with disentangled feature representations learned automatically by well-designed algorithms. This method is capable of, on the one hand, decomposing heterogeneous representations to more fine-grained elements, whilst on the other hand, capturing part-whole relationships using capsules. Extensive experiments performed on several public-available biochemistry datasets demonstrated the effectiveness of the proposed method, compared with nine state-of-the-art graph learning methods.","1557-9964","","10.1109/TCBB.2022.3233354","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10004703","Disentangled Representations;Graph Capsule Networks;Graph Classification;Molecular Function Prediction","Kernel;Task analysis;Graph neural networks;Classification algorithms;Feature extraction;Biological system modeling;Routing","","","","","","","IEEE","2 Jan 2023","","","IEEE","IEEE Early Access Articles"
"Poster: Near-Zero Power Underwater Acoustic Networks Using Scatter Communications Principles","P. Wilmoth; T. Wardak; Y. Ineza; S. Givens; C. Peroni; G. Sklivanitis; D. A. Pados","Dept. of Electrical Engineering, University of Texas at Tyler, Tyler, TX, USA; Dept. of STEAM, Palm Beach State College, Boca Raton, FL, USA; Dept. of Computer Science, Texas Tech University, Lubbock, TX, USA; Center for Connected Autonomy and AI, Florida Atlantic University, Boca Raton, FL, USA; Center for Connected Autonomy and AI, Florida Atlantic University, Boca Raton, FL, USA; Center for Connected Autonomy and AI, Florida Atlantic University, Boca Raton, FL, USA; Center for Connected Autonomy and AI, Florida Atlantic University, Boca Raton, FL, USA","2022 IEEE 19th International Conference on Mobile Ad Hoc and Smart Systems (MASS)","19 Dec 2022","2022","","","726","727","Wireless communications under the surface of the water is challenging as electromagnetic waves –predominantly used for over-the-air communications– cannot travel more than a few centimeters through water. Acoustics is usually the preferred option. However, existing acoustic modem technology is mostly limited to point-to-point communications, while for applications that require large numbers of modems to get widespread deployed in the field, the monetary and energy costs for each modem can become prohibitive for their operation. In this work, we propose to build near-zero power underwater acoustic wireless sensor networks. To that end, we develop and test modem prototypes that consume less than 4 mW for communications and less that 0.5 mW in idle mode. Our early-stage modem prototype demonstrates low communication rates of 100 bps at small distances of 15 cm.","2155-6814","978-1-6654-7180-0","10.1109/MASS56207.2022.00111","NSF(grant numbers:CNS-1950400,CNS-1753406); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9973468","Underwater IoT;backscatter communications","Wireless communication;Wireless sensor networks;Costs;Surface acoustic waves;Electromagnetic scattering;Prototypes;Modems","modems;underwater acoustic communication;wireless sensor networks","acoustic modem technology;early-stage modem prototype;electromagnetic waves;low communication rates;modem prototypes;monetary energy costs;near-zero power underwater acoustic networks;near-zero power underwater acoustic wireless sensor networks;over-the-air communications;point-to-point communications;power 0.5 mW;power 4.0 mW;preferred option;scatter communications principles;size 15.0 cm;wireless communications","","","","2","IEEE","19 Dec 2022","","","IEEE","IEEE Conferences"
"Analyzing Clinical 12-Lead ECG Images Using Deep Learning Algorithms for Objective Detection of Cardiac Diseases","N. Mitra; B. I. Morshed","Department of Computer Science, Texas Tech University, Lubbock, TX, USA; Department of Computer Science, Texas Tech University, Lubbock, TX, USA","2022 IEEE 13th Annual Ubiquitous Computing, Electronics & Mobile Communication Conference (UEMCON)","1 Dec 2022","2022","","","0517","0523","Electrocardiogram (ECG/EKG) is the most common method for the study and detection of cardiovascular diseases. Current clinical ECG devices generate 12-lead ECG traces as images on paper. The majority of artificial intelligence (AI) algorithms created for automated cardiac monitoring are based on ECG data, which necessitates brand-new, expensive equipment that the majority of clinics cannot afford. In this paper, we propose a novel method of using deep learning (DL) techniques to analyze clinical 12-lead ECG images for the objective detection of cardiac diseases. A convolutional neural network (CNN) is a DL technique that uses 2D images as input and convolves them with various filters to produce the required outputs. CNNs can be trained with enormous datasets and millions of parameters. This work introduces a high-performance CNN-based method for the objective diagnosis of heart disorders in ECG images. The proposed model automatically learns a suitable feature representation from raw clinical ECG images and thus negates the need for hand-crafted features. The ECG image dataset of 929 distinct patient records, which contains 12 lead ECG information of different cardiac patients from the Mendeley Database, was used to evaluate the classification performance. Before being analyzed by CNN, all clinical ECG waveform images were converted into two formats: colorful and grayscale images. The proposed system achieved a maximum of 97% accuracy and 96% sensitivity for colored images and 98% accuracy and 97% sensitivity for grayscale images. To validate the result, we classified the images by using dense artificial neural networks (dense ANN) and compared the results with our CNN results and CNN significantly improved the accuracy. As this proposed method is highly accurate and does not economically burden clinics, it can potentially be used as a clinical auxiliary diagnostic tool and effectively optimize medical resources.","","978-1-6654-9299-7","10.1109/UEMCON54665.2022.9965722","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9965722","Cardiac disease detection;clinical ECG equipment;CNN;dense ANN;ECG images;Electrocardiogram (ECG/EKG)","Deep learning;Heart;Sensitivity;Cardiac disease;Electrocardiography;Gray-scale;Convolutional neural networks","cardiology;cardiovascular system;diseases;electrocardiography;learning (artificial intelligence);medical signal processing;neural nets;object detection;patient diagnosis;patient monitoring;signal classification","12 lead ECG information;analyzing clinical 12-lead ECG images;artificial intelligence algorithms;automated cardiac monitoring;burden clinics;cardiac diseases;cardiovascular diseases;clinical auxiliary diagnostic tool;clinical ECG waveform images;colored images;current clinical ECG devices;deep learning algorithms;deep learning techniques;dense artificial neural networks;different cardiac patients;ECG data;ECG image dataset;enormous datasets;grayscale images;high-performance CNN-based method;objective detection;objective diagnosis;raw clinical ECG images","","","","15","IEEE","1 Dec 2022","","","IEEE","IEEE Conferences"
"Detecting PVC Beats by Beat-by-beat Analysis of ECG Signals Using Machine Learning Classifiers for Real-time Predictive Cardiac Health Monitoring","I. H. Tsai; B. I. Morshed","Department of Computer Science, Texas Tech University, Lubbock, TX, USA; Department of Computer Science, Texas Tech University, Lubbock, TX, USA","2022 IEEE 13th Annual Ubiquitous Computing, Electronics & Mobile Communication Conference (UEMCON)","1 Dec 2022","2022","","","0355","0361","Premature Ventricular Contraction (PVC) episodes are redundant heartbeats that disrupt the normal rhythm of the heart. The use of wearable sensors for remote heart monitoring and the implementation of trusted artificial intelligence (AI) algorithms are improvements in the field of smart health (sHealth) using cyber-physical systems (CPS) for telemedicine systems. We detect PVC beats by analyzing electrocardiogram (ECG/EKG) data and perform automatic classification to achieve high accuracy in real-time. In this study, we used a number of PVC heartbeat recordings from the MIT BIH supraventricular arrhythmia database We divided the recordings into a training dataset, which contains 39 ECG data, and a test dataset, which contains the remaining 39 ECG data. Both datasets contain approximately 80,000 samples of normal heartbeats and 7,000 samples of ventricular ectopic We extract combination of signal-specific features and signal-independent features for feature selection and ranking. We apply four algorithms, receiver operator characteristic (ROC) and the area under the ROC curve (AUC) (ROCAUC), constant, quasi constant and duplicate feature removal (Univariate) (CQCDFR), analysis of variance (ANOVA), and root mean square deviation (RMSE) to select and rank the feature. For each algorithm, it has its own selection of signal-independent features, which we combine separately with signal-specific features and test their accuracy. Then, we train the top 10 ranked combined features of each algorithm separately and check the highest performance. We explored the random forest (RF) classifier and support vector machine (SVM) classifier. Compared with other algorithms, the performance of feature selection using ANOVA algorithm before feature ranking is the lowest. The ANOVA algorithm achieved the highest accuracy after picking out the top 10 features. We further separately evaluate the sensitivity, specificity, accuracy, precision and F1 score of the top-ranked features according to the best accuracy obtained by different feature selection algorithms. The classification ANOVA algorithm from RF selects the top 7 features with 97% accuracy, 97.5 sensitivity, 98.1% specificity, 98.1 Precision%, and 95.0% F1 Score. This method can accurately monitor cardiac disease in real-time and analyze ECG beats so that patients can get accurate feedback.","","978-1-6654-9299-7","10.1109/UEMCON54665.2022.9965650","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9965650","ECG classification;Cardiac episodes;unsupervised monitoring;machine learning","Support vector machines;Machine learning algorithms;Heart beat;Electrocardiography;Feature extraction;Real-time systems;Classification algorithms","electrocardiography;feature extraction;feature selection;medical signal detection;medical signal processing;patient monitoring;random forests;signal classification;statistical analysis;support vector machines;telemedicine","analysis of variance (ANOVA);automatic classification;beat-by-beat analysis;classification ANOVA algorithm;cyber-physical systems;duplicate feature removal;ECG data;ECG signals;electrocardiogram data;F1 score;feature ranking;feature selection algorithms;machine learning classifiers;MIT BIH supraventricular arrhythmia database;normal rhythm;premature ventricular contraction episodes;PVC beats;PVC heartbeat recordings;random forest classifier;real-time predictive cardiac health monitoring;receiver operator characteristic curve;redundant heartbeats;remote heart monitoring;root mean square deviation;signal-independent features;signal-specific features;smart health;support vector machine classifier;SVM;telemedicine systems;test dataset;training dataset;trusted artificial intelligence algorithms;wearable sensors","","","","20","IEEE","1 Dec 2022","","","IEEE","IEEE Conferences"
"Markov Decision Process for Modeling Social Engineering Attacks and Finding Optimal Attack Strategies","F. Abri; J. Zheng; A. S. Namin; K. S. Jones","Department of Computer Science, San Jose State University, San Jose, CA, USA; Department of Computer Science, Stephen F. Austin State University, Nacogdoches, TX, USA; Department of Computer Science, Texas Tech University, Lubbock, TX, USA; Department of Psychological Sciences, Texas Tech University, Lubbock, TX, USA","IEEE Access","21 Oct 2022","2022","10","","109949","109968","It is important to comprehend the attacker’s behavior and capacity in order to build a stronger fortress and thus be able to protect valuable assets more effectively. Prior to launching technical and physical attacks, an attacker may enter the reconnaissance stage and gather sensitive information. To collect such valuable data, one of the most effective approaches is through conducting social engineering attacks, borrowing techniques from deception theory. As a result, it is of utmost importance to understand when an attacker behaves truthfully and when the attacker opts to be deceitful. This paper models attacker’s states using the Markov Decision Process (MDP) and studies the attacker’s decision for launching deception attacks in terms of cooperation and deception costs. The study is performed through MDP modeling, where the states of attackers are modeled along with the permissible actions that can be taken. We found that the optimal policy regarding being deceitful or truthful depends on the cost associated with deception and how much the attacker can afford to take the risk of launching deception attacks. More specifically, we observed that when the cost of cooperation is low (e.g., 10%), by taking MDP optimal policy, the attacker cooperates with the victim as much as possible in order to gain their trust; whereas, when the cost of cooperation is high (e.g., 50%), the attacker takes deceptive action earlier in order to minimize the cost of interactions while maximizing the impact of the attack. We report four case studies and simulations through which we demonstrate the trade-off between cooperative and deceptive actions in accordance with their costs to attackers.","2169-3536","","10.1109/ACCESS.2022.3213711","National Science Foundation (NSF)(grant numbers:1723765); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9915591","Attack strategy;cooperative;deceptive;Markov decision process;MDP;optimal solution;social engineering attacks","Costs;Markov processes;Game theory;Data models;Wireless sensor networks;Games;Analytical models;Social engineering (security);Optimal matching","computer network security;cooperative communication;Markov processes","deception theory;MDP optimal policy;Markov decision process;deception attacks;physical attacks;technical attacks;optimal attack strategies;social engineering attacks","","1","","24","CCBY","10 Oct 2022","","","IEEE","IEEE Journals"
"MalView: Interactive Visual Analytics for Comprehending Malware Behavior","H. N. Nguyen; F. Abri; V. Pham; M. Chatterjee; A. S. Namin; T. Dang","Department of Computer Science, Texas Tech University, Lubbock, TX, USA; Department of Computer Science, San Jose State University, San Jose, CA, USA; Department of Computer Science, Sam Houston State University, Huntsville, TX, USA; Department of Computer Science, New Jersey City University, Jersey City, NJ, USA; Department of Computer Science, Texas Tech University, Lubbock, TX, USA; Department of Computer Science, Texas Tech University, Lubbock, TX, USA","IEEE Access","30 Dec 2022","2022","10","","99909","99930","Malicious applications are usually comprehended through two major techniques, namely static and dynamic analyses. Through static analysis, a given malicious program is parsed, and some representative artifacts (e.g., control-flow graphs) are produced without any execution; whereas, the given malicious application needs to be executed when conducting dynamic analysis. These two mainstream techniques for analyzing the given software are effective in detecting certain classes of malware. More specifically, through static analysis, the patterns and signature of the malware are exposed, helping in detecting any known malicious payload hidden in or injected into the code. On the other hand, behavioral and run-time execution patterns of software are explored through dynamic analysis. To ease the analysis process, a third analysis approach, known as the visual representation of the artifacts created by both static and dynamic analysis tools, would also be a supplementary asset for malware experts. This paper introduces MalView, an interactive visualization platform, for malware analysis by which pattern matching techniques on both signature-based and behavioral analysis artifacts can be utilized to 1) classify malware, 2) identify the intention and location of the malicious payload in the artifacts, 3) analyze unknown malware (i.e., zero-day malware) by recognizing any unusual signature or behavior, and 4) explore the time dependencies and thus the system components affected or tampered by the underlying malware. The results of several case studies conducted in this work show that MalView offers more features and information compared to some other visualization tools, facilitating the malware analysis process.","2169-3536","","10.1109/ACCESS.2022.3207782","U.S. National Science Foundation (NSF)(grant numbers:1821560); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9895250","Malware analysis;dynamic analysis;malware visualization system;visual analytics","Malware;Behavioral sciences;Operating systems;Visual analytics;Static analysis;Performance analysis;Data visualization","data analysis;data visualisation;invasive software;pattern matching;program diagnostics","behavioral analysis artifacts;conducting dynamic analysis;control-flow graphs;dynamic analyses;given malicious application;given malicious program;given software;interactive visual analytics;interactive visualization platform;known malicious payload;mainstream techniques;malicious applications;MalView;malware analysis process;malware behavior;malware experts;representative artifacts;static analyses;static analysis;underlying malware;unknown malware;unusual signature;visual representation;visualization tools","","","","87","CCBY","19 Sep 2022","","","IEEE","IEEE Journals"
"Extending Range of Wireless Power Transfer Using a Novel Intermediate Passive Loop with Coils","B. I. Morshed; M. Rahman","Department of Computer Science, Texas Tech University, Lubbock, TX, USA; Department of Computer Science, Texas Tech University, Lubbock, TX, USA","2022 United States National Committee of URSI National Radio Science Meeting (USNC-URSI NRSM)","16 Sep 2022","2022","","","206","207","Wireless power transfer (WPT) suffers from limited range of separation between transmitter and receiver coils. We propose a novel method of using an intermediate passive wireless loop with multiple coils that allows large separation of WPT transmitter and receiver coils. Proof-of-concept results are presented using commercial Qi WPT systems where the transmitter and receiver coils were separated by 10 inches, while still providing almost identical voltage and current outputs.","","978-1-946815-15-6","10.23919/USNC-URSINRSM57467.2022.9881416","National Science Foundation(grant numbers:1932281); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9881416","","Coils;Wireless communication;Wearable computers;Radio transmitters;Receivers;Wireless power transfer;Voltage","coils;inductive power transmission;receivers;transmitters","Qi WPT systems;intermediate passive loop;wireless power transfer;transmitter;receiver coils;multiple coils;size 10.0 inch","","","","6","","16 Sep 2022","","","IEEE","IEEE Conferences"
"Decentralization Using Quantum Blockchain: A Theoretical Analysis","Z. Yang; T. Salman; R. Jain; R. D. Pietro","Department of Computer Science and Engineering, Washington University, St. Louis, MO, USA; Department of Computer Science, Texas Tech University, Lubbock, TX, USA; Department of Computer Science and Engineering, Washington University, St. Louis, MO, USA; College of Science and Engineering, Hamad Bin Khalifa University, Doha, Qatar","IEEE Transactions on Quantum Engineering","12 Oct 2022","2022","3","","1","16","Blockchain technology has been prominent recently due to its applications in cryptocurrency. Numerous decentralized blockchain applications have been possible due to blockchains’ nature of distributed, secured, and peer-to-peer storage. One of its technical pillars is using public-key cryptography and hash functions, which promise a secure, pseudoanonymous, and distributed storage with nonrepudiation. This security is believed to be difficult to break with classical computational powers. However, recent advances in quantum computing have raised the possibility of breaking these algorithms with quantum computers, thus, threatening the blockchains’ security. Quantum-resistant blockchains are being proposed as alternatives to resolve this issue. Some propose to replace traditional cryptography with postquantum cryptography—others base their approaches on quantum computer networks or quantum internets. Nonetheless, a new security infrastructure (e.g., access control/authentication) must be established before any of these could happen. This article provides a theoretical analysis of the quantum blockchain technologies that could be used for decentralized identity authentication. We put together a conceptual design for a quantum blockchain identity framework and give a review of the technical evidence. We investigate its essential components and feasibility, effectiveness, and limitations. Even though it currently has various limitations and challenges, we believe a decentralized perspective of quantum applications is noteworthy and likely.","2689-1808","","10.1109/TQE.2022.3207111","NPRP(grant numbers:#NPRP11S-0109-180242); Qatar National Research Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9893393","Blockchains;consensus protocol;decentralized applications;identity management systems;quantum computing;quantum networks","Blockchains;Quantum computing;Cryptography;Hash functions;Authentication;Security;Peer-to-peer computing","authorisation;cryptography;Internet;peer-to-peer computing;public key cryptography;quantum computing;quantum cryptography","quantum computing;quantum computers;quantum-resistant blockchains;traditional cryptography;postquantum cryptography-others;quantum computer networks;quantum internets;security infrastructure;theoretical analysis;quantum blockchain technologies;decentralized identity authentication;quantum blockchain identity framework;decentralized perspective;quantum applications;blockchain technology;numerous decentralized blockchain applications;peer-to-peer storage;technical pillars;public-key cryptography;hash functions;classical computational powers","","1","","69","CCBY","15 Sep 2022","","","IEEE","IEEE Journals"
"Modeling and Analysis of Intermittent Federated Learning Over Cellular-Connected UAV Networks","C. -H. Liu; D. -C. Liang; R. -H. Gau; L. Wei","Department of Electrical and Computer Engineering, Mississippi State University, USA; Institute of Communications Engineering, National Yang Ming Chiao Tung University, Hsinchu, Taiwan; Institute of Communications Engineering, National Yang Ming Chiao Tung University, Hsinchu, Taiwan; Department of Computer Science, Texas Tech University, Lubbock, TX, USA","2022 IEEE 95th Vehicular Technology Conference: (VTC2022-Spring)","25 Aug 2022","2022","","","1","6","Federated learning (FL) is a promising distributed learning technique particularly suitable for wireless learning scenarios since it can accomplish a learning task without raw data transportation so as to preserve data privacy and lower network resource consumption. However, current works on FL over wireless networks do not profoundly study the fundamental performance of FL over wireless networks that suffers from communication outage due to channel impairment and network interference. To accurately exploit the performance of FL over wireless networks, this paper proposes a novel intermittent FL model over a cellular-connected Unmanned Aerial Vehicle (UAV) network, which characterizes communication outage from UAV (clients) to their server and data heterogeneity among the datasets at UAVs. We propose an analytically tractable framework to derive the uplink outage probability and use it to devise a simulation-based approach so as to evaluate the performance of the proposed intermittent FL model. Our findings reveal how the intermittent FL model is impacted by uplink communication outage and UAV deployment. Extensive numerical simulations are provided to show the consistency between the simulated and analytical performances of the proposed intermittent FL model.","2577-2465","978-1-6654-8243-1","10.1109/VTC2022-Spring54318.2022.9860913","National Science Foundation; Mississippi State University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9860913","Federated learning;deep learning;unmanned aerial vehicle network;outage probability;point process","Analytical models;Vehicular and wireless technologies;Wireless networks;Probability;Autonomous aerial vehicles;Collaborative work;Power system reliability","autonomous aerial vehicles;cellular radio;data privacy;learning (artificial intelligence);probability;radiofrequency interference;telecommunication network reliability","intermittent federated learning;cellular-connected UAV networks;wireless learning scenarios;learning task;raw data transportation;data privacy;lower network resource consumption;wireless networks;impairment;network interference;novel intermittent FL model;cellular-connected Unmanned Aerial Vehicle network;data heterogeneity;uplink communication outage;UAV deployment;simulated performances;analytical performances","","","","12","IEEE","25 Aug 2022","","","IEEE","IEEE Conferences"
"ContextMiner: Mining Contextual Features for Conceptualizing Knowledge in Security Texts","L. F. Gutiérrez; A. Namin","Department of Computer Science, Texas Tech University, Lubbock, TX, USA; Department of Computer Science, Texas Tech University, Lubbock, TX, USA","IEEE Access","19 Aug 2022","2022","10","","85891","85904","This paper presents ContextMiner, a novel natural language processing (NLP) framework to automatically capture contextual features for the purpose of extracting meaningful context-aware phrases from cybersecurity unstructured textual data. The framework utilizes basic attributes such as part-of-speech tagging, dependency parsing, and a domain-specific grammar to extract the contextual features. The effectiveness and applications of ContextMiner are evaluated and presented from two different perspectives: qualitative and quantitative. As for the qualitative analysis, our case studies show that the proposed framework is capable of retrieving additional contents from the given texts, both in a labeled and unlabeled setting, and thus building context-aware phrases in comparison with existing approaches. From a quantitative point of view, we evaluate ContextMiner as a pre-processing step to perform named entity recognition (NER). Our results show that ContextMiner reduces the corpus up to 70% while maintaining 85% of its relevant entities, with a small drop in the classification metrics. Finally, we explored the utilization of ContextMiner in the construction and reasoning of knowledge graphs.","2169-3536","","10.1109/ACCESS.2022.3198944","U.S. National Science Foundation (NSF)(grant numbers:1723765,1821560,1564293); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9857853","Dependency parsing;feature extraction;machine learning;natural language processing;word embeddings","Feature extraction;Computer security;Data mining;Syntactics;Natural language processing;Machine learning;Tagging","data mining;grammars;information retrieval;learning (artificial intelligence);natural language processing;text analysis","ContextMiner;mining contextual features;conceptualizing knowledge;security texts;natural language processing framework;meaningful context-aware phrases;cybersecurity unstructured textual data;part-of-speech tagging;domain-specific grammar","","1","","35","CCBY","16 Aug 2022","","","IEEE","IEEE Journals"
"Vulnerability Detection in Smart Contracts Using Deep Learning","S. Gopali; Z. A. Khan; B. Chhetri; B. Karki; A. S. Namin","Department of Computer Science, Texas Tech University; Department of Computer Science, Texas Tech University; Department of Computer Science, Texas Tech University; Department of Computer Science, Texas Tech University; Department of Computer Science, Texas Tech University","2022 IEEE 46th Annual Computers, Software, and Applications Conference (COMPSAC)","10 Aug 2022","2022","","","1249","1255","Various decentralized applications have deployed millions of smart contracts (SCs) on the Blockchain networks. SCs enable programmable transactions involving the transfer of monetary assets between peers on a Blockchain network without any need to a central authority. However, similar to any software program, SCs may contain security issues. Software se-curity engineers and researchers have already uncovered several Ethereum BlockChain and SC vulnerabilities. Still, researchers continuously discover many more security flaws in deployed SCs. Indeed, the popularity of SCs attracts adversaries to launch new attack vectors. Thus, efficient vulnerability detection is necessary. This paper lists broad known vulnerabilities in SCs and classifies them based on the multi-class categories such as Suicidal, Prodigal, Greedy, and Normal SCs. The paper adopts artificial recurrent neural network architecture such as Long Short-Term Memory (LSTM) and Temporal Convolutional Network (TCN) used in deep learning to identify and then classify vulnerable Scs.","0730-3157","978-1-6654-8810-5","10.1109/COMPSAC54236.2022.00197","National Science Foundation(grant numbers:1723765,1821560); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9842527","Temporal Convolutional Network (TCN);Long Short-Term Memory LSTM;Smart Contracts (SC)","Deep learning;Recurrent neural networks;Conferences;Smart contracts;Computer architecture;Decentralized applications;Software","blockchains;contracts;learning (artificial intelligence);pattern classification;recurrent neural nets","smart contracts;deep learning;decentralized applications;Blockchain network;programmable transactions;monetary assets;central authority;software program;security issues;software se-curity engineers;security flaws;efficient vulnerability detection;broad known vulnerabilities;artificial recurrent neural network architecture;Temporal Convolutional Network","","1","","54","IEEE","10 Aug 2022","","","IEEE","IEEE Conferences"
"CSDLEEG: Identifying Confused Students Based on EEG Using Multi-View Deep Learning","H. Abu-gellban; Y. Zhuang; L. Nguyen; Z. Zhang; E. Imhmed","Department of Computer Science, Texas Tech University; Department of Computer Science, Texas Tech University; Computer Science and Data Science, Meharry Medical College; Computer Science Division, Clemson University; Department of Computer Science, New Mexico State University","2022 IEEE 46th Annual Computers, Software, and Applications Conference (COMPSAC)","10 Aug 2022","2022","","","1217","1222","Distance learning has dramatically increased in recent years because of advanced technology. In addition, numerous universities had to offer courses in online mode in 2020 and 2021 because of the COVID-19 pandemic. However, there are more challenges in distance learning than in the traditional learning method (e.g., feedback and interaction). Recently, researchers started using simple EEG headsets to identify confused students during online courses based on machine learning approaches. However, they faced unpleasant accuracy using traditional machine learning algorithms or nondeep neural networks. In this paper, we present a data-driven approach based on a multi-view deep learning technique called CSDLEEG to identify confused students. We employ the students' demographic information and EEG signals to feed our novel neural networks. The results show that our proposed approach is superior to state-of-the-art methods for 98% accuracy and 98% F1-score.","0730-3157","978-1-6654-8810-5","10.1109/COMPSAC54236.2022.00192","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9842723","Distance Learning;Massive Open Online Courses (MOOC);Deep Learning;Multi-View;Confused Stu-dents","Deep learning;Learning systems;Headphones;Computer aided instruction;Machine learning algorithms;Pandemics;Neural networks","computer aided instruction;distance learning;electroencephalography;learning (artificial intelligence);neural nets","CSDLEEG;identifying confused students;EEG using multiview deep learning;distance learning;numerous universities;online mode;COVID-19 pandemic;traditional learning method;feedback;simple EEG headsets;online courses;machine learning approaches;nondeep neural networks;data-driven approach;deep learning technique","","","","30","IEEE","10 Aug 2022","","","IEEE","IEEE Conferences"
"Conceptual Mappings of Conventional Software and Machine Learning-based Applications Development","S. Angel; A. S. Namin","Department of Computer Science, Texas Tech University; Department of Computer Science, Texas Tech University","2022 IEEE 46th Annual Computers, Software, and Applications Conference (COMPSAC)","10 Aug 2022","2022","","","1223","1230","This paper presents two concept models to compare and contrast conventional software development life cycle(SDLC) and the development of smart applications where Artificial Intelligence/Machine Learning (AI/ML) modules are integrated into the software products. The first concept model illustrates the intersections and differences between SDLC and AI/ML-based application development; whereas, the second concept model depicts the processes/concepts involved in data preprocessing stage of AI/ML-based application development. The concept models show that while there are some similarities between SDLC and AI/ML-based application development; AI/ML-based development follows its own unique features and challenges that need development of further software engineering techniques.","0730-3157","978-1-6654-8810-5","10.1109/COMPSAC54236.2022.00193","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9842719","Software Development Life cycle (SDLC);Machine Learning Applications;Concept Map","Uncertainty;Data preprocessing;Machine learning;Learning (artificial intelligence);Software;Data models;Software measurement","learning (artificial intelligence);software engineering","conceptual mappings;machine learning-based applications development;software development life cycle;artificial intelligence-machine learning modules;AI-ML-based application development;SDLC;data preprocessing stage","","","","8","IEEE","10 Aug 2022","","","IEEE","IEEE Conferences"
"Active Crowdsourcing for Multilabel Annotation","J. Zhang; M. Wu; C. Zhou; V. S. Sheng","School of Cyber Science and Engineering, Southeast University, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; Department of Computer Science, Texas Tech University, Lubbock, TX, USA","IEEE Transactions on Neural Networks and Learning Systems","","2022","PP","99","1","11","Multilabel annotation is a critical step to generate training sets when learning classification models in various application domains, but asking domain experts to provide labels is usually time-consuming and expensive, which cannot meet the current requirement of the fast evolution of the models in the big data era. Although crowdsourcing provides a fast solution to acquire labels for multilabel learning, it faces the risk of high data acquisition cost and low label quality. This article proposes a novel one-coin label-dependent active crowdsourcing (OCLDAC) method to iteratively query noisy labels from crowd workers and learn multilabel classification models. In each iteration of active learning, integrated labels of instances are first inferred by a novel one-coin label-dependent model, which utilizes a mixture of multiple independent Bernoulli distributions to explore and exploit correlations among the labels to increase the accuracy of truth inference. Then, instances, labels, and workers are selected according to the novel strategies that incorporate the distribution of noisy labels, the prediction probability of learning models, label correlations, and the reliability of crowd workers. Simulations on eight multilabel datasets and evaluation on one real-world crowdsourcing dataset consistently show that the proposed OCLDAC significantly outperforms the state-of-the-art methods and their variants.","2162-2388","","10.1109/TNNLS.2022.3194022","National Key Research and Development Program of China(grant numbers:2018AAA0102002); National Natural Science Foundation of China(grant numbers:62076130,91846104,61902186); Open Research Projects of Zhejiang Lab(grant numbers:2019KD0AD01/015); Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9848427","Active learning;crowdsourcing learning;multilabel classification;truth inference","Crowdsourcing;Predictive models;Correlation;Annotations;Task analysis;Uncertainty;Training","","","","","","","IEEE","2 Aug 2022","","","IEEE","IEEE Early Access Articles"
"Neural Network Modeling Attacks on Arbiter-PUF-Based Designs","N. Wisiol; B. Thapaliya; K. T. Mursi; J. -P. Seifert; Y. Zhuang","Security in Telecommunications, Technische Universität Berlin, Berlin, Germany; Department of Computer Science, Texas Tech University, Lubbock, TX, USA; Department of Cybersecurity, College of Computer Science and Engineering, University of Jeddah, Jeddah, Saudi Arabia; Security in Telecommunications, Technische Universität Berlin, Berlin, Germany; Department of Computer Science, Texas Tech University, Lubbock, TX, USA","IEEE Transactions on Information Forensics and Security","1 Aug 2022","2022","17","","2719","2731","By revisiting, improving, and extending recent neural-network based modeling attacks on XOR Arbiter PUFs from the literature, we show that XOR Arbiter PUFs, (XOR) Feed-Forward Arbiter PUFs, and Interpose PUFs can be attacked faster, up to larger security parameters, and with an order of magnitude fewer challenge-response pairs than previously known both in simulation and in silicon data. To support our claim, we discuss the differences and similarities of recently proposed modeling attacks and offer a fair comparison of the performance of these attacks by implementing all of them using the popular machine learning framework Keras and comparing their performance against the well-studied Logistic Regression attack. Our findings show that neural-network-based modeling attacks have the potential to outperform traditional modeling attacks on PUFs and must hence become part of the standard toolbox for PUF security analysis; the code and discussion in this paper can serve as a basis for the extension of our results to PUF designs beyond the scope of this work.","1556-6021","","10.1109/TIFS.2022.3189533","National Science Foundation(grant numbers:2103563); German Ministry for Education and Research as BBDC 2(grant numbers:01IS18025A); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9819893","Physical unclonable function;strong PUFs;machine learning;modeling attacks;arbiter PUF","Security;Data models;Analytical models;Cryptography;Neural networks;Machine learning;Predictive models","asynchronous circuits;copy protection;cryptography;feedforward neural nets;learning (artificial intelligence);logic design;regression analysis","neural network modeling attacks;XOR arbiter PUF;interpose PUF;challenge-response pairs;neural-network-based modeling attacks;PUF security analysis;logistic regression attack;neural-network based modeling attacks;arbiter-PUF-based designs;feed-forward arbiter PUF;machine learning framework Keras","","3","","37","IEEE","7 Jul 2022","","","IEEE","IEEE Journals"
"Automatic Detection of Situational Context Using AI from Minimal Sensor Modality","N. Mitra; B. I. Morshed","Department of Computer Science, Texas Tech University, Lubbock, TX, USA; Department of Computer Science, Texas Tech University, Lubbock, TX, USA","2022 IEEE International Conference on Electro Information Technology (eIT)","7 Jul 2022","2022","","","358","362","There is a vast improvement in the sector of human healthcare and education delivery using artificial intelligence (AI) such as recommender system. For this purpose, many modern technology are being used like smart wearable device to make the diagnosis of different types of human disease and automated tutoring systems. For these AI algorithms, there can be high error rates if situational contexts are ignored. Currently there is no automated approach to detect situational context. In this work, we propose a novel approach to automatically detect situational context using clustering algorithm type AI from minimal sensor modality. We begin the process by converting a few sensor data to a multitude of axes, then determine situational context from these axes by using clustering algorithm. Here, we evaluated the machine learning Clustering algorithm performance on the simulated data and compared the characteristics of their performance. The results show 89.4% accuracy for Seven situational contexts and 89.8% accuracy for eight situational contexts. This preliminary work shows the feasibility of detecting situational context automatically from a few sensor data by converting the sensor data to multiple axes and applying clustering algorithm.","2154-0373","978-1-6654-8009-3","10.1109/eIT53891.2022.9813854","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9813854","KMEANS algorithm;machine learning;situational context detection","Machine learning algorithms;Error analysis;Wearable computers;Clustering algorithms;Medical services;Machine learning;Artificial intelligence","behavioural sciences computing;diseases;intelligent tutoring systems;learning (artificial intelligence);medical diagnostic computing;pattern clustering;recommender systems","situational context;minimal sensor modality;sensor data;situational contexts;human healthcare;education delivery;artificial intelligence;recommender system;human disease;automated tutoring systems;clustering algorithm type AI","","","","12","IEEE","7 Jul 2022","","","IEEE","IEEE Conferences"
"Beat-by-beat Classification of ECG Signals with Machine Learning Algorithm for Cardiac Episodes","I. H. Tsai; B. I. Morshed","Department of Computer Science, Texas Tech University, Lubbock, TX, USA; Department of Computer Science, Texas Tech University, Lubbock, TX, USA","2022 IEEE International Conference on Electro Information Technology (eIT)","7 Jul 2022","2022","","","311","314","Heart failure (HF) is a common clinical syndrome of cardiac episode leading to a variety of cardiac diseases. Detecting these cardiac episodes from electrocardiogram (ECG or EKG) data and classifying these large data automatically with high accuracy in real-time is critical for useful application of wearables targeting cardiac disease monitoring. With this motivation, in this study, we used the BIDMC Congestive Heart Failure (CHF) datasets (from PhysioNet database). A total of 15 patient records was analyzed, which have NYHA Class level III and IV patients from the database. Simultaneous measurements of the 2 leads of ECG were stored in the record. The captured data was sampled at 250 Hz. The extracted features were for three categories: temporal, spectral, and statistical. In total, we extracted 28 features out of which 7 were of amplitude types, 6 were based on frequency, and the remaining 15 were statistical features. Machine learning models explored include SVM, KNN, ensemble tree, neural network, decision tree, naive bayes, and logistic regression. We evaluated different model performance in each patient data and combined patient data. In our analysis, neural network was the best performer in terms of accuracy for cardiac patients. We further studied neural network to test sensitivity, specificity, accuracy, precision, f1-score to evaluate the best performer statistics. Neural network has 99.5% overall accuracy for interpatient data classification, and was also among the best performers. In interpatient classification, the performance was: sensitivity 99.80%, specificity 99.0%. accuracy 99.42%, precision 99.80%, and F1 score 99.64%. Accurate detection of ECG beat classes using this approach can allow real-time cardiac disease monitoring.","2154-0373","978-1-6654-8009-3","10.1109/eIT53891.2022.9813902","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9813902","Cardiac episodes;ECG classification;machine learning;unsupervised monitoring","Training;Sensitivity;Neural networks;Cardiac disease;Electrocardiography;Feature extraction;Real-time systems","decision trees;diseases;electrocardiography;feature extraction;medical signal detection;medical signal processing;naive Bayes methods;nearest neighbour methods;neural nets;patient monitoring;regression analysis;signal classification;signal sampling;support vector machines","cardiac episode;clinical syndrome;patient records;statistical features;machine learning models;neural network;model performance;combined patient data;cardiac patients;f1-score;performer statistics;interpatient data classification;ECG beat classes;real-time cardiac disease monitoring;beat-by-beat classification;ECG signals;BIDMC congestive heart failure datasets;heart failure;PhysioNet database;NYHA Class level III patients;NYHA Class level IV patients;extracted features;SVM;KNN;ensemble tree;decision tree;Naive Bayes;logistic regression;frequency 250.0 Hz","","","","13","IEEE","7 Jul 2022","","","IEEE","IEEE Conferences"
"Wafer Pattern Counting, Detection and Classification Based on Encoder-Decoder CNN Structure","Y. Lin","Department of Computer Science, Texas Tech University, Lubbock, TX, USA","2022 IEEE International Conference on Electro Information Technology (eIT)","7 Jul 2022","2022","","","109","113","This paper designs an automatic wafer pattern counting pipeline based on a convolutional neural network (CNN) based structure, also called the WPCCNN network pipeline. The study will utilize deep learning algorithms to detect, binary classify and count wafer patterns. In the sample dataset, over two hundred wafers have been scanned by industrial computed tomography, containing 11 different patterns for the dataset images. Each image includes three processed steps. Moreover, it utilizes the lightweight CNN structure to demonstrate detection, classification, and estimated counting [1, 3]. Besides, the study also uses encoder and decoder structure on the CNN algorithm to obtain the closest expected output. Compared to traditional object counting methods, such as localization and density estimation, using this new method to count objects will be more accurate, faster, and more accessible [1–3]. The experiment results indicate that our algorithm is highly accurate with the paring between the original patterns and the labeled markers. The average counting accuracy is 99.6% in a single wafer.","2154-0373","978-1-6654-8009-3","10.1109/eIT53891.2022.9813870","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9813870","Deep learning;Convolutional neural network (CNN);Encoder;Decoder;Image classification;Detection;Pattern Counting;Wafer;GPU","Deep learning;Location awareness;Image segmentation;Pipelines;Neural networks;Estimation;Classification algorithms","computerised tomography;convolutional neural nets;deep learning (artificial intelligence);image classification","encoder-decoder CNN structure;automatic wafer pattern;convolutional neural network based structure;WPCCNN network pipeline;deep learning;sample dataset;industrial computed tomography;dataset images;lightweight CNN structure;classification;decoder structure;CNN algorithm;object counting methods;localization;density estimation;original patterns;average counting accuracy;single wafer;wafer pattern counting pipeline","","","","19","IEEE","7 Jul 2022","","","IEEE","IEEE Conferences"
"Classification of COVID-19 Disease Severity using CT Scans via Deep Convolutional Neural Networks","M. Davis; B. I. Morshed","Saint Ignatius College, Chicago, IL, USA; Department of Computer Science, Texas Tech University, Lubbock, TX, USA","2022 IEEE International Conference on Electro Information Technology (eIT)","7 Jul 2022","2022","","","401","404","The respiratory virus Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2), commonly known as COVID-19, has caused wide concern and a need to be able to accurately determine its effects on individual lung capacities. Computerized Tomography (CT) scans were chosen as the main datatype for determining whether a patient had COVID-19 or had normal lung capacities. The rationale is that there is an inherent lack of CT scan experts, especially in counties with low socio-economic status (SES). Thus, an automated and objective artificially intelligent (AI) algorithm can assist in the hope to provide more efficacious use CT scans for classification of COVID-19 and monitoring of disease progression. In this study, a wide variety of CT scans with different formats were tested to determine the best approach for binary classification models using deep learning (DL) techniques. A total of three publicly available COVID-19 datasets were tested using a 2-dimensional and a 3-dimensional algorithm, where each dataset had its own subsets and unique parameters. A finalized version of the 3-dimensional model was shown to achieve a high accuracy, precision, sensitivity, and F1 Score of 86.6% each. The developed method provides an objective measure to automatically classify COVID-19 patients from CT scans","2154-0373","978-1-6654-8009-3","10.1109/eIT53891.2022.9813873","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9813873","Automated classification;COVID-19;CT scan;Deep learning;SARS-CoV-2.","COVID-19;Deep learning;Solid modeling;Three-dimensional displays;Sensitivity;Computed tomography;Lung","computerised tomography;diseases;image classification;learning (artificial intelligence);lung;medical computing;medical image processing;microorganisms;neural nets;patient diagnosis;pattern classification;pneumodynamics","COVID-19 disease severity;CT scans;deep convolutional neural networks;respiratory virus Acute Respiratory Syndrome Coronavirus 2;SARS-CoV-2;individual lung capacities;Computerized Tomography scans;normal lung capacities;CT scan experts;binary classification models;publicly available COVID-19 datasets;COVID-19 patients","","","","18","IEEE","7 Jul 2022","","","IEEE","IEEE Conferences"
"Implementation of Intermediate Passive Loop Coils to Extend the Range of Qi Wireless Charging","M. Rahman; B. I. Morshed","Department of Computer Science, Texas Tech University, Lubbock, TX, USA; Department of Computer Science, Texas Tech University, Lubbock, TX, USA","2022 IEEE MTT-S International Microwave Biomedical Conference (IMBioC)","13 Jun 2022","2022","","","132","134","Wireless power transfer (WPT) based system such as Qi charging technology can not operate with a separation between transmitter and receiver coils. In this paper, a novel passive wireless loop made with magnetic wire is proposed to extend the range between the transmitter and receiver coil. One end of the proposed loop coil is placed on the Qi transmitter while the other end is placed on the Qi receiver. Experimental results are presented for coils with different number of turns and diameter, where Qi transmitter and receiver were separated by a distance of 10 inches. The proposed setup was able to reasonably provide similar performance as the traditional setup of coil nositions. while increasing the range of Oi charging.","","978-1-6654-2340-3","10.1109/IMBioC52515.2022.9790251","National Science Foundation(grant numbers:2105766); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9790251","wireless power transfer;Qi charging;passive loop coil;efficiency","Coils;Wireless communication;Transmitters;Magnetic separation;Wires;Receivers;Wireless power transfer","coils;inductive power transmission","coil nositions;Qi receiver;Qi transmitter;loop coil;receiver coil;Qi charging technology;wireless power transfer based system;Qi wireless charging;intermediate passive loop coils;distance 10 inch","","1","","10","IEEE","13 Jun 2022","","","IEEE","IEEE Conferences"
"Toward Ubiquitous and Flexible Coverage of UAV-IRS-Assisted NOMA Networks","C. -H. Liu; M. A. Syed; L. Wei","Department of Electrical & Computer Engineering, Mississippi State University, MS, USA; Department of Electrical & Computer Engineering, Mississippi State University, MS, USA; Department of Computer Science, Texas Tech University, Lubbock, TX, USA","2022 IEEE Wireless Communications and Networking Conference (WCNC)","19 May 2022","2022","","","1749","1754","This paper studies how to achieve a high and flexible coverage performance of a large-scale cellular network that enables unmanned aerial vehicles (UAVs) for non-orthogonal multiple access (NOMA) transmission to simultaneously serve multiple users. The considered cellular network consists of a tier of base stations and a tier of UAVs. Each UAV is mounted with an intelligent reflecting surface (IRS) in order to serve as an aerial IRS reflecting signals between a base station and a user in the network. All the UAVs in the network are deployed based on a newly proposed three-dimensional (3D) point process that leads to a tractable and accurate analysis of the association statistics, which is traditionally difficult to analyze due to the mobility of UAVs. In light of this, we are able to analyze the downlink coverage of UAV-IRS-assisted NOMA transmission for two users and derive the corresponding coverage probabilities. Our coverage analyses shed light on the optimal allocations of transmit power between NOMA users and UAVs to accomplish the goal of ubiquitous and flexible NOMA transmission. We also conduct numerical simulations to validate our coverage analytical results while demonstrating the improved coverage performance achieved by aerial IRSs.","1558-2612","978-1-6654-4266-4","10.1109/WCNC51071.2022.9771729","National Science Foundation; Mississippi State University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9771729","Coverage;Unmanned Aerial Vehicle;Intelligent Reflecting Surface;NOMA","Cellular networks;NOMA;Base stations;Schedules;Three-dimensional displays;Probability;Autonomous aerial vehicles","autonomous aerial vehicles;cellular radio;probability;remotely operated vehicles","ubiquitous NOMA transmission;flexible NOMA transmission;coverage analytical results;improved coverage performance;flexible coverage;UAV-IRS-assisted NOMA networks;high coverage;large-scale cellular network;unmanned aerial vehicles for nonorthogonal;access transmission;multiple users;considered cellular network;tier;base station;aerial IRS;downlink coverage;UAV-IRS-assisted NOMA transmission;corresponding coverage probabilities;coverage analyses;NOMA users","","3","","13","IEEE","19 May 2022","","","IEEE","IEEE Conferences"
"A Lightweight and Anonymous Authentication and Key Agreement Protocol for Wireless Body Area Networks","C. Pu; H. Zerkle; A. Wall; S. Lim; K. -K. R. Choo; I. Ahmed","Department of Computer Sciences and Electrical Engineering, Marshall University, Huntington, WV, USA; Department of Computer Sciences and Electrical Engineering, Marshall University, Huntington, WV, USA; Department of Computer Sciences and Electrical Engineering, Marshall University, Huntington, WV, USA; Department of Computer Science, Texas Tech University, Lubbock, TX, USA; Department of Information Systems and Cyber Security, The University of Texas at San Antonio, San Antonio, TX, USA; Department of Electrical Engineering and Computer Science, Howard University, Washington, DC, USA","IEEE Internet of Things Journal","21 Oct 2022","2022","9","21","21136","21146","As a major building block of Healthcare 4.0, wireless body area networks (WBANs) play an important role in collecting patient’s real-time physical phenomena through small wearable or implantable intelligent medical devices and communicating with remote medical experts using short-range wireless communication techniques. However, the challenges of securing information access are partly evidenced by the difficulty in designing secure and efficient security protocols. For example, existing authentication and key agreement schemes have either potential security vulnerabilities or high communication and computation overhead. In this article, we propose a lightweight and anonymous authentication and key agreement protocol, also called liteAuth, for WBANs. In our approach, mutual authentication and session key agreement are achieved using the Tinkerbell map-based random shuffling, physical unclonable function, one-way hash function, and bitwise exclusive OR operation. The security of liteAuth is first verified using the AVISPA tool, and then its cyber resilience is analyzed. In addition, we develop a real-world testbed, implement liteAuth and two existing schemes (i.e., PSLAP and HARCI), and conduct experiments for performance evaluation and analysis. Experimental results indicate that liteAuth can improve the performance of communication overhead and computation time as well as reduce energy consumption, while meeting all security requirements.","2327-4662","","10.1109/JIOT.2022.3175756","West Virginia Research Higher Education Policy Commission, Division of Science and Research(grant numbers:dsr.20.1698-001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9776522","Authentication and key agreement protocol;lightweight and anonymous;security and privacy;wireless body area networks (WBANs)","Security;Authentication;Protocols;Medical services;Wireless communication;Sensors;Chaotic communication","authorisation;body area networks;cryptographic protocols;health care;power consumption;telecommunication power management;telecommunication security","anonymous authentication;AVISPA tool;bitwise exclusive OR operation;communication overhead;computation overhead;cyber resilience;efficient security protocols;energy consumption;Healthcare 4.0;high communication;implantable intelligent medical devices;key agreement protocol;key agreement schemes;lightweight authentication;liteAuth security;mutual authentication;one-way hash function;patient real-time physical phenomena;physical unclonable function;potential security vulnerabilities;random shuffling;real-world testbed;remote medical experts;secure security protocols;security requirements;session key agreement;short-range wireless communication techniques;small wearable intelligent medical devices;Tinkerbell map;WBAN;wearable devices;wireless body area networks","","6","","38","IEEE","17 May 2022","","","IEEE","IEEE Journals"
"A Lightweight and Privacy-Preserving Mutual Authentication and Key Agreement Protocol for Internet of Drones Environment","C. Pu; A. Wall; K. -K. R. Choo; I. Ahmed; S. Lim","Department of Computer Sciences and Electrical Engineering, Marshall University, Huntington, WV, USA; Department of Computer Sciences and Electrical Engineering, Marshall University, Huntington, WV, USA; Department of Information Systems and Cyber Security, The University of Texas at San Antonio, San Antonio, TX, USA; Department of Electrical Engineering and Computer Science, Howard University, Washington, DC, USA; Department of Computer Science, Texas Tech University, Lubbock, TX, USA","IEEE Internet of Things Journal","7 Jun 2022","2022","9","12","9918","9933","With accelerated advances in various technologies, drones, better known as unmanned aerial vehicles (UAVs), are increasingly commonplace and consequently have a more pronounced impact on society. For example, Internet of Drones (IoD), a new communication paradigm offering fundamental navigation assistance and access to information, has widespread applications ranging from agricultural drones in farming to surveillance drones in the COVID-19 pandemic. The increasingly prominent role of IoD in our society also reinforces the importance of securing such systems against various data privacy and security threats. Operationally, it can be challenging to adopt conventional off-the-shelf security products in an IoD system due to the underpinning characteristics of drones (e.g., dynamic and open communication channel). Therefore in this article, we propose a lightweight and privacy-preserving mutual authentication and key agreement protocol, hereafter referred to as PMAP. The latter uses a physical unclonable function (PUF) and chaotic system to support mutual authentication and establish a secure session key between communication entities in the IoD system. To be specific, PMAP consists of two schemes, namely: 1)  ${\mathrm{ PMAP}}^{D2Z}$  (that mutually authenticates drone and zone service provider (ZSP) and establishes secure session keys) and 2)  ${\mathrm{ PMAP}}^{D2D}$  (that mutually authenticates drones and establishes secure session keys). In addition, PMAP supports conditional privacy preserving so that the genuine identity of drones can only be revealed by trusted ZSPs. We evaluate the security of PMAP using automated validation of Internet security protocols and application (AVISPA), as well as provide formal and informal security analysis to show the resilience of PMAP against various security attacks. We also evaluate the performance of PMAP through extensive experiments and compare its performance with existing AKA and IBE-Lite schemes, whose findings show that PMAP achieves better performance in terms of computation cost, energy consumption, and communication overhead.","2327-4662","","10.1109/JIOT.2022.3163367","NASA West Virginia EPSCoR(grant numbers:80NSSC20M0055); West Virginia HEPC’s Division of Science and Research(grant numbers:dsr.20.1698-001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9745033","Authentication and key agreement;chaotic system;drone;Internet of Drones (IoD);physical unclonable function (PUF)","Drones;Security;Authentication;Protocols;Internet of Things;Costs;Chaotic communication","authorisation;computer network security;data privacy;Internet of Things;message authentication;public key cryptography;remotely operated vehicles","communication overhead;energy consumption;IBE-Lite schemes;physical unclonable function;COVID-19 pandemic;Internet of drones environment;mutually authenticates drones;zone service provider;mutually authenticates drone;secure session key;chaotic system;lightweight privacy-preserving mutual authentication;open communication channel;dynamic communication channel;IoD system;off-the-shelf security products;security threats;data privacy;surveillance drones;agricultural drones;fundamental navigation assistance;communication paradigm;unmanned aerial vehicles;drones environment;key agreement protocol;security attacks;informal security analysis;provide formal security analysis;Internet security protocols;conditional privacy;PMAP","","14","","33","IEEE","30 Mar 2022","","","IEEE","IEEE Journals"
"Demo: An Experimental Environment Based On Mini-PCs For Federated Learning Research","F. Freitag; P. Vilchez; L. Wei; C. -H. Liu; M. Selimi; I. Koutsopoulos","Computer Architecture Department, UPC BarcelonaTech., Spain; Computer Architecture Department, UPC BarcelonaTech., Spain; Department of Computer Science, Texas Tech University, Lubbock, TX, USA; Department of Electrical and Computer Engineering, Mississippi State University, Starkville, MS, USA; Max van der Stoel Institute, South East European University, North Macedonia; Department of Informatics, Athens University of Economics and Business, Athens, Greece","2022 IEEE 19th Annual Consumer Communications & Networking Conference (CCNC)","10 Feb 2022","2022","","","927","928","There is a growing research interest in Federated Learning (FL), a promising approach for data privacy preservation and proximity of training to the network edge, where data is generated. Resource consumption for Machine Learning (ML) training and inference is important for edge nodes, but most of the proposed protocols and algorithms for FL are evaluated by simulations. In this demo paper, we present an environment based on distributed mini-PCs to enable experimental study of FL protocols and algorithms. We have installed low-capacity mini-PCs within a wireless city-level mesh network and deployed container-based FL components on these nodes. We show the deployed FL clients and server at different nodes in the city and demonstrate how an FL experiment can be set and run in a real environment.","2331-9860","978-1-6654-3161-3","10.1109/CCNC49033.2022.9700579","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9700579","Federated Learning;Edge/cloud computing;Mini-PCs;Test-bed","Training;Wireless communication;Mesh networks;Protocols;Machine learning algorithms;Urban areas;Machine learning","cloud computing;data privacy;inference mechanisms;learning (artificial intelligence);telecommunication security;wireless mesh networks","edge nodes;FL protocols;distributed mini-PCs;low-capacity mini-PCs;wireless city-level mesh network;federated learning;data privacy preservation;network edge;resource consumption;container-based FL components;machine learning training;machine learning inference;cloud computing","","","","3","IEEE","10 Feb 2022","","","IEEE","IEEE Conferences"
"A Hybrid Deep Network Framework for Android Malware Detection","H. -J. Zhu; L. -M. Wang; S. Zhong; Y. Li; V. S. Sheng","School of Computer Science and Communication Engineering, Jiang Su University, Zhenjiang, Jiangsu, China; School of Computer Science and Communication Engineering, Jiang Su University, Zhenjiang, Jiangsu, China; Department of Computer Science and Technology, Nanjing University, Nanjing, Jiangsu, China; School of Computer Science and Communication Engineering, Jiang Su University, Zhenjiang, Jiangsu, China; Department of Computer Science, Texas Tech University, Lubbock, TX, USA","IEEE Transactions on Knowledge and Data Engineering","7 Nov 2022","2022","34","12","5558","5570","Android is a growing target for malicious software (malware) because of its popularity and functionality. Malware poses a serious threat to users’ privacy, money, equipment and file integrity. A series of data-driven malware detection methods were proposed. However, there exist two key challenges for these methods: (1) how to learn effective feature representation from raw data; (2) how to reduce the dependence on the prior knowledge or human labors in feature learning. Inspired by the success of deep learning methods in the feature representation learning community, we propose a malware detection framework which starts with learning rich-features by a novel unsupervised feature learning algorithm Merged Sparse Auto-Encoder (MSAE). In order to extract more compact and discriminative feature from the rich-features to further boost the malware detection capability, a hybrid deep network learning algorithm Stacked Hybrid Learning MSAE and SDAE (SHLMD) is established by further incorporating a classical deep learning method Stacked Denoising Auto-encoders (SDAE). After that, we feed the feature learned by MSAE and SHLMD respectively to classification algorithms, e.g., Support Vector Machine (SVM) or K-NearestNeighbor (KNN), to train a malware detection model. Evaluation results on two real-world datasets demonstrate that SHLMD achieves 94.46 and 90.57 percent accuracy respectively, which outperforms the classical unsupervised feature representation learning Sparse Auto-encoder (SAE). MSAE performs similarly to SAE. SHLMD can further improve the performance of MSAE and the supervised fine-tuned method SDAE. Besides, we compare the performance of our methods with that of state-of-the-art detection approaches, including classical deep-learning-based methods. Extensive experiments show that our proposed methods are effective enough to detect Android malware.","1558-2191","","10.1109/TKDE.2021.3067658","National Key R&D Program of China(grant numbers:2017YFB1400700); National Natural Science Foundation of China(grant numbers:U1736216,61802154,U1836116); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9387579","Feature extraction or construction;machine learning;modeling and prediction;neural nets","Malware;Feature extraction;Smart phones;Deep learning;Static analysis;Learning systems;Support vector machines","Android (operating system);deep learning (artificial intelligence);invasive software;nearest neighbour methods;pattern classification;support vector machines;unsupervised learning","Android malware detection;data-driven malware detection methods;deep learning methods;feature representation learning community;human labors;hybrid deep network framework;K-nearest neighbor;KNN;malicious software;merged sparse auto-encoder;MSAE;SDAE;SHLMD;stacked hybrid learning;support vector machine;SVM;unsupervised feature learning;unsupervised feature representation","","4","","52","IEEE","26 Mar 2021","","","IEEE","IEEE Journals"
"Where to Go Next: A Spatio-Temporal Gated Network for Next POI Recommendation","P. Zhao; A. Luo; Y. Liu; J. Xu; Z. Li; F. Zhuang; V. S. Sheng; X. Zhou","Institute of Artificial Intelligence, School of Computer Science and Technology, Soochow University, Suzhou, China; Institute of Artificial Intelligence, School of Computer Science and Technology, Soochow University, Suzhou, China; Rutgers University, Piscataway, NJ, USA; Institute of Artificial Intelligence, School of Computer Science and Technology, Soochow University, Suzhou, China; Institute of Artificial Intelligence, School of Computer Science and Technology, Soochow University, Suzhou, China; Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, China; Department of Computer Science, Texas Tech University, Lubbock, TX, USA; University of Queensland, Brisbane, QLD, Australia","IEEE Transactions on Knowledge and Data Engineering","1 Apr 2022","2022","34","5","2512","2524","Next Point-of-Interest (POI) recommendation which is of great value to both users and POI holders is a challenging task since complex sequential patterns and rich contexts are contained in extremely sparse user check-in data. Recently proposed embedding techniques have shown promising results in alleviating the data sparsity issue by modeling context information, and Recurrent Neural Network (RNN) has been proved effective in the sequential prediction. However, existing next POI recommendation approaches train the embedding and network model separately, which cannot fully leverage rich contexts. In this paper, we propose a novel unified neural network framework, named NeuNext, which leverages POI context prediction to assist next POI recommendation by joint learning. Specifically, the Spatio-Temporal Gated Network (STGN) is proposed to model personalized sequential patterns for users’ long and short term preferences in the next POI recommendation. In the POI context prediction, rich contexts on POI sides are used to construct graph, and enforce the smoothness among neighboring POIs. Finally, we jointly train the POI context prediction and the next POI recommendation to fully leverage labeled and unlabeled data. Extensive experiments on real-world datasets show that our method outperforms other approaches for next POI recommendation in terms of Accuracy and MAP.","1558-2191","","10.1109/TKDE.2020.3007194","National Natural Science Foundation of China(grant numbers:61876117,6187- 6217,61772356,61728205,61972069,61872258); Priority Academic Program Development of Jiangsu Higher Education Institutions; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9133505","Next POI recommendation;POI context prediction;joint learning","Logic gates;Context modeling;Data models;Recurrent neural networks;Task analysis;Markov processes","learning (artificial intelligence);neural nets;recommender systems;recurrent neural nets","Spatio-Temporal Gated Network;next POI recommendation;next Point-of-Interest recommendation;Recurrent Neural Network;POI recommendation approaches;network model;leverage rich contexts;neural network framework;POI context prediction;POI sides;neighboring POIs","","54","","51","IEEE","6 Jul 2020","","","IEEE","IEEE Journals"
"A Self-Play and Sentiment-Emphasized Comment Integration Framework Based on Deep Q-Learning in a Crowdsourcing Scenario","H. Rong; V. S. Sheng; T. Ma; Y. Zhou; M. Al-Rodhaan","School of Artificial Intelligence, Nanjing University of Information Science and Technology, Jiangsu, Nanjing, China; Department of Computer Science, Texas Tech University, Lubbock, TX, USA; School of Computer and Software, Nanjing University of Information Science and Technology, Jiangsu, Nanjing, China; Department of Computer Science and Software Engineering, Auburn University, Auburn, AL, USA; College of Computer and Information Sciences, King Saud University, Riyadh, Kingdom of Saudi Arabia","IEEE Transactions on Knowledge and Data Engineering","3 Feb 2022","2022","34","3","1021","1037","Crowdsourcing is a hotspot research field which can facilitate machine learning by collecting labels to train models. Consequently, the state-of-the-art research efforts in crowdsourcing focus on truth inference or label integration, to remove inconsistent labels or to alleviate biased labeling. In turn, the integrated labels will be used to fine-tune machine learning models. Particularly, in this paper, we change the target of truth inference in crowdsourcing from discrete labels to multiple comments given by online participants, that is, the integration of the crowdsourced comments. For such a goal, we propose a Self-play and Sentiment-Emphasized Comment Integration Framework (SSECIF), based on deep Q-learning, with three unique features. First, our framework SSECIF can generate the comment integration in a totally self-play way, without relying on the ground truth generated by human effort. Second, the integrated comment generated by SSECIF can include salient content with low redundancy. Third, the proposed framework SSECIF has emphasized, with a higher intensity, the sentiment in the integrated comment, in order to reflect the attitude or opinion more obviously. Extensive evaluation on real-world datasets demonstrates that SSECIF has achieved the best overall performance in terms of both effectiveness and efficiency, compared with the state-of-the-art methods.","1558-2191","","10.1109/TKDE.2020.2993272","National Natural Science Foundation of China(grant numbers:U1736105,61876217); China Scholarship Council; Female Center for Scientific and Medical Colleges, King Saud University; Deanship of Scientific Research; King Saud University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9090285","Crowdsourcing;comment integration;reinforcement learning;deep Q-learning;sentiment analysis","Crowdsourcing;Task analysis;Machine learning;Redundancy;Computational modeling;Labeling;Learning (artificial intelligence)","crowdsourcing;deep learning (artificial intelligence);sentiment analysis","deep Q-learning;truth inference;label integration;machine learning;SSECIF;self-play and sentiment-emphasized comment integration framework;crowdsourcing","","2","","48","IEEE","8 May 2020","","","IEEE","IEEE Journals"
"Forecasting People’s Needs in Hurricane Events from Social Network","L. Nguyen; Z. Yang; J. Li; Z. Pan; G. Cao; F. Jin","Department of Computer Science, Texas Tech University, Lubbock, TX, USA; Department of Computer Science, Texas Tech University, Lubbock, TX, USA; Department of Civil, Environmental, and Construction Engineering, Texas Tech University, Lubbock, TX, USA; Kinetica DB Inc., Arlington, VA, USA; Department of Geosciences, Texas Tech University, Lubbock, TX, USA; Department of Computer Science, Texas Tech University, Lubbock, TX, USA","IEEE Transactions on Big Data","13 Jan 2022","2022","8","1","229","240","Social networks can serve as a valuable communication channel for asking for help, offering assistance, and coordinating rescue activities in disaster because it allows users to continuously update critical information in the fast-changing disaster environment. This paper presents a novel sequence to sequence based framework for forecasting people’s needs during disasters using social media and weather data. It consists of two Long Short-Term Memory (LSTM) models, one of which encodes input sequences of weather information and the other plays as a conditional decoder that decodes the encoded vector and forecasts the survivors’ needs. Case studies using data collected during Hurricane Sandy in 2012, Hurricane Harvey and Hurricane Irma in 2017 demonstrate that the proposed approach outperformed the statistical language model n-gram, LSTM generative model, and convolutional neural network (CNN) based model. This research indicates its great promise for enhancing disaster management such as evacuation planning and commodity delivery.","2332-7790","","10.1109/TBDATA.2019.2941887","National Science Foundation(grant numbers:CNS-1737634); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8839842","Disaster relief;needs forecasting;concern flow;LSTM;hurricane events;sequence to sequence model","Hurricanes;Forecasting;Predictive models;Twitter;Weather forecasting","disasters;emergency management;recurrent neural nets;social networking (online);storms","rescue activities;critical information;disaster environment;sequence based framework;forecasting people;social media;weather data;long short-term memory models;input sequences;weather information;conditional decoder;encoded vector;hurricane Sandy;hurricane Harvey;hurricane Irma;statistical language model n-gram;LSTM generative model;convolutional neural network based model;disaster management;hurricane events;social network;valuable communication channel","","13","","36","IEEE","16 Sep 2019","","","IEEE","IEEE Journals"
"A Review of Publicly Patient-Centered Alzheimer’s disease datasets","R. Wang; C. -L. Shen; V. Sheng; H. Wu; Z. Toler; S. Temesgen","Department of Pathology, Texas Tech University Health Sciences Center, Lubbock, TX, USA; Department of Pathology, Texas Tech University Health Sciences Center, Lubbock, TX, USA; Department of Computer Science, Texas Tech University, Lubbock, TX, USA; Department of Mathematics and Computer and Sciencedept, West Virginia State University Institute, WV, USA; Department of Mathematics and Computer and Sciencedept, West Virginia State University Institute, WV, USA; Department of Mathematics and Computer and Sciencedept, West Virginia State University Institute, WV, USA","2021 International Conference on Computational Science and Computational Intelligence (CSCI)","22 Jun 2022","2021","","","1182","1188","Alzheimer’s disease (AD), the major cause of dementia, is becoming a global health issue. This review includes major publicly available patient-centered AD datasets, covering three main categories of data – clinical, imaging, and genetic, to facilitate researchers to pinpoint suitable dataset(s) for their studies. It overviews these datasets with their associated studies, data storage locations, and most importantly, details of the data such as cognitive exam results, biomarker measurements, neurological images, genotyping and expression data. They provide an informative and useful portal for AD or Machine Learning researchers and can help them pinpoint the desired datasets suitable for their studies.","","978-1-6654-5841-2","10.1109/CSCI54926.2021.00247","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9799273","Alzheimer’s disease;dataset;big data","Scientific computing;Memory;Machine learning;Genetics;Alzheimer's disease;Portals;Computational intelligence","diseases;genetics;learning (artificial intelligence);medical image processing;neurophysiology","genotyping;expression data;desired datasets;publicly patient-centered alzheimer;global health issue;publicly available patient-centered;suitable dataset;associated studies;data storage locations;neurological images","","","","16","IEEE","22 Jun 2022","","","IEEE","IEEE Conferences"
"Abstractive Text Summarization via Stacked LSTM","I. Siddhartha; H. Zhan; V. S. Sheng","Department of Computer Science, Texas Tech University, Lubbock, TX, USA; Department of Computer Science, Texas Tech University, Lubbock, TX, USA; Department of Computer Science, Texas Tech University, Lubbock, TX, USA","2021 International Conference on Computational Science and Computational Intelligence (CSCI)","22 Jun 2022","2021","","","437","442","In the past, there have been many models proposed for text summarization via sequence to sequence training (seq2seq), attention mechanism, and transformers. Although these methods achieve an advance regarding the performance, these models fail to create a more complex feature representation of the current input and consequently gain inferior performance for modeling the long staggered sentences and modeling the complex inter-sentence dependencies. In order to address this issue, we utilize a more complex feature representation for summarization via stacked LSTM. In this case, the main reason for stacking LSTM is to allow for greater model complexity. For a simple encoder, we stack layers to create a hierarchical feature representation with attention. We generate the text summaries for any test text in terms of predicting the target sequence. With the proposed method, we achieve a better performance compared to the existing state-of-the-art phrase-based system on the task of text summarization on gigaword dataset. Furthermore, Experimental results on this dataset show that our framework performs well in terms of various ROUGE scores.","","978-1-6654-5841-2","10.1109/CSCI54926.2021.00143","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9798919","Summarization;Seq2seq;Stacked LSTM","Training;Measurement;Scientific computing;Computational modeling;Stacking;Predictive models;Transformers","language translation;natural language processing;text analysis","abstractive text summarization;attention mechanism;complex feature representation;complex inter-sentence dependencies;current input;greater model complexity;hierarchical feature representation;inferior performance;long staggered sentences;seq2seq;sequence training;stack layers;stacked LSTM;target sequence;test text;text summaries","","","","27","IEEE","22 Jun 2022","","","IEEE","IEEE Conferences"
"Modeling Follow-Unfollow Mechanism in Social Networks with Evolutionary Game","J. Chen; M. S. Hossain; A. Serwadda; F. Han","Department of Math and Computer Science, Dickinson State University, Dickinson, ND, USA; Computer Science Department, Southern Connecticut State University, New Haven, CT, USA; Department of Computer Science, Texas Tech University, Lubbock, TX, USA; The Hilltop Institute, University of Maryland, Baltimore County, Baltimore, MD, USA","2021 IEEE International Conference on Agents (ICA)","4 Mar 2022","2021","","","37","40","In online social networks (OSN), followers count is a sign of the social influence of an account. Some users expect to increase the followers count by following more accounts. However, in reality more followings do not generate more followers. In this paper, we propose a two player follow-unfollow game model and then introduce a factor for promoting cooperation. Based on the two player follow-unfollow game, we create an evolutionary follow-unfollow game with more players to simulate a miniature social network. We design an algorithm and conduct the simulation. From the simulation, we find that our algorithm for the evolutionary follow-unfollow game is able to converge and produce a stable network. Results obtained with different values of the cooperation promotion factor show that the promotion factor increases the total connections in the network especially through increasing the number of the follow follow connections.","","978-1-6654-0716-8","10.1109/ICA54137.2021.00013","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9723814","social network;evolutionary game;follow unfollow mechanism","Correlation;Social networking (online);Conferences;Games","game theory;social networking (online)","online social networks;followers count;social influence;player follow-unfollow game model;miniature social network;modeling follow-unfollow mechanism;evolutionary game;cooperation promotion factor","","","","12","IEEE","4 Mar 2022","","","IEEE","IEEE Conferences"
"Device Identification for IoT Security","P. Roemsri; R. Hewett","Department of Computer Science, Texas Tech University, Lubbock, Texas, USA; Department of Computer Science, Texas Tech University, Lubbock, Texas, USA","2021 IEEE 6th International Conference on Signal and Image Processing (ICSIP)","28 Jan 2022","2021","","","866","870","IoT (Internet of things) devices have become integral parts of our everyday life to function especially in smart homes/offices/cities. However, their huge varieties and scales make it hard to manage the security and the quality of services of IoT networks with generic policies. Furthermore, IoT devices owned by employees can unknowingly endanger the security and integrity of the network from other IoT entities they are connected to. Fortunately, IoT devices of the same type tend to have similar vulnerabilities making attack prevention more manageable. Thus, identifying the device types is crucial for IoT security. This paper presents an approach to building a classifier to identify different types of IoT devices using various machine learning techniques. While most existing empirical studies use network traffic data, we use communication protocol data. The paper describes the data collection/treatment, the experiments using four machine learning techniques and compares our proposed approach with an existing work based on the Jaccard similarity measure. The results show that both approaches have competitive accuracy of up to 99% but the Jaccard approach does not scale well (e.g., about 1000 times more than the training time of our approach).","","978-1-6654-3904-6","10.1109/ICSIP52628.2021.9688598","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9688598","IoT devise;Internet of things;Machine learning;Decision tree;Random forest;Extremely randomized trees;Naïve Bayes;Categorical Naïve Bayes;Jaccard Index;Levenshtein distance;Local network","Training;Protocols;Image processing;Machine learning;Telecommunication traffic;Quality of service;Forestry","computer network security;Internet of Things;learning (artificial intelligence);protocols;telecommunication traffic","communication protocol data;Jaccard similarity measure;data collection-treatment;Internet of Things;network traffic data;machine learning techniques;IoT entities;IoT devices;IoT networks;IoT security;device identification","","","","24","IEEE","28 Jan 2022","","","IEEE","IEEE Conferences"
"Estimating crowd-worker's reliability with interval-valued labels to improve the quality of crowdsourced work","M. Spurling; C. Hu; H. Zhan; V. S. Sheng","Dept. of Comp. Sci. & Eng., University of Central Arkansas, Conway, AR, USA; Dept. of Comp. Sci. & Eng., University of Central Arkansas, Conway, AR, USA; Dept. of Computer Science, Texas Tech University, Lubbock, TX, USA; Dept. of Computer Science, Texas Tech University, Lubbock, TX, USA","2021 IEEE Symposium Series on Computational Intelligence (SSCI)","24 Jan 2022","2021","","","01","08","With inputs from human crowds, usually through the Internet, crowdsourcing has become a promising methodology in AI and machine learning for applications that require human knowledge. Researchers have recently proposed interval-valued labels (IVLs), instead of commonly used binary-valued ones, to manage uncertainty in crowdsourcing [19]. However, that work has not yet taken the crowd worker's reliability into consideration. Crowd workers usually come with various social and economic backgrounds, and have different levels of reliability. To further improve the overall quality of crowdsourcing with IVLs, this work presents practical methods that quantitatively estimate worker's reliability in terms of his/her correctness, confidence, stability, and predictability from his/her IVLs. With worker's reliability, this paper proposes two learning schemes: weighted interval majority voting (WIMV) and weighted preferred matching probability (WPMP). Computational experiments on sample datasets demonstrate that both WIMV and WPMP can significantly improve learning results in terms of higher precision. accuracy. and F1 -score than other methods.","","978-1-7281-9048-8","10.1109/SSCI50451.2021.9660043","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9660043","crowdsourcing;interval-valued label;worker's reliability;correctness;confidence;stability;predictability","Crowdsourcing;Economics;Uncertainty;Machine learning;Reliability;Computational intelligence","crowdsourcing;Internet;learning (artificial intelligence);personnel;probability;quality control","WPMP;WIMV;weighted interval majority voting;Internet;crowd-worker reliability estimation;economic backgrounds;social backgrounds;human knowledge;machine learning;human crowds;crowdsourced work;interval-valued labels;weighted preferred matching probability;IVL","","","","39","IEEE","24 Jan 2022","","","IEEE","IEEE Conferences"
"CSRDA: Cost-sensitive Regularized Dual Averaging for Handling Imbalanced and High-dimensional Streaming Data","Z. Chen; Z. Fang; V. Sheng; A. Edwards; K. Zhang","Department of Computer Science, Xavier University of Louisiana; TBiostatistics, School of Public Health, LSU Health Sciences Center; Department of Computer Science, Texas Tech University; Department of Computer Science, Xavier University of Louisiana; Department of Computer Science, Xavier University of Louisiana","2021 IEEE International Conference on Big Knowledge (ICBK)","14 Jan 2022","2021","","","164","173","Class-imbalance is one of the most challenging problems in online learning due to its impact on the prediction capability of data stream mining models. Most existing approaches for online learning lack an effective mechanism to handle high-dimensional streaming data with skewed class distributions, resulting in insufficient model interpretation and deterioration of online performance. In this paper, we develop a cost-sensitive regularized dual averaging (CSRDA) method to tackle this problem. Our proposed method substantially extends the influential regularized dual averaging (RDA) method by formulating a new convex optimization function. Specifically, two $R$ 1 -norm regularized cost-sensitive objective functions are directly optimized, respectively. We then theoretically analyze CSRDA's regret bounds and the bounds of primal variables. Thus, CSRDA benefits from achieving a theoretical convergence of balanced cost and sparsity for severe imbalanced and high-dimensional streaming data mining. To validate our method, we conduct extensive experiments on six benchmark streaming datasets with varied imbalance ratios. The experimental results demonstrate that, compared to other baseline methods, CSRDA not only improves classification performance, but also successfully captures sparse features more effectively, hence has better interpretability.","","978-1-6654-3858-2","10.1109/ICKG52313.2021.00031","NIH(grant numbers:U54MD007595); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9667745","Online learning;Streaming data;Imbalance ratio;Cost-sensitive metrics;Sparsity","Costs;Benchmark testing;Predictive models;Linear programming;Data models;Stability analysis;Convex functions","data mining;learning (artificial intelligence);pattern classification;sampling methods","high-dimensional streaming data;class-imbalance;online learning;data stream mining models;skewed class distributions;insufficient model interpretation;online performance;cost-sensitive regularized dual averaging method;influential regularized dual averaging method;convex optimization function;norm regularized cost-sensitive;CSRDA's regret bounds;CSRDA benefits;balanced cost;data mining","","2","","35","IEEE","14 Jan 2022","","","IEEE","IEEE Conferences"
"Gated Graph Neural Networks (GG-NNs) for Abstractive Multi-Comment Summarization","H. Zhan; K. Zhang; C. Hu; V. S. Sheng","Department of Computer Science, Texas Tech University; Department of Computer Science, Xavier University of Louisiana; Department of Computer Science, University of Central Arkansas; Department of Computer Science, Texas Tech University","2021 IEEE International Conference on Big Knowledge (ICBK)","14 Jan 2022","2021","","","323","330","Summarization of long sequences into a concise statement is a core problem in natural language processing, which requires a non-trivial understanding of the weakly structured text. Therefore, integrating crowdsourced multiple users' comments into a concise summary is even harder because (1) it requires transferring the weakly structured comments to structured knowledge. Besides, (2) the users comments are informal and noisy. In order to capture the long-distance relationships in staggered long sentences, we propose a neural multi-comment summarization (MCS) system that incorporates the sentence relationships via graph heuristics that utilize relation knowledge graphs, i.e., sentence relation graphs (SRG) and approximate dis-course graphs (ADG). Motivated by the promising results of gated graph neural networks (GG- NNs) on highly structured data, we develop a GG-NNs with sequence encoder that incorporates SRG or ADG in order to capture the sentence relationships. Specifi-cally, we employ the GG- NNs on both relation knowledge graphs, with the sentence embeddings as the input node features and the graph heuristics as the edges' weights. Through multiple layer-wise propagations, the GG- NNs generate the salience for each sentence from high-level hidden sentence features. Consequently, we use a greedy heuristic to extract salient users' comments while avoiding the noise in comments. The experimental results show that the proposed MCS improves the summarization performance both quantitatively and qualitatively.","","978-1-6654-3858-2","10.1109/ICKG52313.2021.00050","US National Science Foundation(grant numbers:OIA 1946391); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9667693","graph neural network;multi-comment summarization;graph data structure","Knowledge engineering;Conferences;Redundancy;Artificial neural networks;Logic gates;Feature extraction;Data structures","graph theory;learning (artificial intelligence);natural language processing;neural nets;text analysis","gated graph neural networks;GG-NNs;abstractive multicomment summarization;long sequences;concise statement;natural language processing;nontrivial understanding;weakly structured text;multiple users;weakly structured comments;structured knowledge;users comments;long-distance relationships;staggered long sentences;multicomment summarization system;sentence relationships;graph heuristics;relation knowledge graphs;approximate dis-course graphs;GG- NNs;highly structured data;sentence embeddings;high-level hidden sentence features;salient users;summarization performance","","","","37","IEEE","14 Jan 2022","","","IEEE","IEEE Conferences"
"The Applications of Blockchains in Addressing the Integration and Security of IoT Systems: A Survey","Z. A. Khan; A. S. Namin","Department of Computer Science, Texas Tech University; Department of Computer Science, Texas Tech University","2021 IEEE International Conference on Big Data (Big Data)","13 Jan 2022","2021","","","2421","2426","The Internet of Things (IoT) has already changed our daily lives by integrating smart devices together towards delivering high quality services to its clients. These devices when integrated together form a network through which massive amount of data can be produced, transferred, and shared. A critical concern is the security and integrity of such a complex platform to ensure the sustainability and reliability of these IoT-based systems. Blockchain is an emerging technology that has demonstrated its unique features and capabilities for different problems and application domains including IoT-based systems. This survey paper reviews the adaptation of Blockchain in the context of IoT to represent how this technology is capable of addressing the integration and security problems of devices connected to IoT systems. The innovation of this survey is that we present a survey based upon the integration approaches and security issues of IoT data and discuss the role of Blockchain in connection with these issues.","","978-1-6654-3902-2","10.1109/BigData52589.2021.9671299","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9671299","Internet of Things;Vulnerabilities;Blockchain;Classification of IoT Integration","Technological innovation;Conferences;Big Data;Blockchains;Security;Internet of Things;Reliability","blockchains;computer network security;data integration;Internet of Things","IoT systems;smart devices;IoT-based systems;blockchain;security problems;security issues;IoT data;integration approach","","2","","52","IEEE","13 Jan 2022","","","IEEE","IEEE Conferences"
"RDNet: Deep Learning Model for Predicting pH H20 and pHKCl from Soil Vis-NIR Spectra","V. Pham; D. C. Weindorf; T. Dang","Department of Computer Science, Sam Houston State University, Huntsville, Texas, USA; Department of Earth and Atmospheric Sciences, Central Michigan University, Mount Pleasant, Michigan, USA; Department of Computer Science, Texas Tech University, Lubbock, Texas, USA","2021 IEEE International Conference on Big Data (Big Data)","13 Jan 2022","2021","","","3436","3445","Soil properties are vital to profiling and utilizing soil resources. Conventional approaches to measurements of soil properties often involve costly, environmental-unfriendly, and time-consuming laboratory procedures. Conversely, machine learning (ML) and deep learning (DL) are gaining traction in giving rapid, non-destructive, and cost-saving alternatives to predictions of soil properties. These ML/DL models are convenient and fast because they utilize spectral data, such as visible and near-infrared (Vis-NIR) spectra, that can be easily collected using proximal sensors for their training and prediction purposes. However, existing ML/DL approaches to this problem pose several limitations, such as having small sample sizes, needing to divide the sample data into local areas to increase accuracy, and having relatively low accuracy. Therefore, this work experiments various ML/DL methods that leverage Vis-NIR spectra collected from a rather large number of soil samples distributed all over the world to predict $p{H_{{H_2}O}}$ and pHKCl. We then propose a DL method, called RDNet, that outperforms the other existing approaches. We also utilize visualizations to verify if the proposed model learns legitimate information from the training data.","","978-1-6654-3902-2","10.1109/BigData52589.2021.9671527","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9671527","machine learning;deep learning;soil Vis-NIR spectra;soil property predictions","Training;Deep learning;Soil properties;Neural networks;Stacking;Training data;Data visualization","learning (artificial intelligence);regression analysis;soil","deep learning model;predicting pH;KCl;soil Vis-NIR spectra;soil properties;soil resources;time-consuming laboratory procedures;prediction purposes;leverage Vis-NIR spectra;soil samples","","1","","34","IEEE","13 Jan 2022","","","IEEE","IEEE Conferences"
"A Comparison of TCN and LSTM Models in Detecting Anomalies in Time Series Data","S. Gopali; F. Abri; S. Siami-Namini; A. S. Namin","Department of Computer Science, Texas Tech University; Department of Computer Science, Texas Tech University; School of Planning and Public Policy, Rutgers University; Department of Computer Science, Texas Tech University","2021 IEEE International Conference on Big Data (Big Data)","13 Jan 2022","2021","","","2415","2420","There exist several data-driven approaches that enable us model time series data including traditional regression-based modeling approaches (i.e., ARIMA). Recently, deep learning techniques have been introduced and explored in the context of time series analysis and prediction. A major research question to ask is the performance of these many variations of deep learning techniques in predicting time series data. This paper compares two prominent deep learning modeling techniques. The Recurrent Neural Network (RNN)-based Long Short-Term Memory (LSTM) and the convolutional Neural Network (CNN)-based Temporal Convolutional Networks (TCN) are compared and their performance and training time are reported. According to our experimental results, both modeling techniques per-form comparably having TCN-based models outperform LSTM slightly. Moreover, the CNN-based TCN model builds a stable model faster than the RNN-based LSTM models.","","978-1-6654-3902-2","10.1109/BigData52589.2021.9671488","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9671488","Temporal convolutional network (TCN);Long Short-Term Memory (LSTM);Anomaly detection","Deep learning;Training;Recurrent neural networks;Conferences;Time series analysis;Big Data;Data models","convolutional neural nets;data handling;deep learning (artificial intelligence);learning (artificial intelligence);recurrent neural nets;regression analysis;time series","time series data;data-driven approaches;traditional regression-based modeling approaches;deep learning techniques;time series analysis;deep learning modeling techniques;training time;TCN-based models;CNN-based TCN model;stable model;RNN-based LSTM models;recurrent neural network;long short-term memory;convolutional neural network;temporal convolutional networks","","4","","20","IEEE","13 Jan 2022","","","IEEE","IEEE Conferences"
"OnlineDC: Leveraging Temporal Driving Behavior to Facilitate Driver Classification","H. Abu-Gellban; Y. Zhuang; L. Nguyen; F. Jin; Z. Zhang","Department of Computer Science, Texas Tech University; Department of Computer Science, Texas Tech University; Computer Science and Data Science, Meharry Medical College; Department of Statistics, George Washington University; Computer Science Division, Clemson University","2021 IEEE International Conference on Big Data (Big Data)","13 Jan 2022","2021","","","2857","2866","Driver classification is used recently for vehicle anti-burglary and fake driver accounts based on driving behavior. Anti-burglary is a challenging problem as it leans on external devices to defend against vehicle theft. Several researchers analyzed the driving behavior to identify drivers, but they faced several challenges to produce a stable model for the cold start problem and for medium-long sequences. In addition, some approaches had an unpleasant performance when the action space increased (> 2 drivers). In this paper, we propose a novel approach named OnlineDC (Online Driver Classification), which leverages temporal driving behavior to identify a human subject behind the wheel. Our method utilizes the Gated Recurrent Unit (GRU) and the ResNet with the Squeeze-Excite blocks (SE) to analyze the long-short term patterns of driving behaviors. Moreover, we fostered the performance by building and applying the Feature Generation (FG) algorithm to extract spectral, temporal, and statistical features from the sensing data of vehicles. We conducted extensive experiments to show how our approach outperformed state-of-the-art baseline methods. The results also showed that our solution could resolve the cold-start problem for short patterns.","","978-1-6654-3902-2","10.1109/BigData52589.2021.9671591","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9671591","Driver Classification;Cybersecurity;Deep Learning;Neural Network;Internet of Things","Performance evaluation;Neural networks;Wheels;Big Data;Feature extraction;Classification algorithms;Sensors","learning (artificial intelligence);recommender systems;statistical analysis;traffic engineering computing","OnlineDC;temporal driving behavior;vehicle anti-burglary;fake driver accounts;vehicle theft;cold start problem;medium-long sequences;Online Driver Classification;spectral features;temporal features;statistical features;cold-start problem","","1","","58","IEEE","13 Jan 2022","","","IEEE","IEEE Conferences"
"Extraction of Respiration Rate from Wrist ECG Signals","M. Rahman; B. I. Morshed","Department of Computer Science, Texas Tech University, Lubbock, USA; Department of Computer Science, Texas Tech University, Lubbock, USA","2021 IEEE 12th Annual Ubiquitous Computing, Electronics & Mobile Communication Conference (UEMCON)","10 Jan 2022","2021","","","0565","0570","Respiratory behavior is one of the important parameters that indicate any physiological changes in human body. However, using a respiration sensor device for continuous monitoring is inconvenient and expensive. In this paper, an approach to acquire the respiration signal from the wrist electrocardiogram (ECG) is proposed. An analog front end (AFE) sampled at 100 Hz is used to collect ECG signals from the wrist to compute and verify the corresponding heart rate (HR) with a commercial ECG device. Signal processing mechanisms are applied on the raw data to denoise the ECG signal. The captured ECG signal is further processed to extract a breathing pattern to calculate a respiration rate (RR) in breath per minute (BPM). The extracted BPMs are compared with a commercial respiration monitor to validate the data by following a protocol at 5 different BPMs (12, 15, 20, 24 and 30). For each BPM, commercial respiration monitor is validated at first. Then, data are taken simultaneously wearing wrist electrodes and commercial respiratory device to validate the performance of our proposed method at different BPMs. The results indicate high accuracy of the proposed system which is low-cost, simpler to implement, can be integrated with a wearable device and remove the demand of any dedicated sensor for RR measurements.","","978-1-6654-0690-1","10.1109/UEMCON53757.2021.9666489","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9666489","wearable;electrocardiogram;respiration rate;signal processing;cyber physical system;mobile health","Wrist;Performance evaluation;Heart rate;Electrodes;Protocols;Wearable computers;Smart healthcare","electrocardiography;medical signal detection;medical signal processing;patient monitoring;pneumodynamics","commercial respiration monitor;wrist electrodes;commercial respiratory device;wearable device;respiration rate;wrist ECG signals;respiratory behavior;physiological changes;human body;respiration sensor device;continuous monitoring;respiration signal;wrist electrocardiogram;commercial ECG device;signal processing mechanisms;captured ECG signal;extracted BPMs","","2","","28","IEEE","10 Jan 2022","","","IEEE","IEEE Conferences"
"ECCH: Erasure Coded Consistent Hashing for Distributed Storage Systems","Y. Xiong; J. Zhou; L. Su; W. Wang; Y. Chen","Chinese Academy of Sciences, Institute of Information Engineering, Beijing, China; Chinese Academy of Sciences, Institute of Information Engineering, Beijing, China; Chinese Academy of Sciences, Institute of Information Engineering, Beijing, China; Chinese Academy of Sciences, Institute of Information Engineering, Beijing, China; Department of Computer Science, Texas Tech University, Lubbock, USA","2021 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Big Data & Cloud Computing, Sustainable Computing & Communications, Social Computing & Networking (ISPA/BDCloud/SocialCom/SustainCom)","22 Dec 2021","2021","","","177","184","In this paper, we propose ECCH, an Erasure Coded Consistent Hashing scheme to make better data placement in distributed storage systems. It combines the inherent advantages of consistent hashing together with the storage-efficiency of erasure coding technology. Specifically, ECCH divides data block stream of files into groups according to block IDs. In each group, it encoded data blocks with additional parity blocks by erasure coding. All encoded blocks in the same group are stored on different nodes with consistent hashing distribution. For node failure or data loss, ECCH locates required data through the ID of missing block in a same group for fast recovery. To deal with node changes, ECCH introduces a design of multi-version hash rings to manage data layout. It can prevent the impact of data migration on erasure coding, while achieving data balance with little data movement. We have implemented ECCH on the Sheepdog, a distributed object-based storage system. Evaluation results show that ECCH can greatly improve the space utilization of hashing-based storage systems, while achieving efficient fault tolerance.","","978-1-6654-3574-1","10.1109/ISPA-BDCloud-SocialCom-SustainCom52081.2021.00036","Beijing Municipal Science and Technology Commission; Chinese Academy of Sciences; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9644781","Consistent hashing;erasure coding;replication;data distribution;fault tolerance","Fault tolerance;Data centers;Cloud computing;Scalability;Layout;Fault tolerant systems;Distributed databases","distributed processing;fault tolerant computing;file organisation;storage management","data placement;data blocks;parity blocks;erasure coding;multiversion hash rings;data layout;data migration;data balance;data movement;distributed object-based storage system;hashing-based storage systems;erasure coded consistent hashing;block ID;ECCH;Sheepdog","","1","","30","IEEE","22 Dec 2021","","","IEEE","IEEE Conferences"
"Machine Learning-based Vulnerability Study of Interpose PUFs as Security Primitives for IoT Networks","B. Thapaliya; K. T. Mursi; Y. Zhuang","Department of Computer Science, Texas Tech University, Lubbock, Texas, USA; Department of Cyber Security, College of Computer Science and Engineering, University of Jeddah, Jeddah, Saudi Arabia; Department of Computer Science, Texas Tech University, Lubbock, Texas, USA","2021 IEEE International Conference on Networking, Architecture and Storage (NAS)","22 Nov 2021","2021","","","1","7","Security is of importance for communication networks, and many network nodes, like sensors and IoT devices, are resource-constrained. Physical Unclonable Functions (PUFs) leverage physical variations of the integrated circuits to produce responses unique to individual circuits and have the potential for delivering security for low-cost networks. But before a PUF can be adopted for security applications, all security vulnerabilities must be discovered. Recently, a new PUF known as Interpose PUF (IPUF) was proposed, which was tested to be secure against reliability-based modeling attacks and machine learning attacks when the attacked IPUF is of small size. A recent study showed IPUFs succumbed to a divide-and-conquer attack, and the attack method requires the position of the interpose bit known to the attacker, a condition that can be easily obfuscated by using a random interpose position. Thus, large IPUFs may still remain secure against all known modeling attacks if the interpose position is unknown to attackers. In this paper, we present a new modeling attack method of IPUFs using multilayer neural networks, and the attack method requires no knowledge of the interpose position. Our attack was tested on simulated IPUFs and silicon IPUFs implemented on FPGAs, and the results showed that many IPUFs which were resilient against existing attacks cannot withstand our new attack method, revealing a new vulnerability of IPUFs by re-defining the boundary between secure and insecure regions in the IPUF parameter space.","","978-1-7281-7744-1","10.1109/NAS51552.2021.9605405","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9605405","physical unclonable function;Interpose PUF;machine learning;neural network;FPGA","Training;Neural networks;Machine learning;Network security;Physical unclonable function;Nonhomogeneous media;Reliability engineering","computer network security;cryptographic protocols;divide and conquer methods;Internet of Things;learning (artificial intelligence);neural nets","security vulnerabilities;interpose PUF;reliability-based modeling attacks;machine learning attacks;attacked IPUF;-conquer attack;interpose bit;random interpose position;modeling attacks;modeling attack method;multilayer neural networks;secure regions;insecure regions;IPUF parameter space;machine learning-based vulnerability study;security primitives;IoT networks;communication networks;network nodes;IoT devices;Physical Unclonable Functions leverage physical variations;integrated circuits;individual circuits;low-cost networks;security applications;simulated IPUF;silicon IPUF","","1","","17","IEEE","22 Nov 2021","","","IEEE","IEEE Conferences"
"Sal-HMAX: An Enhanced HMAX Model in Conjunction With a Visual Attention Mechanism to Improve Object Recognition Task","Z. S. Shariatmadar; K. Faez; A. S. Namin","Electrical Engineering Department, Amirkabir University of Technology, Tehran, Iran; Electrical Engineering Department, Amirkabir University of Technology, Tehran, Iran; Department of Computer Science, Texas Tech University, Lubbock, TX, USA","IEEE Access","24 Nov 2021","2021","9","","154396","154412","The Hierarchical Max-pooling models (HMAX) have demonstrated excellent outperformance when integrated with various computer vision algorithms for the purpose of recognizing objects in images. However, the conventional HMAX model has two main problems: 1) it is computationally expensive to learn base matrixes, especially at layer S2 (matching layer), and 2) the patch selection in the standard HMAX model is randomly selected resulting in generating redundant and uninformed extracted patches. In this paper, a combination of the HMAX model and a selective attention mechanism is proposed to address the aforementioned drawbacks of HMAX models. Applying a selective mechanism of attention filters out unnecessary information and highlights more important and significant parts of a given image., An attention function is used to increase the matching velocity at the S2 layer, since through attention we only consider patches with more details. On the other hand, high operational precision is expected due to the extraction of distinct patches in the training phase of the S2 layer. The results of experiments show that the proposed model outperforms the conventional HMAX model. The proposed model establishes a mean accuracy of 93.7% on the first ten best-classified categories using the Caltech-101 dataset.","2169-3536","","10.1109/ACCESS.2021.3127928","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9614121","HMAX model;saliency map;object recognition;entropy;retinal ganglion cell","Brain modeling;Visualization;Feature extraction;Object recognition;Biological system modeling;Standards;Image color analysis","computer vision;feature extraction;image classification;image representation;learning (artificial intelligence);object recognition","uninformed extracted patches;selective attention mechanism;selective mechanism;attention filters;attention function;conventional HMAX model;enhanced HMAX model;visual attention mechanism;improve object recognition task;Hierarchical Max-pooling models;computer vision algorithms;layer S2;matching layer;patch selection;standard HMAX model;redundant extracted patches","","","","63","CCBYNCND","12 Nov 2021","","","IEEE","IEEE Journals"
"TimeRadar: Visualizing the Dynamics of Multivariate Communities via Timeline Views","N. V. T. Nguyen; J. Hass; T. Dang","Computer Science Department, Texas Tech University, Lubbock, TX, USA; Office of the CTIO, Dell Technologies, Texas, USA; Computer Science Department, Texas Tech University, Lubbock, TX, USA","2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)","9 Sep 2021","2021","","","350","356","Analyzing temporal event sequences play an important role in many application domains, such as workload behavior analysis, hardware fault diagnosis, and natural disaster resilience. As data volume keeps growing, real-world temporal event sequences are often noisy, missing, and complex, thus making it a daunting task to convey much of the information from a comprehensive overview for analysts. This work proposes a visual approach based on clustering and superimposing to construct a high-level overview of sequential event data while balancing the amount of information and its cardinality. We also implement an interactive prototype, called TimeRadar, that allows domain analysts to simultaneously analyze sequence clustering, extract distribution patterns, drill multiple levels of detail to accelerate the analysis. This work aims to provide an abstracted view of temporal event sequences where significant events are highlighted. The TimeRadar is demonstrated through case studies with real-world temporal datasets of various sizes.","0730-3157","978-1-6654-2463-9","10.1109/COMPSAC51774.2021.00057","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9529749","temporal event sequence;event sequence;information visualization;radar chart;superimposing technique;high dimensional data;high level overview;visualization technique;timeradar visualization;high performance computing center;temporal event sequence analysis","Fault diagnosis;Visualization;Conferences;Prototypes;Software;Hardware;Noise measurement","data analysis;data visualisation;pattern clustering","data volume;temporal event sequences;visual approach;superimposing;high-level overview;sequential event data;interactive prototype;domain analysts;sequence clustering;abstracted view;multivariate communities;timeline views;application domains;workload behavior analysis;hardware fault diagnosis;natural disaster resilience;TimeRadar","","1","","27","IEEE","9 Sep 2021","","","IEEE","IEEE Conferences"
"COVID-19 SIHR Modeling and Dynamic Analysis","Z. Pan; T. Wang; Y. Zhang","Department of Computer Science, Texas Tech University; Department of Mathematical Sciences, University of Cincinnati; Department of Computer Science, Texas Tech University","2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)","9 Sep 2021","2021","","","1711","1716","We propose a novel disease transmission model: the Susceptible-Infected-Hospitalized-Recovered (SIHR) model, which is a modification of the classical SIR model commonly used in modeling the spread of infectious diseases for understanding epidemic duration, number of infected people throughout the duration, and peak number of infected people etc. More specifically, we introduce a new hospitalization state, denoted by H, between the I and R state in the SIR model, and such new state is constrained by the number of hospital beds denoted by M. We perform study on the dynamics of the novel SIHR model. Our numerical results based on the COVID-19 dataset from Wuhan, China show that the SIHR model illustrates much better fitting with the dataset than the classic SIR model. The computational results demonstrate how and when one should increase the hospital beds number M based on detailed numerical analysis.","0730-3157","978-1-6654-2463-9","10.1109/COMPSAC51774.2021.00255","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9529717","","COVID-19;Analytical models;Hospitals;Pandemics;Numerical analysis;Infectious diseases;Computational modeling","diseases;epidemics;hospitals;numerical analysis","COVID-19 SIHR modeling;dynamic analysis;novel disease transmission model;Susceptible-Infected-Hospitalized-Recovered model;classical SIR model;infectious diseases;epidemic duration;infected people;peak number;hospitalization state;novel SIHR model;COVID-19 dataset;classic SIR model;hospital beds number M","","","","20","IEEE","9 Sep 2021","","","IEEE","IEEE Conferences"
"Attack Prediction using Hidden Markov Model","S. Dass; P. Datta; A. S. Namin","Department of Computer Science, Texas Tech University, Lubbock, Texas, USA; Department of Computer Science, Texas Tech University, Lubbock, Texas, USA; Department of Computer Science, Texas Tech University, Lubbock, Texas, USA","2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)","9 Sep 2021","2021","","","1695","1702","It is important to predict any adversarial attacks and their types to enable effective defense systems. Often it is hard to label such activities as malicious ones without adequate analytical reasoning. We propose the use of Hidden Markov Model (HMM) to predict the family of related attacks. Our proposed model is based on the observations often agglomerated in the form of log files and from the target or the victim’s perspective. We have built an HMM-based prediction model and implemented our proposed approach using Viterbi algorithm, which generates a sequence of states corresponding to stages of a particular attack. As a proof of concept and also to demonstrate the performance of the model, we have conducted a case study on predicting a family of attacks called Action Spoofing.","0730-3157","978-1-6654-2463-9","10.1109/COMPSAC51774.2021.00253","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9529370","Hidden Markov Model;Viterbi algorithm;At-tack prediction;Attack family;Action spoofing","Viterbi algorithm;Computational modeling;Conferences;Software algorithms;Hidden Markov models;Predictive models;Prediction algorithms","hidden Markov models;inference mechanisms;security of data","attack prediction;hidden Markov model;adversarial attacks;effective defense systems;malicious ones;adequate analytical reasoning;related attacks;HMM-based prediction model;particular attack;HMM;Viterbi algorithm;action spoofing","","2","","19","IEEE","9 Sep 2021","","","IEEE","IEEE Conferences"
"Finite Element Simulation of Inkjet Printed Flexible Parallel Plate MIM Capacitors on Polyimide Film","M. M. R. Momota; A. Mohapatra; B. I. Morshed","Department of Computer Science, Texas Tech University, Lubbock, TX, USA; Engineering and Computer Science, California State University at Fullerton, Los Angeles, CA, USA; Department of Computer Science, Texas Tech University, Lubbock, TX, USA","2021 IEEE International Conference on Electro Information Technology (EIT)","26 Jul 2021","2021","","","255","259","As the potential usage of flexible electronics inkjet printing (IJP) is rapidly growing in flexible electronics, we present a Finite Element Analysis (FEA) with electrostatic modeling of a Metal-Insulator-Metal (MIM) type parallel plate capacitor using COMSOL Multiphysics designed for application in flexible electronic circuits. In this study, silver was used as the conductive metal parallel plates and Poly(4-vinylphenol) (PVP) was used as the insulator material. We compared our simulated result with IJP parallel plate capacitors where the bottom and top plate was printed with JS B40 silver ink and dielectric layer was printed with PVP dielectric ink. We also compared our simulation results with ideal calculated capacitance values. Our simulated results are promising and matched closely with the calculated and experimental results from fabricated capacitances. We demonstrated the change of capacitance due to variance of design parameters, such as, the area of the capacitance. Our printed IJP capacitors provided us the capacitance in the range of 8.8 pF to 467 pF for capacitor area 1 to 36 mm, while the simulated capacitance range was recorded between 9 pF to 455 pF. For four coat PVP the minimum and maximum capacitance obtained from simulations were 13.3 pF and 455 pF for capacitor area 1 mm2 and 36 mm2 respectively. The simulated capacitances with six coat PVP were 9 pF and 310 pF for 1 mm2 and 36 mm2 capacitor area respectively. For flexible electronics devices like body-worn sensors, IJP electronic components will be significant in near future and this paper lays the key foundation for that endeavor.","2154-0373","978-1-6654-1846-1","10.1109/EIT51626.2021.9491846","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9491846","COMSOL Multiphysics;Finite Element Analysis;Flexible electronics;Inkjet printing Parallel plate capacitors;Polymer dielectric","Silver;Simulation;Capacitors;Polyimides;Capacitance;Finite element analysis;Dielectrics","capacitance;capacitive sensors;capacitors;dielectric materials;finite element analysis;flexible electronics;ink jet printing;MIM devices;polymer films;silver","insulator material;IJP parallel plate capacitors;JS B40 silver ink;PVP dielectric ink;ideal calculated capacitance values;fabricated capacitances;printed IJP capacitors;simulated capacitance range;coat PVP;maximum capacitance;simulated capacitances;capacitor area;flexible electronics devices;IJP electronic components;Finite Element simulation;inkjet printed flexible parallel plate MIM capacitors;flexible electronics inkjet printing;Metal-Insulator-Metal type parallel plate capacitor;flexible electronic circuits;conductive metal parallel plates","","1","","17","IEEE","26 Jul 2021","","","IEEE","IEEE Conferences"
"A Novel Method for Sleep Score Estimation Using Wearable Sensors with a Deep Sequential Neural Network","M. J. Rahman; B. I. Morshed","Department of Electrical and Computer Engineering, University of Memphis, Memphis, USA; Department of Computer Science, Texas Tech University, Lubbock, USA","2021 IEEE International Conference on Electro Information Technology (EIT)","26 Jul 2021","2021","","","304","308","The use of sleep score as a measure of fitness and wellness is getting popular in Smart Health as it provides an objective assessment of sleep quality. However, reliable estimation of sleep scores from wearable sensor data only is challenging. In this study, we investigated the estimation of sleep score using only features available from single-channel ECG or single-channel EEG data. We used partial correlation and conditional permutation importance for feature selection; then compared extreme gradient boosting, artificial neural network, and sequential neural network for developing a regression model for sleep score estimation. TabNet- an attention-based deep sequential learning model achieved the best performance of RMSE = 5.47 and R-squared value of 0.59 in the test set for sleep score estimation using only spectral features of single-channel EEG. The results pave the way for reliable and interpretable sleep score estimation using a wearable device.","2154-0373","978-1-6654-1846-1","10.1109/EIT51626.2021.9491896","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9491896","Attention Model;Electroencephalography;Regression;Sleep Score;Smart Health;TabNet;Wearable Sensor","Performance evaluation;Sleep;Wearable computers;Smart healthcare;Estimation;Brain modeling;Electroencephalography","electrocardiography;electroencephalography;learning (artificial intelligence);medical signal processing;neural nets;patient monitoring;regression analysis;sleep","deep sequential neural network;sleep quality;wearable sensor data;single-channel EEG data;artificial neural network;attention-based deep sequential learning model;reliable sleep score estimation;interpretable sleep score estimation","","","","20","IEEE","26 Jul 2021","","","IEEE","IEEE Conferences"
"Estimation of Respiration Rate using an Inertial Measurement Unit Placed on Thorax-Abdomen","M. Rahman; B. I. Morshed","Department of Computer Science, Texas Tech University, Lubbock, TX, USA; Department of Computer Science, Texas Tech University, Lubbock, TX, USA","2021 IEEE International Conference on Electro Information Technology (EIT)","26 Jul 2021","2021","","","1","5","Respiration rate is one of the important measures of any physiological changes in human body. In this paper, an inertial measurement unit (IMU) is used to detect the chest movement and estimate the respiration rate from the real-time signal. A commercial inertial motion sensor chip used in this study that produces linear motion vector as streaming data. The signal from the motion sensor was sampled at 10 Hz. Signal processing was applied to denoise respiration signals from the values of linear motion vectors. Then, an algorithm of calculating respiration rate, was used to estimate breath per minute (BPM). The results were compared with a commercial respiration monitor belt logger sensor as the ground truth. The IMU sensor was tested at 5 different BPMs (12, 15, 20, 24, and 30) to validate the data from the IMU sensor and from the commercial respiration belt using a protocol where different BPM was maintained. The results show high accuracy of the proposed system which is simpler to use, cheaper to protype, and can be integrated with a wearable device and a custom smartphone app using edge computing technique.","2154-0373","978-1-6654-1846-1","10.1109/EIT51626.2021.9491900","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9491900","Data validation;Inertial measurement unit;Linear motion;Respiration rate;Wearable","Semiconductor device measurement;Measurement units;Protocols;Wearable computers;Signal processing algorithms;Signal processing;Belts","biological organs;biomedical measurement;body sensor networks;medical signal processing;mobile computing;motion measurement;patient monitoring;pneumodynamics;signal denoising;smart phones;wearable sensors","smartphone app;edge computing;wearable device;linear motion vectors;chest movement;thorax-abdomen;inertial measurement unit;IMU sensor;commercial respiration monitor belt logger sensor;respiration signals;signal processing;linear motion vector;commercial inertial motion sensor chip;frequency 10.0 Hz","","4","","13","IEEE","26 Jul 2021","","","IEEE","IEEE Conferences"
"Generalization of Data Reliability Metric (DReM) Mechanism for Pulsatile Bio-signals","M. S. Zaman; B. I. Morshed","Department of Electrical and Computer Engineering, University of Memphis, Memphis, TN, USA; Department of Computer Science, Texas Tech University, Lubbock, TX, USA","2021 IEEE International Conference on Electro Information Technology (EIT)","26 Jul 2021","2021","","","282","286","Due to rapid development of wearable technologies used for health monitoring, a robust data reliability assessment technique is required. Choosing the right sets of Data reliability metrics (DReM) can improve the performance of data reliability assessment and thus, it can help in reliable data acquisitions of bio-signals. Traditional reliability assessment techniques rely significantly on the signal specific peak detection algorithms. It impedes the endeavor of generalizing signal independent data reliability assessment techniques. In this work, we explored nine signal independent statistical candidates for DReM and finalized five top features as our DReMs which can identify acceptable signal segments from unacceptable segments with 0.83 precision, 0.84 recall and 0.83 F-1 score on accurately identifying acceptable pulses (ECG, PPG and respiratory signal). Additionally, the five DReMs are capable of detecting unacceptable signal segments with 0.92 precision, 0.92 recall and 0.92 F-1 score. We proposed optimal Random Forest classifier model with excellent Receiver Operating Characteristics (ROC) with significant Area Under the Curve (AUC) value of 0.994.","2154-0373","978-1-6654-1846-1","10.1109/EIT51626.2021.9491839","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9491839","Data Reliability;Signal Quality;DReM;Bio-signals;mHealth","Radio frequency;Measurement;Support vector machines;Ranking (statistics);Heart rate;Receivers;Electrocardiography","data acquisition;electrocardiography;feature extraction;image segmentation;learning (artificial intelligence);medical image processing;medical signal processing;pattern classification;reliability;sensitivity analysis;statistical analysis","Data reliability metric mechanism;DReM;pulsatile bio-signals;robust data reliability assessment technique;Data reliability metrics;reliable data acquisitions;traditional reliability assessment techniques;signal specific peak detection algorithms;signal independent data reliability assessment techniques;acceptable signal segments;respiratory signal;unacceptable signal segments","","","","15","IEEE","26 Jul 2021","","","IEEE","IEEE Conferences"
"Smart Health Integrated Framework and Topology (SHIFT) for Smart and Connected Community","B. I. Morshed","Department of Computer Science, Texas Tech University, Lubbock, TX, USA","2021 IEEE International Conference on Electro Information Technology (EIT)","26 Jul 2021","2021","","","250","254","A smart and connected communities (S&CC) will utilize existing and emerging technologies to collect heterogeneous spatiotemporally distributed data and artificial intelligence (AI) to seamlessly generate meaningful knowledge that will benefit both individuals and S&CC. We have developed a framework for Health and Wellbeing of S&CC that includes existing and emerging sensors for data collection from users of the community, a custom smartphone app with real-time AI algorithms for edge-computing, and a webserver for spatiotemporal visualization of abstracted information for community stakeholders. We propose to extend this framework towards an enhanced Smart Health Integrated Framework and Topology (SHIFT) through incorporating a uniform hierarchical layer-based architecture for S&CC. The proposed concept was simulated to depict data processing and visualization approach. The proposed framework takes advantage of evolving topology of smart sensors and devices, in addition to being transferable and scalable.","2154-0373","978-1-6654-1846-1","10.1109/EIT51626.2021.9491837","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9491837","AI;Edge computing;Mobile health;Privacy;Scalable framework;Smart health;Spatiotemporal data","Smart healthcare;Urban areas;Distributed databases;Data visualization;Real-time systems;Topology;Spatiotemporal phenomena","artificial intelligence;data analysis;data visualisation;intelligent sensors;Internet;mobile computing;smart phones","artificial intelligence;meaningful knowledge;S&CC;data collection;custom smartphone app;real-time AI algorithms;spatiotemporal visualization;community stakeholders;SHIFT;uniform hierarchical layer-based architecture;depict data processing;visualization approach;smart sensors;connected community;smart communities;connected communities;spatiotemporally distributed data;enhanced smart health integrated framework and topology","","","","19","IEEE","26 Jul 2021","","","IEEE","IEEE Conferences"
"Variational Self-attention Network for Sequential Recommendation","J. Zhao; P. Zhao; L. Zhao; Y. Liu; V. S. Sheng; X. Zhou","School of Computer Science and Technology, Soochow University, Suzhou, China; School of Computer Science and Technology, Soochow University, Suzhou, China; School of Computer Science and Technology, Soochow University, Suzhou, China; Rutgers University, New Jersey, USA; Department of Computer Science, Texas Tech University, Lubbock, USA; The Hong Kong University of Science and Technology, Hong Kong SAR, China","2021 IEEE 37th International Conference on Data Engineering (ICDE)","22 Jun 2021","2021","","","1559","1570","Sequential recommendation has become an attractive topic in recommender systems. Existing sequential recommendation methods, including the methods based on the state-of-the-art self-attention mechanism, usually employ deterministic neural networks to represent user preferences as fixed-points in the latent feature spaces. However, the fixed-point vector lacks the ability to capture the uncertainty and dynamics of user preferences that are prevalent in recommender systems. In this paper, we propose a new Variational Self-Attention Network (VSAN), which introduces a variational autoencoder (VAE) into the self-attention network to capture latent user preferences. Specifically, we represent the obtained self-attention vector as density via variational inference, whose variance well characterizes the uncertainty of user preferences. Furthermore, we employ self-attention networks to learn the inference process and generative process of VAE, which well captures long-range and local dependencies. Finally, we evaluate our proposed method VSAN with two public real-world datasets. Our experimental results show the effectiveness of our model compared to the state-of-the-art approaches.","2375-026X","978-1-7281-9184-3","10.1109/ICDE51399.2021.00138","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458633","variational;attention;sequential recommendation","Uncertainty;Conferences;Neural networks;Data engineering;Recommender systems","data mining;learning (artificial intelligence);neural nets;recommender systems","sequential recommendation methods;state-of-the-art self-attention mechanism;deterministic neural networks;fixed-points;latent feature spaces;fixed-point vector;recommender systems;Variational Self-Attention Network;variational autoencoder;latent user preferences;self-attention vector;variational inference;method VSAN;Variational Self-attention Network;attractive topic","","10","","45","IEEE","22 Jun 2021","","","IEEE","IEEE Conferences"
"Machine Learning Attack on a Multiplexer PUF Variant Using Silicon Data: a Case Study on rMPUFs","M. A. Alamro; K. T. Mursi","Department of Computer Science, Texas Tech University, Lubbock, USA; College of Computer Science and Engineering, University of Jeddah, Jeddah, Saudi Arabia","2021 IEEE 6th International Conference on Computer and Communication Systems (ICCCS)","21 Jun 2021","2021","","","1017","1022","The security of Physical Unclonable Functions (PUFs) remains central and persists as an obstacle to the applicability of some designs. A Multiplexer PUF variant called (rMPUF) was recently proposed to enhance the security of the delay-based PUFs, which they show that the design is secure against machine learning attacks based on simulated data. In this paper, we take the analysis on rMPUFs to the next level by implementing these designs on field-programmable gate arrays (FPGAs) and evaluate their resistance against modeling attacks using silicon CRPs. Our risk analysis on rMPUFs with up to seven MUXs stages using different challenge sizes show that all designs are vulnerable to machine learning attacks. Security enhancements are modest and come at the high cost of hardware overhead. Nevertheless, the proposed rMPUFs can be a good candidate for weak PUFs with access-restricted protocols, considering the stability of the responses that do not deteriorate with enhancements in the designs' security.","","978-1-6654-1256-8","10.1109/ICCCS52626.2021.9449179","National Science Foundation(grant numbers:CNS-1526055); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9449179","physical unclonable functions;IoTs Security;machine learning;IoTs authentication;FPGA","Multiplexing;Protocols;Machine learning;Physical unclonable function;Stability analysis;Silicon;Hardware","cryptography;field programmable gate arrays;learning (artificial intelligence);logic design;risk analysis","silicon data;rMPUF;Physical Unclonable Functions;Multiplexer PUF variant;delay-based PUFs;field-programmable gate arrays;modeling attacks;silicon CRPs;machine learning attacks;security enhancements;weak PUFs;designs","","2","","33","IEEE","21 Jun 2021","","","IEEE","IEEE Conferences"
"HAM: Hotspot-Aware Manager for Improving Communications With 3D-Stacked Memory","X. Wang; A. Tumeo; J. D. Leidel; J. Li; Y. Chen","Department of Computer Science, Texas Tech University, Lubbock, TX, USA; High Performance Computing Group of Pacific Northwest National Laboratory, Richland, WA, USA; Tactical Computing Laboratories, Muenster, TX, USA; Department of Computer Science, Texas Tech University, Lubbock, TX, USA; Department of Computer Science, Texas Tech University, Lubbock, TX, USA","IEEE Transactions on Computers","24 May 2021","2021","70","6","833","848","Emerging High-Performance Computing (HPC) workloads, such as graph analytics, machine learning, and big data science, are data-intensive. Data-intensive workloads usually present fine-grained memory accesses with limited or no data locality, and thus incur frequent cache misses and low utilization of memory bandwidth. 3D-stacked memory devices such as Hybrid Memory Cube (HMC) and High Bandwidth Memory (HBM) can provide significantly higher bandwidth than conventional memory modules. However, the traditional interfaces and optimization methods for JEDEC DDR devices do not allow to fully exploit the potential performance of 3D-stacked memory with the massive amount of irregular memory accesses of data-intensive applications. In this article, we propose a novel Hotspot-Aware Manager (HAM) infrastructure for 3D-stacked memory devices capable of optimizing memory access streams via request aggregation, hotspot detection, and in-memory prefetching. We present the HAM design and implementation, and simulate it on a system using RISC-V embedded cores with attached HMC devices. We extensively evaluate HAM with over 12 benchmarks and applications representing diverse irregular memory access patterns. The results show that, on average, HAM reduces redundant requests by 37.51 percent and increases the prefetch buffer hit rate by 4.2 times, compared to a baseline streaming prefetcher. On the selected benchmark set, HAM provides performance gains of 21.81 percent in average (up to 34.28 percent), and power savings of 35.07 percent over a standard 3D-stacked memory.","1557-9956","","10.1109/TC.2021.3066982","National Science Foundation(grant numbers:CCF-1409946,CCF-1718336,OAC-1835892,CNS-1817094,CNS-1939140); Pacific Northwest National Laboratory; Data-Model Convergence (DMC) Initiative; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9381710","Memory hotspot;3D-stacked memory;coalescing;prefetching;HBM;HMC;communications","Memory management;Random access memory;Bandwidth;Program processors;Prefetching;Three-dimensional displays;Performance evaluation","buffer storage;DRAM chips;memory architecture;parallel processing","data-intensive applications;hotspot-aware manager infrastructure;in-memory prefetching;attached HMC devices;irregular memory access patterns;high-performance computing workloads;big data science;data-intensive workloads;fine-grained memory accesses;data locality;hybrid memory cube;high bandwidth memory;JEDEC DDR devices;three dimensional-stacked memory devices;HPC;memory access stream optimisation;request aggregation;hotspot detection;HAM design;RISC-V embedded cores;prefetch buffer hit rate;baseline streaming prefetcher","","","","45","IEEE","18 Mar 2021","","","IEEE","IEEE Journals"
"Human-Guided Robot Behavior Learning: A GAN-Assisted Preference-Based Reinforcement Learning Approach","H. Zhan; F. Tao; Y. Cao","Department of Computer Science, Texas Tech University, Lubbock, TX, USA; Department of Electrical and Computer Engineering, University of Texas at San Antonio, San Antonio, Texas, USA; Department of Electrical and Computer Engineering, University of Texas at San Antonio, San Antonio, Texas, USA","IEEE Robotics and Automation Letters","24 Mar 2021","2021","6","2","3545","3552","Human demonstrations can provide trustful samples to train reinforcement learning algorithms for robots to learn complex behaviors in real-world environments. However, obtaining sufficient demonstrations may be impractical because many behaviors are difficult for humans to demonstrate. A more practical approach is to replace human demonstrations by human queries, i.e., preference-based reinforcement learning. One key limitation of the existing algorithms is the need for a significant amount of human queries because a large number of labeled data is needed to train neural networks for the approximation of a continuous, high-dimensional reward function. To reduce and minimize the need for human queries, we propose a new GAN-assisted human preference-based reinforcement learning approach that uses a generative adversarial network (GAN) to learn human preferences and then replace the role of human in assigning preferences. The adversarial neural network is simple and only has a binary output, hence requiring much less human queries to train. Moreover, a maximum entropy based reinforcement learning algorithm is designed to shape the loss towards the desired regions or away from the undesired regions. To show the effectiveness of the proposed approach, we present some studies on complex robotic tasks without access to the environment reward in a typical MuJoCo robot locomotion environment. The obtained results show our method can achieve a reduction of about 99.8% human time without performance sacrifice.","2377-3766","","10.1109/LRA.2021.3063927","Army Research Office(grant numbers:W911NF-21-1-0103); Office of Naval Research(grant numbers:N000141912278,N000141712613); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9369902","Generative adversarial network (GAN);human preferences;reinforcement learning","Trajectory;Reinforcement learning;Neural networks;Robots;Training;Entropy;Task analysis","control engineering computing;human-robot interaction;intelligent robots;learning (artificial intelligence);neural nets","high-dimensional reward function;continuous reward function;MuJoCo robot locomotion environment;adversarial neural network;generative adversarial network;human-guided robot behavior learning;GAN-assisted human preference-based reinforcement learning;maximum entropy based reinforcement learning;human time;human preferences;human queries;human demonstrations;robot behavior learning","","6","","36","IEEE","4 Mar 2021","","","IEEE","IEEE Journals"
"Multilevel Identification and Classification Analysis of Tor on Mobile and PC Platforms","L. Wang; H. Mei; V. S. Sheng","School of Computer Science and Communication Engineering, Jiangsu University, Zhenjiang, China; School of Computer Science and Communication Engineering, Jiangsu University, Zhenjiang, China; Department of Computer Science, Texas Tech University, Lubbock, TX, USA","IEEE Transactions on Industrial Informatics","19 Nov 2020","2021","17","2","1079","1088","In digitalized and automated systems, more and more intelligent devices have become an import part of industrial Internet of Things (IIOT). However, the lack of security in IIOT makes people facing unprecedented threats from the Dark web. Traffic classification is an important means to prevent anonymous attacks. However, the growing usage of smartphones in daily life is deeply changing the nature of network traffic, which makes traffic classification more challenging. In this article, we propose a Tor traffic identification and multilevel classification framework based on network flow features, which realizes the identification of anonymous traffic (L1), traffic types (L2) of anonymous traffic, and applications (L3) on a mobile and a PC platform, respectively. We further analyze differences between the mobile and the PC platform. We conclude that the impact of time-related features is higher than that of the nontime-related features on the mobile platform, while it is opposite on the PC platform. And it is more difficult to identify and classify Tor types (L2) and specific Tor applications (L3) on the mobile platform than on the PC platform, including using different number of features and early identification and classification.","1941-0050","","10.1109/TII.2020.2988870","National Natural Science Foundation of China(grant numbers:U1736216); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9072571","Anonymous network;mobile anonymous network;The Onion Router (Tor);Tor identification;traffic classification","Mobile handsets;Cryptography;Tools;Internet;Machine learning;Operating systems;Informatics","Internet;pattern classification;telecommunication traffic","nontime-related features;time-related features;traffic types;anonymous traffic;network flow features;multilevel classification framework;Tor traffic identification;network traffic;anonymous attacks;traffic classification;IIOT;import part;intelligent devices;automated systems;digitalized systems;specific Tor applications;Tor types;PC platform;mobile platform","","16","","31","IEEE","20 Apr 2020","","","IEEE","IEEE Journals"
"Location-Aware Service Recommendations With Privacy-Preservation in the Internet of Things","W. Lin; X. Zhang; L. Qi; W. Li; S. Li; V. S. Sheng; S. Nepal","Institute of VR and Intelligent System, Hangzhou Normal University, Hangzhou, China; Department of Computing, Macquarie University, Sydney, NSW, Australia; School of Information Science and Engineering, Qufu Normal University, Rizhao, China; School of Computer Engineering and Technology, Shanghai University, Shanghai, China; Computer Science and Creative Technologies Department, University of the West of England, Bristol, U.K.; Department of Computer Science, Texas Tech University, Lubbock, TX, USA; CSIRO, Eveleigh, NSW, Australia","IEEE Transactions on Computational Social Systems","29 Jan 2021","2021","8","1","227","235","With the ever-increasing maturity and popularization of the Internet of Things (IoT), tremendous business applications developed by distinct enterprises or organizations have been encapsulated into lightweight web services that can easily be accessed or invoked remotely. However, the big volume of candidate web services places a heavy burden on the users' service selection decision-making process. Under the circumstance, a variety of intelligent recommendation solutions have been developed to reduce the high decision-making cost. Traditional resolutions usually challenge in two aspects. First, the recommendation parameters, i.e., the quality of services (QoS), usually relies on user/service location heavily; therefore, low-quality recommended results may be returned to users if user/service location information is overlooked. Second, historical QoS data often contain partial sensitive information of users; therefore, it becomes a necessity to protect the sensitive QoS data while making accurate recommendation decisions. To tackle the above challenges, we introduce the concepts of user/service location information and locality-sensitive hashing (LSH) in the domain and propose a location-aware recommendation approach with privacy-preservation capability. A wide range of experiments is set up based on the popular WS-DREAM data set, whose results prove the effectiveness and efficiency of our approach.","2329-924X","","10.1109/TCSS.2020.2965234","National Key Research and Development Program of China(grant numbers:2017YFE0117500); National Science Foundation of China(grant numbers:61872219,61672276,61702277); Natural Science Foundation of Shandong Province(grant numbers:ZR2019MF001); Open Project funded by State Key Laboratory of Novel Software Technology, Nanjing University(grant numbers:KFKT2019B19); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8982047","Locality-sensitive hashing (LSH);location;privacy;quality of service (QoS);recommender system","Quality of service;Web services;Decision making;Data privacy;Internet of Things;Privacy;Intelligent sensors","data privacy;decision making;Internet of Things;location based services;mobile computing;quality of service;recommender systems;Web services","location-aware service recommendations;business applications;lightweight Web services;candidate Web services;high decision-making cost;historical QoS data;partial sensitive information;sensitive QoS data;privacy-preservation capability;Internet of Things;user service selection;quality of services;locality-sensitive hashing;LSH;WS-DREAM data set","","10","","44","IEEE","4 Feb 2020","","","IEEE","IEEE Journals"
"Photo2Trip: Exploiting Visual Contents in Geo-Tagged Photos for Personalized Tour Recommendation","P. Zhao; C. Xu; Y. Liu; V. S. Sheng; K. Zheng; H. Xiong; X. Zhou","Institute of AI, School of Computer Science and Technology, Soochow University, Suzhou, China; Institute of AI, School of Computer Science and Technology, Soochow University, Suzhou, China; NEC Labs America, Princeton, NJ, USA; Department of Computer Science, Texas Tech University, Lubbock, TX, USA; University of Electronic Science and Technology, Chengdu, China; Management Science and Information Systems Department, Rutgers University, Piscataway, NJ, USA; University of Queensland, Brisbane, QLD, Australia","IEEE Transactions on Knowledge and Data Engineering","8 Mar 2021","2021","33","4","1708","1721","Recently accumulated massive amounts of geo-tagged photos provide an excellent opportunity to understand human behaviors and can be used for personalized tour recommendation. However, no existing work has considered the visual content information in these photos for tour recommendation. We believe the visual features of photos provide valuable information on measuring user / Point-of-Interest (POI) similarities, which is challenging due to data sparsity. To this end, in this paper, we propose a visual feature enhanced tour recommender system, named ‘Photo2Trip’, to utilize the visual contents and collaborative filtering models for recommendation. Specifically, we propose a Visual-enhanced Probabilistic Matrix Factorization model (VPMF), which integrates visual features into the collaborative filtering model, to learn user interests by leveraging the historical travel records. We then extend VPMF to End-to-End training framework to incorporate users (POIs) latent factors into the learning process of the visual content of photos, which generalizes the applicability of the proposed VPMF framework in tour recommendation. Extensive empirical studies verify that our proposed visual-enhanced personalized tour recommendation method outperforms other benchmark methods in terms of recommendation accuracy. The results also show that visual features are effective in alleviating the data sparsity and cold start problems on personalized tour recommendation.","1558-2191","","10.1109/TKDE.2019.2943854","National Natural Science Foundation of China(grant numbers:61876117,61876217,61772356,6172820561972069); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8850032","Tour recommendation;collaborative filtering;visual content","Visualization;Collaboration;Probabilistic logic;Training;Planning;Correlation;Task analysis","collaborative filtering;learning (artificial intelligence);matrix decomposition;recommender systems","visual-enhanced personalized tour recommendation method;End-to-End training framework;Visual-enhanced Probabilistic Matrix Factorization model;collaborative filtering model;visual feature enhanced tour recommender system;Point-of-Interest similarities;visual features;visual content information;recently accumulated massive amounts;geo-tagged photos;Visual contents;Photo2Trip","","9","","49","IEEE","26 Sep 2019","","","IEEE","IEEE Journals"
"Trigger-Based Incremental Data Processing with Unified Sync and Async Model","D. Dai; Y. Chen; D. Kimpe; R. B. Ross","Department of Computer Science, Texas Tech University, Lubbock, TX, USA; Department of Computer Science, Texas Tech University, Lubbock, TX, USA; Mathematics and Computer Science Division, Argonne National Laboratory, Chicago, IL, USA; Mathematics and Computer Science Division, Argonne National Laboratory, Chicago, IL, USA","IEEE Transactions on Cloud Computing","5 Mar 2021","2021","9","1","372","385","In recent years, more and more applications in the cloud have needs to process large-scale on-line datasets, which evolve over time as new entries are added and existing entries are modified. Several programming frameworks, such as Percolator and Oolong, are proposed for such incremental data processing and can achieve efficient processing with an event-driven abstraction. However, these frameworks are inherently asynchronous, leaving the heavy burden of managing synchronization to applications' developers, which further significantly restricts their usabilities. In this study, we propose a trigger-based incremental computing framework in the cloud, called Domino, with both synchronous and asynchronous mechanisms to coordinate parallel triggers. With this new framework, both synchronous and asynchronous applications can be seamlessly developed. Use cases and extensive evaluation results confirm that it can deliver sufficient performance, and also is easy to use for incremental applications in large-scale distributed computing.","2168-7161","","10.1109/TCC.2018.2830348","U.S. Department of Defense; U.S. Department of Energy(grant numbers:DE-AC02-06CH11357); National Science Foundation(grant numbers:CNS-1338078,CNS-1162488); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8392435","Programming framework;cloud;incremental computing","Synchronization;Cloud computing;Programming;Data models;Servers;Computational modeling;Runtime","distributed processing;program diagnostics;synchronisation","event-driven abstraction;trigger-based incremental computing framework;synchronous mechanisms;asynchronous mechanisms;parallel triggers;large-scale distributed computing;trigger-based incremental data processing;unified sync;async model;programming frameworks;large-scale on-line datasets","","2","","39","IEEE","21 Jun 2018","","","IEEE","IEEE Journals"
"CDMI: A Clockwise-Displacement Algorithm to Compute Multiplicative Inverse","H. Abu-gellban; L. Nguyen","Department of Computer Science, Texas Tech University, USA; Whitacre College of Engineering, Texas Tech University, USA","2020 International Conference on Computational Science and Computational Intelligence (CSCI)","23 Jun 2021","2020","","","1407","1410","A multiplicative inverse (MI) algorithm is used in several fields, like cryptography algorithms. There are many MIs, however, these algorithms suffer from using several multiplication and division operations, which take more execution time than additions and subtractions. We created a new algorithm called Clockwise-Displacement (CDMI) by using addition and subtraction operations in the iterative steps instead of multiplications and divisions. Additionally, numerous MIs face the undecidable problem because of the floating-point issue. Whereas, CDMI tackles this issue by converting the domain from the floatingpoint space to integer space. Therefore, CDMI declines the time consuming to calculate the multiplicative inverse by applying fewer divisions and multiplications (expensive operations) and addresses the rounding error issue in some MI Algorithms.","","978-1-7281-7624-6","10.1109/CSCI51800.2020.00261","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458207","Multiplicative Inverse;Security;Public Key Techniques;Asymmetric Techniques;Cryptography","Scientific computing;Iterative algorithms;Cryptography;Faces;Computational intelligence","cryptography;floating point arithmetic;iterative methods;mathematics computing","clockwise-displacement algorithm;division operations;cryptography algorithms;multiplicative inverse algorithm;MI algorithms;floating-point issue;CDMI","","2","","19","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"Energy Usage of Deep Learning in Smart Cities","S. Puangpontip; R. Hewett","Department of Computer Science, Texas Tech University, Lubbock, USA; Department of Computer Science, Texas Tech University, Lubbock, USA","2020 International Conference on Computational Science and Computational Intelligence (CSCI)","23 Jun 2021","2020","","","1143","1148","Deep learning has increasingly become an essential component of many Smart City functions including smart city lightings, emergency rescues, smart drainage, and smart parking. These functions operate continuously in real-time throughout the day. Thus, excessive energy usage of deep learning computation can negatively impact economic benefits and efficiency of smart cities. The situation can escalate when dealing with resource-constrained large-scale smart cities of huge Internet-of-Things and networks with large numbers and varieties of sensors. To effectively sustain, manage and protect smart cities from failures due to energy overload, the awareness of energy consumption by deep learning computation is unavoidably necessary. Most recent research in smart cities focuses on using deep learning to perform certain tasks but does not address energy issues. This paper presents a formal approach to estimating energy consumption of deep learning and illustrates its use in smart cities. In particular, we develop a fine-grained mathematical model that extends an existing model to include the quantification of MAC (multiply-and-accumulate) operations as well as data access from a memory hierarchy. This paper focuses on deep and convolutional neural networks. We describe the proposed approach and validate the results obtained from our model by comparing them against those of existing work. The proposed approach is applied to three (deep) neural systems in smart cities, namely smart drainage, smart transportation and smart parking systems, all of which yield promising results.","","978-1-7281-7624-6","10.1109/CSCI51800.2020.00214","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9457878","energy consumption;deep learning;smart city","Deep learning;Energy consumption;Smart cities;Scientific computing;Lighting;Real-time systems;Smart transportation","convolutional neural nets;deep learning (artificial intelligence);emergency services;energy consumption;failure analysis;Internet of Things;lighting;mathematical analysis;power distribution economics;power distribution faults;power engineering computing;sensors;smart cities;smart power grids;sustainable development","deep learning computation;energy consumption;smart drainage;smart city lightings;emergency rescue;economic benefits;smart parking systems;smart cities efficiency;Internet-of-things;sensors;energy overload failures;mathematical model;MAC operations;multiply-and-accumulate operations;deep neural networks;convolutional neural networks;smart transportation","","2","","14","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"Ethereum Smart Contracts: Vulnerabilities and their Classifications","Z. A. Khan; A. Siami Namin","Department of Computer Science, Texas Tech University; Department of Computer Science, Texas Tech University","2020 IEEE International Conference on Big Data (Big Data)","24 May 2021","2020","","","1","10","Smart contract (SC) is an extension of BlockChain technology. Ethereum BlockChain was the first to incorporate SC and thus started a new era of crypto-currencies and electronic transactions. Solidity helps to program the SCs. Still, soon after Solidity's emergence in 2014, Solidity-based SCs suffered many attacks that deprived the SC account holders of their precious funds. The main reason for these attacks was the presence of vulnerabilities in SC. This paper discusses SC vulnerabilities and classifies them according to the domain knowledge of the faulty operations. This classification is a source of reminding developers and software engineers that for SC's safety, each SC requires proper testing with effective tools to catch those classes' vulnerabilities.","","978-1-7281-6251-5","10.1109/BigData50022.2020.9439088","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9439088","Smart Contract;Ethereum;EVM;vulnerabilities;Solidity;tools","Conferences;Smart contracts;Blockchain;Big Data;Tools;Software;Safety","blockchains;computer crime;contracts;cryptocurrencies;software engineering","crypto-currencies;electronic transactions;SC account holders;Ethereum smart contracts;BlockChain technology;solidity-based SC;SC vulnerability classification;Ethereum BlockChain","","6","","57","IEEE","24 May 2021","","","IEEE","IEEE Conferences"
"Coordinating Disaster Emergency Response with Heuristic Reinforcement Learning","Z. Yang; L. Nguyen; J. Zhu; Z. Pan; J. Li; F. Jin","Department of Statistics, George Washington University; Department of Computer Science, Texas Tech University; Department of Civil and Environmental Engineering, University of California, Davis; Department of Computer Science, Texas Tech University; Walmart Global Tech; Department of Statistics, George Washington University","2020 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)","24 Mar 2021","2020","","","565","572","Ahstract-A crucial and time-sensitive task when any disaster occurs is to rescue victims and distribute resources to the right groups and locations. This task is challenging in populated urban areas, due to a huge burst of help requests made in a very short period. To improve the efficiency of the emergency response in the immediate aftermath of a disaster, we propose a heuristic multi-agent reinforcement learning scheduling algorithm, named as ResQ, which can effectively schedule a rapid deployment of volunteers to rescue victims in dynamic settings. The core concept is to quickly identify victims and volunteers from social network data and then schedule rescue parties with an adaptive learning algorithm. This framework performs two key functions: 1) identify trapped victims and volunteers, and 2) optimize the volunteers' rescue strategy in a complex time-sensitive environment. The proposed ResQ algorithm can speed up the training processes through a heuristic function which reduces the state-action space by identifying a set of particular actions over others. Experimental results showed that the proposed heuristic multi-agent reinforcement learning based scheduling outperforms several state-of-art methods, in terms of both reward rate and response times.","2473-991X","978-1-7281-1056-1","10.1109/ASONAM49781.2020.9381416","NSF(grant numbers:CNS-1737634); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9381416","","Schedules;Social networking (online);Heuristic algorithms;Urban areas;Reinforcement learning;Emergency services;Task analysis","disasters;emergency management;learning (artificial intelligence);multi-agent systems;scheduling;social networking (online)","coordinating disaster emergency response;help requests;scheduling algorithm;social network data;adaptive learning algorithm;trapped victims;ResQ algorithm;heuristic multiagent reinforcement learning scheduling algorithm;volunteer rescue strategy","","","","29","USGov","24 Mar 2021","","","IEEE","IEEE Conferences"
"An Interactive Platform to Track Global COVID-19 Epidemic","Z. Pan; D. Mehta; A. Tiwari; S. Ireddy; Z. Yang; F. Jin","Department of Computer Science, Texas Tech University; Department of Computer Science, Texas Tech University; Department of Computer Science, Texas Tech University; Department of Computer Science, Texas Tech University; Department of Statistics, George Washington University; Department of Statistics, George Washington University","2020 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)","24 Mar 2021","2020","","","948","951","This project built a world-wide database of coron-avirus cases, which helps to model the spread of the coronavirus disease (COVID-19), and to identify policy and social factors that impact the spread of COVID-19. Four essential tasks are implemented: 1) build a comprehensive database of coronavirus cases world-wide; 2) visualize the heatmap of confirmed cases for each country, provide detailed spreading trends for each countries and comparison among countries; 3) collect tweets about COVID-19 in real-time and extract people's daily concern flow; 4) integrate breaking news such as first confirmed/death case in each country. This demo will provide decision-makers with accurate data-driven representations in an easy to understand format that enables them to make more timely and cost-effective preparation and response plans.","2473-991X","978-1-7281-1056-1","10.1109/ASONAM49781.2020.9381436","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9381436","","COVID-19;Social networking (online);Tools;Market research;Real-time systems;Visual databases;Monitoring","data visualisation;database management systems;decision making;diseases;epidemics;interactive systems;medical information systems;social networking (online)","interactive platform;world-wide database;Coronavirus disease;social factors;comprehensive database;Coronavirus cases;global COVID-19 epidemic;COVID-19;heatmap visualization;spreading trends;tweets;breaking news;decision-makers;data-driven representations;cost-effective preparation;response plans","","2","","8","IEEE","24 Mar 2021","","","IEEE","IEEE Conferences"
"A Sentimental and Semantical Analysis on Facebook Comments to Detect Latent Patterns","M. N. Moghadasi; Z. Safari; Y. Zhuang","Department of Computer Science, Texas Tech University, Lubbock, Texas, USA; Department of Computer Science, Texas Tech University, Lubbock, Texas, USA; Department of Computer Science, Texas Tech University, Lubbock, Texas, USA","2020 IEEE International Conference on Big Data (Big Data)","19 Mar 2021","2020","","","4665","4671","Social media posts and their comments are rich in variation of subjects and an interesting pool for opinion mining. Individuals engage in social communications on Facebook through three behaviors: like, share and comment on a Facebook post. Responses to comments are grouped under the respective comment as a conversation thread. Conversation threads become interesting when users have conflicting views with the article posted, or with the opinion of another user. Our research goal is to answer questions such as why some posts in Facebook receive more attention than others? Are conversation threads following a similar pattern between subjects like sport and politics? Is there any harmony between conversation threads of different subjects? We investigated how individuals react to different conversation subjects in the Facebook through a comprehensive analysis. Our aim is to discover semantic and sentimental patterns in conversation threads categories. Finally, we employed Natural Language Processing techniques such as semantic and sentimental analysis and statistical methods like average response time (ART) and average comment length (ACL) of a post and observed that there are interesting patterns exists among different conversation threads.","","978-1-7281-6251-5","10.1109/BigData50022.2020.9378425","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9378425","Social Analysis;Data Mining;Semantic Analysis","Social networking (online);Statistical analysis;Instruction sets;Semantics;Subspace constraints;Big Data;Time factors","data mining;natural language processing;social networking (online);sport;statistical analysis","sentimental analysis;average comment length;interesting patterns;different conversation threads;semantical analysis;facebook comments;latent patterns;social media posts;interesting pool;opinion mining;social communications;Facebook post;respective comment;conversation thread;similar pattern;different conversation subjects;semantic patterns;sentimental patterns;conversation threads categories;semantic analysis","","4","","26","IEEE","19 Mar 2021","","","IEEE","IEEE Conferences"
"A Concern Analysis of Federal Reserve Statements: The Great Recession vs. The COVID-19 Pandemic","L. F. Gutiérrez; S. Siami-Namini; N. Tavakoli; A. S. Namin","Department of Computer Science, Texas Tech University; Department of Computer Science, Mississippi State University; Department of Computer Science, Georgia Institute of Technology; Department of Computer Science, Texas Tech University","2020 IEEE International Conference on Big Data (Big Data)","19 Mar 2021","2020","","","2079","2086","It is important and informative to compare and contrast major economic crises in order to confront novel and unknown cases such as the COVID-19 pandemic. The 2006 Great Recession and then the 2019 pandemic have a lot to share in terms of unemployment rate, consumption expenditures, and interest rates set by Federal Reserve. In addition to quantitative historical data, it is also interesting to compare the contents of Federal Reserve statements for the period of these two crises and find out whether Federal Reserve cares about similar concerns or there are some other issues that demand separate and unique monetary policies. This paper conducts an analysis to explore the Federal Reserve concerns as expressed in their statements for the period of 2005 to 2020. The concern analysis is performed using natural language processing (NLP) algorithms and a trend analysis of concern is also presented. We observe that there are some similarities between the Federal Reserve statements issued during the Great Recession with those issued for the 2019 COVID-19 pandemic.","","978-1-7281-6251-5","10.1109/BigData50022.2020.9377828","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9377828","Natural Language Processing;Concern Analysis;the COVID-19 Pandemic;the Great Recession","COVID-19;Pandemics;Economic indicators;Big Data;Market research;Natural language processing;Unemployment","economic cycles;financial management;macroeconomics;natural language processing;socio-economic effects;unemployment","Federal Reserve statements;Federal Reserve concerns;concern analysis;2019 COVID-19 pandemic;2006 Great Recession;interest rates;major economic crisis;unemployment rate;consumption expenditures;monetary policies;natural language processing;NLP algorithms;trend analysis","","1","","20","IEEE","19 Mar 2021","","","IEEE","IEEE Conferences"
"A Sensitivity Analysis of Evolutionary Algorithms in Generating Secure Configurations","S. Dass; A. S. Namin","Computer Science Department, Texas Tech University; Computer Science Department, Texas Tech University","2020 IEEE International Conference on Big Data (Big Data)","19 Mar 2021","2020","","","2065","2072","The growth of Cyber-physical Systems (CPS) has been increased in recent years. This has led to the coupling of highly complex cyber-physical components. With the integration of such complex components, new security challenges have emerged. Studies involving security issues in CPS have been quite difficult to be generalized due to the presence of heterogeneity and the diversity of the CPS components. These systems are subject to various vulnerabilities, threats and attacks, as a consequence of complex versions of CPS being introduced over time. This paper deals with vulnerabilities caused due to improper configurations in the software component of cyber-physical systems. Evolutionary algorithms such as Genetic Algorithms (GA) and Particle Swarm Optimization (PSO) can be employed to adequately test the underlying software for certain categories of vulnerabilities. This paper provides a detailed sensitivity analysis of these evolutionary algorithms in order to find out whether changing parameters involved in tuning these algorithms affect the overall performance. This analysis is based on the estimate of the number of generation of secure vulnerability pattern vectors under the variation of different parameters. The results indicate that while there is no evidence of influential parameters in Genetic Algorithms (i.e., mutation rate and population size), changes in the parameters involved in Particle Swarm Optimization algorithms (i.e., velocity rate and fitness range) have some positive impacts on the number of secure configurations generated.","","978-1-7281-6251-5","10.1109/BigData50022.2020.9378307","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9378307","Cyber-Physical Systems;security;Genetic Algorithm;Particle Swarm Optimization;Sensitivity Analysis","Sensitivity analysis;Cyber-physical systems;Big Data;Software;Security;Particle swarm optimization;Genetic algorithms","evolutionary computation;genetic algorithms;particle swarm optimisation;security of data;sensitivity analysis","evolutionary algorithms;generating secure configurations;cyber-physical systems;highly complex cyber-physical components;complex components;security challenges;security issues;CPS components;complex versions;improper configurations;software component;Genetic Algorithms;underlying software;detailed sensitivity analysis;secure vulnerability pattern vectors;Particle Swarm Optimization algorithms","","1","","20","IEEE","19 Mar 2021","","","IEEE","IEEE Conferences"
"Sent2Vec: A New Sentence Embedding Representation With Sentimental Semantic","M. N. Moghadasi; Y. Zhuang","Computer Science Department, Texas Tech University, Lubbock, TX, USA; Computer Science Department, Texas Tech University, Lubbock, TX, USA","2020 IEEE International Conference on Big Data (Big Data)","19 Mar 2021","2020","","","4672","4680","Text classification is considered as one of the primary task in many Natural Language Processing (NLP) applications. In industrial applications of NLP, sentimental analysis is a task to understand how satisfied a user is after receiving a service or buying a product. The traditional approach is to convert a text into a format of numeric vector before feeding into machine learning algorithm. This representation of a word refers to word embedding. However the traditional embedding methods often model the syntactic context of words but ignore the sentiment information of text [1]. This can impact on the accuracy of a classification model to predict the correct sentimental score for a text. In this paper, we present Sent2Vec, an alternative embedding representation that includes the sentimental semantic of a sentence in its embedding vector. We utilized the unsupervised Smoothed Inverse Frequency (uSIF) sentence embedding method in the Sent2Vec neural network over a multi million samples dataset. The new sentence embedding presented, can be used as features in downstream (un)supervised tasks, which also leads to better or comparable results compared to sophisticated methods. Furthermore, with a simple logistic regression classifier, Sent2Vec reaches competitive performance to state-of-the-art results on several datasets when combined with GloVe(6B).","","978-1-7281-6251-5","10.1109/BigData50022.2020.9378337","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9378337","Word Embedding;Sentimental Analysis;NLP","Vocabulary;Semantics;Neural networks;Big Data;Natural language processing;Numerical models;Task analysis","natural language processing;neural nets;numerical analysis;pattern classification;regression analysis;sentiment analysis;unsupervised learning;vectors","new sentence embedding representation;sentence sentimental semantic;text classification;natural language processing;sentimental analysis;numeric vector;machine learning algorithm;word embedding;classification model;text sentiment information;sentimental score;embedding vector;unsupervised smoothed inverse frequency sentence embedding method;Sent2Vec neural network;downstream supervised tasks;NLP;word representation;word syntactic context;uSIF sentence embedding method;multimillion sample dataset;downstream unsupervised tasks;simple logistic regression classifier","","6","","30","IEEE","19 Mar 2021","","","IEEE","IEEE Conferences"
"Not All Areas Are Equal: Detecting Thoracic Disease With ChestWNet","Z. Yang; Z. Pan; S. Liang; F. Jin","Department of Statistics, George Washington University; Department of Computer Science, Texas Tech University; Department of Computer Science, Texas Tech University; Department of Statistics, George Washington University","2020 IEEE International Conference on Big Data (Big Data)","19 Mar 2021","2020","","","3447","3452","Automating pneumonia diagnosis from X-ray images could significantly improve patient diagnosing outcomes. A major challenge is that disease information (features) must be extracted directly from the image backgrounds. Motivated by recent advances in Convolutional Neural Network (CNN), we propose a hierarchical weighting deep learning model, ChestWNet, that combines DenseNet and transfer learning to detect and localize thoracic diseases from chest x-rays. Hierarchical weighting networks are designed to assign scores reflecting the importance of specific pixels (regions), and learning weights at pixel-, region-, and image-levels, jointly learning these hierarchical weighting networks and the image classification network in an end-to-end manner. Chest X-ray datasets are customized to solve the unbalancing label problem in these datasets. Extensive experiments show that ChestWNet significantly outperforms other established prediction methods, and can also be applied to similar scenarios with fixed point-of-interest regions in images.","","978-1-7281-6251-5","10.1109/BigData50022.2020.9377793","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9377793","","Lung;Prediction methods;Big Data;Feature extraction;X-ray imaging;Diseases;Image classification","convolutional neural nets;deep learning (artificial intelligence);diagnostic radiography;diseases;feature extraction;image classification;medical image processing;patient diagnosis","convolutional neural network;ChestWNet;transfer learning;thoracic disease detection;hierarchical weighting networks;image classification network;chest X-ray datasets;pneumonia diagnosis;X-ray images;patient diagnosis;image level;hierarchical weighting deep learning;pixel level;region level;feature extraction","","","","16","IEEE","19 Mar 2021","","","IEEE","IEEE Conferences"
"Predicting Consequences of Cyber-Attacks","P. Datta; N. Lodinger; A. S. Namin; K. S. Jones","Department of Computer Science, Texas Tech University; Department of Psychological Sciences, Texas Tech University; Department of Computer Science, Texas Tech University; Department of Psychological Sciences, Texas Tech University","2020 IEEE International Conference on Big Data (Big Data)","19 Mar 2021","2020","","","2073","2078","Cyber-physical systems posit a complex number of security challenges due to interconnection of heterogeneous devices having limited processing, communication, and power capabilities. Additionally, the conglomeration of both physical and cyber-space further makes it difficult to devise a single security plan spanning both these spaces. Cyber-security researchers are often overloaded with a variety of cyber-alerts on a daily basis many of which turn out to be false positives. In this paper, we use machine learning and natural language processing techniques to predict the consequences of cyberattacks. The idea is to enable security researchers to have tools at their disposal that makes it easier to communicate the attack consequences with various stakeholders who may have little to no cybersecurity expertise. Additionally, with the proposed approach researchers' cognitive load can be reduced by automatically predicting the consequences of attacks in case new attacks are discovered. We compare the performance through various machine learning models employing word vectors obtained using both tf-idf and Doc2Vec models. In our experiments, an accuracy of 60% was obtained using tf-idf features and 57% using Doc2Vec method for models based on LinearSVC model.","","978-1-7281-6251-5","10.1109/BigData50022.2020.9377825","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9377825","","Cyberspace;Machine learning;Big Data;Tools;Natural language processing;Stakeholders;Load modeling","document handling;learning (artificial intelligence);natural language processing;security of data","machine learning models;cyber-attacks;cyber-physical systems;complex number;heterogeneous devices;power capabilities;single security plan;cyber-alerts;false positives;natural language processing;attack consequences;tf-idf features;Doc2Vec method;LinearSVC model","","3","","28","IEEE","19 Mar 2021","","","IEEE","IEEE Conferences"
"Email Embeddings for Phishing Detection","L. F. Gutiérrez; F. Abri; M. Armstrong; A. S. Namin; K. S. Jones","Department of Computer Science, Texas Tech University; Department of Computer Science, Texas Tech University; Department of Psychological Sciences, Texas Tech University; Department of Computer Science, Texas Tech University; Department of Psychological Sciences, Texas Tech University","2020 IEEE International Conference on Big Data (Big Data)","19 Mar 2021","2020","","","2087","2092","The problem of detecting phishing emails through machine learning techniques has been discussed extensively in the literature. Conventional and state-of-the-art machine learning algorithms have demonstrated the possibility of building classifiers with high accuracy. The existing research studies treat phishing and genuine emails through general indicators and thus it is not exactly clear what phishing features are contributing to variations of the classifiers. In this paper, we crafted a set of phishing and legitimate emails with similar indicators in order to investigate whether these cues are captured or disregarded by email embeddings, i.e., vectorizations. We then fed machine learning classifiers with the carefully crafted emails to find out about the performance of email embeddings developed. Our results show that using these indicators, email embeddings techniques is effective for classifying emails as phishing or legitimate.","","978-1-7281-6251-5","10.1109/BigData50022.2020.9377821","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9377821","Natural Language Processing;Phishing Emails;Email Embeddings","Machine learning algorithms;Phishing;Conferences;Natural languages;Machine learning;Big Data;Electronic mail","computer crime;learning (artificial intelligence);pattern classification;unsolicited e-mail","genuine emails;general indicators;phishing features;legitimate emails;email embeddings;embeddings techniques;phishing detection;phishing emails;machine learning techniques","","5","","24","IEEE","19 Mar 2021","","","IEEE","IEEE Conferences"
"Predicting Emotions Perceived from Sounds","F. Abri; L. F. Gutiérrez; A. Siami Namin; D. R. W. Sears; K. S. Jones","Computer Science Department, Texas Tech University; Computer Science Department, Texas Tech University; Computer Science Department, Texas Tech University; College of Visual and Performing Arts, Texas Tech University; Department of Psychological Sciences, Texas Tech University","2020 IEEE International Conference on Big Data (Big Data)","19 Mar 2021","2020","","","2057","2064","Sonification is the science of communication of data and events to users through sounds. Auditory icons, earcons, and speech are the common auditory display schemes utilized in sonification, or more specifically in the use of audio to convey information. Once the captured data are perceived, their meanings, and more importantly, intentions can be interpreted more easily and thus can be employed as a complement to visualization techniques. Through auditory perception it is possible to convey information related to temporal, spatial, or some other context-oriented information. An important research question is whether the emotions perceived from these auditory icons or earcons are predictable in order to build an automated sonification platform. This paper conducts an experiment through which several mainstream and conventional machine learning algorithms are developed to study the prediction of emotions perceived from sounds. To do so, the key features of sounds are captured and then are modeled using machine learning algorithms using feature reduction techniques. We observe that it is possible to predict perceived emotions with high accuracy. In particular, the regression based on Random Forest demonstrated its superiority compared to other machine learning algorithms.","","978-1-7281-6251-5","10.1109/BigData50022.2020.9377842","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9377842","Emotion prediction;perceived emotion;sound;machine learning;Emo-Soundscape","Machine learning algorithms;Conferences;Data visualization;Big Data;Sonification;Audio user interfaces;Random forests","acoustic signal processing;affective computing;auditory displays;emotion recognition;feature extraction;random forests;regression analysis","emotions prediction;auditory icons;earcons;visualization techniques;auditory perception;automated sonification platform;machine learning;feature reduction;auditory display schemes;random forest","","3","","23","IEEE","19 Mar 2021","","","IEEE","IEEE Conferences"
"Google Trends Analysis of COVID-19 Pandemic","Z. Pan; H. L. Nguyen; H. Abu-gellban; Y. Zhang","Department of Computer Science, Texas Tech University; College of Engineering, Texas Tech University; Department of Computer Science, Texas Tech University; Department of Computer Science, Texas Tech University","2020 IEEE International Conference on Big Data (Big Data)","19 Mar 2021","2020","","","3438","3446","The World Health Organization (WHO) announced that COVID-19 was a pandemic disease on the 11th of March as there were 118K cases in several countries and territories. Numerous researchers worked on forecasting the number of confirmed cases since anticipating the growth of the cases helps governments adopting knotty decisions to ease the lockdowns orders for their countries. These orders help several people who have lost their jobs and support gravely impacted businesses. Our research aims to investigate the relation between Google search trends and the spreading of the novel coronavirus (COVID-19) over countries worldwide, to predict the number of cases. We perform a correlation analysis on the keywords of the related Google search trends according to the number of confirmed cases reported by the WHO. After that, we applied several machine learning techniques (Multiple Linear Regression, Nonnegative Integer Regression, Deep Neural Network), to forecast the number of confirmed cases globally based on historical data as well as the hybrid data (Google search trends). Our results show that Google search trends are highly associated with the number of reported confirmed cases, where the Deep Learning approach outperforms other forecasting techniques. We believe that it is not only a promising approach for forecasting the confirmed cases of COVID-19, but also for similar forecasting problems that are associated with the related Google trends.","","978-1-7281-6251-5","10.1109/BigData50022.2020.9377852","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9377852","coronavirus;covid’19;forecasting;search trends;neural networks;machine learning;spatio-temporal analysis","COVID-19;Pandemics;Neural networks;Predictive models;Market research;Internet;Forecasting","deep learning (artificial intelligence);diseases;information retrieval;medical computing;regression analysis;search engines","Google trends analysis;COVID-19 pandemic;World Health Organization;pandemic disease;related Google search trends;reported confirmed cases","","4","","39","IEEE","19 Mar 2021","","","IEEE","IEEE Conferences"
"Linguistic Features for Detecting Fake Reviews","F. Abri; L. F. Gutiérrez; A. S. Namin; K. S. Jones; D. R. W. Sears","Department of Computer Science, Texas Tech University; Department of Computer Science, Texas Tech University; Department of Computer Science, Texas Tech University; Department of Psychological Sciences, Texas Tech University; Performing Arts Research Lab, Texas Tech University","2020 19th IEEE International Conference on Machine Learning and Applications (ICMLA)","23 Feb 2021","2020","","","352","359","Online reviews play an integral part for success or failure of businesses. Prior to purchasing services or goods, customers first review the online comments submitted by previous customers. However, it is possible to superficially boost or hinder some businesses through posting counterfeit and fake reviews. This paper explores a natural language processing approach to identify fake reviews. We present a detailed analysis of linguistic features for distinguishing fake and trustworthy online reviews. We study 15 linguistic features and measure their significance and importance towards the classification schemes employed in this study. Our results indicate that fake reviews tend to include more redundant terms and pauses, and generally contain longer sentences. The application of several machine learning classification algorithms revealed that we were able to discriminate fake from real reviews with high accuracy using these linguistic features.","","978-1-7281-8470-8","10.1109/ICMLA51294.2020.00063","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9356253","fake review;deception detection;machine learning;linguistic features","Conferences;Machine learning;Linguistics;Feature extraction;Natural language processing;Classification algorithms;Business","Internet;learning (artificial intelligence);linguistics;natural language processing;pattern classification;purchasing;text analysis","fake reviews;trustworthy online reviews;study 15 linguistic features","","3","","30","IEEE","23 Feb 2021","","","IEEE","IEEE Conferences"
"Optimizing CNN using Fast Fourier Transformation for Object Recognition","V. Nair; M. Chatterjee; N. Tavakoli; A. S. Namin; C. Snoeyink","Department of Computer Science, Texas Tech University; Department of Computer Science, Texas Tech University; Department of Computer Science, Georgia Institute of Technology; Department of Computer Science, Texas Tech University; Department of Mechanical and Aerospace Engineering, State University of New York at Buffalo","2020 19th IEEE International Conference on Machine Learning and Applications (ICMLA)","23 Feb 2021","2020","","","234","239","This paper proposes to use Fast Fourier Transformation-based U-Net (a refined ""fully convolutional networks"") and perform image convolution in neural networks. Leveraging the Fast Fourier Transformation, it reduces the image convolution costs involved in the Convolutional Neural Networks (CNNs) and thus reduces the overall computational costs. The proposed model identifies the object information from the images. We apply the Fast Fourier transform algorithm on an image data set to obtain more accessible information about the image data, before segmenting them through the U-Net architecture. More specifically, we implement the FFT-based convolutional neural network to improve the training time of the network. The proposed approach was applied to publicly available Broad Bioimage Benchmark Collection (BBBC) dataset. Our model demonstrated improvement in training time during convolution from 600 - 700 ms/step to 400-500 ms/step. We evaluated the accuracy of our model using Intersection over Union (IoU) metric showing significant improvements.","","978-1-7281-8470-8","10.1109/ICMLA51294.2020.00046","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9356208","Object Recognition;CNN;Fast Fourier Transformation;Image Processing","Training;Measurement;Convolution;Biological system modeling;Neural networks;Object recognition;Convolutional neural networks","convolutional neural nets;fast Fourier transforms;image classification;image segmentation;learning (artificial intelligence);object recognition","refined fully convolutional networks;image convolution costs;object information;image data;FFT-based convolutional neural network;object recognition;fast Fourier transformation-based U-Net;BBBC dataset;Broad Bioimage Benchmark Collection dataset","","1","","27","IEEE","23 Feb 2021","","","IEEE","IEEE Conferences"
"ContiMap: Continuous Heatmap for Large Time Series Data","V. Pham; N. Nguyen; T. Dang","Computer Science Department, Texas Tech University; Computer Science Department, Texas Tech University; Computer Science Department, Texas Tech University","2020 Visualization in Data Science (VDS)","18 Feb 2021","2020","","","42","51","Limited human cognitive load, limited computing resources, and finite display resolutions are the major obstacles for developing interactive visualization systems in large-scale data analysis. Recent technological innovation has significantly improved computing power, such as faster CPUs and GPUs, as well as display resources, including ultra-high-resolution displays and video walls. However, large and complex data is still ahead in the run as we are generating huge amounts of data daily. Our strategy to bridge these gaps is to present the right amount of information through the use of compelling graphics. This paper proposes an approximation algorithm and a web prototype for representing a high-level abstraction of time series based on heatmap designs. Our approach aims to handle a significant amount of time series data arising from various application domains, such as cybersecurity, sensor network, and gene expression analysis.","","978-1-7281-9284-0","10.1109/VDS51726.2020.00009","Texas Tech University; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355216","Continuous heatmap;time-series visualization;approximation algorithm;multivariate data analysis","Heating systems;Visualization;Technological innovation;Time series analysis;Data visualization;Rendering (computer graphics);Time factors","cognition;data analysis;data visualisation;interactive systems;time series","continuous heatmap;time series data;human cognitive load;computing resources;finite display resolutions;interactive visualization systems;large-scale data analysis;recent technological innovation;display resources;ultra-high-resolution displays;video walls;complex data;high-level abstraction;heatmap designs","","2","","49","IEEE","18 Feb 2021","","","IEEE","IEEE Conferences"
"Blockchain Based End-to-End Tracking System for Distributed IoT Intelligence Application Security Enhancement","L. Xu; Z. Gao; X. Fan; L. Chen; H. Kim; T. Suh; W. Shi","Computer Science Department, University of Texas Rio Grande Valley; Computer Science Department, Auburn University at Montgomery; IoTeX; Computer Science Department, Texas Tech University; Computer Science Department, Korea University; Computer Science Department, Korea University; Computer Science Department, University of Houston","2020 IEEE 19th International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom)","9 Feb 2021","2020","","","1028","1035","IoT devices provide a rich data source that is not available in the past, which is valuable for a wide range of intelligence applications, especially deep neural network (DNN) applications that are data-thirsty. An established DNN model provides useful analysis results that can improve the operation of IoT systems in turn. The progress in distributed/federated DNN training further unleashes the potential of integration of IoT and intelligence applications. When a large number of IoT devices are deployed in different physical locations, distributed training allows training modules to be deployed to multiple edge data centers that are close to the IoT devices to reduce the latency and movement of large amounts of data. In practice, these IoT devices and edge data centers are usually owned and managed by different parties, who do not fully trust each other or have conflicting interests. It is hard to coordinate them to provide end-to-end integrity protection of the DNN construction and application with classical security enhancement tools. For example, one party may share an incomplete data set with others, or contribute a modified sub DNN model to manipulate the aggregated model and affect the decision-making process. To mitigate this risk, we propose a novel blockchain based end-toend integrity protection scheme for DNN applications integrated with an IoT system in the edge computing environment. The protection system leverages a set of cryptography primitives to build a blockchain adapted for edge computing that is scalable to handle a large number of IoT devices. The customized blockchain is integrated with a distributed/federated DNN to offer integrity and authenticity protection services.","2324-9013","978-1-6654-0392-4","10.1109/TrustCom50675.2020.00137","National Research Foundation of Korea(grant numbers:NRF-2019R1A2C1088390); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9343035","Blockchain;IoT;DNN;Security","Training;Data centers;Computational modeling;Blockchain;Tools;Data models;Edge computing","cryptography;data integrity;Internet of Things;learning (artificial intelligence);neural nets","end-to-end integrity protection scheme;modified sub DNN model;incomplete data set;multiple edge data centers;distributed training;established DNN model;data-thirsty;deep neural network applications;intelligence applications;rich data source;distributed IoT intelligence application security enhancement;IoT devices;IoT system;DNN applications","","","","23","IEEE","9 Feb 2021","","","IEEE","IEEE Conferences"
"FPGA Based Blockchain System for Industrial IoT","L. Xu; L. Chen; Z. Gao; H. Kim; T. Suh; W. Shi","Computer Science Department, University of Texas Rio Grande Valley; Computer Science Department, Texas Tech University; Computer Science Department, Auburn University at Montgomery; Computer Science Department, Auburn University at Montgomery; Computer Science Department, Korea University; Computer Science Department, University of Houston","2020 IEEE 19th International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom)","9 Feb 2021","2020","","","876","883","Industrial IoT (IIoT) is critical for industrial infrastructure modernization and digitalization. Therefore, it is of utmost importance to provide adequate protection of the IIoT system. A modern IIoT system usually consists of a large number of devices that are deployed in multiple locations and owned/managed by different entities who do not fully trust each other. These features make it harder to manage the system in a coherent manner and utilize existing security mechanisms to offer adequate protection. The emerging blockchain technology provides a powerful tool for IIoT system management and protection because the IIoT nature of distributed deployment and involvement of multiple stakeholders fits the design philosophy of blockchain well. Most existing blockchain construction mechanisms are not scalable enough and too heavy for an IIoT system. One promising way to overcome these limitations is utilizing hardware based trusted execution environment (TEE) in blockchain construction. However, most of the existing works on this direction do not consider the characteristics of IIoT devices (e.g., fixed functionality and limited supply) and face several limitations when they are applied for IIoT system management and protection, such as high energy consumption, single root-oftrust, and low decentralization level. To mitigate these challenges, we propose a novel field programmable gate array (FPGA) based blockchain system. It leverages the FPGA to build a simple but efficient TEE for IIoT devices, and removes the single root-of-trust by allowing all stakeholders to participate in the management of the devices. The FPGA based blockchain system shifts the computation/storage intensive part of blockchain management to more powerful computers but still involves the IIoT devices in the block construction to achieve a high level of decentralization. We implement the major FPGA components of the design and evaluate the performance of the whole system with a simulation tool to demonstrate its feasibility for IIoT applications.","2324-9013","978-1-6654-0392-4","10.1109/TrustCom50675.2020.00118","National Research Foundation of Korea(grant numbers:NRF-2019R1A2C1088390); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9343149","Industrial IoT;FPGA;TEE;blockchain","Blockchain;Tools;Hardware;Stakeholders;Security;Field programmable gate arrays;Industrial Internet of Things","blockchains;field programmable gate arrays;Internet of Things;trusted computing","IIoT devices;IIoT system management;root-of-trust;field programmable gate array based blockchain system;FPGA based blockchain system;blockchain management;IIoT applications;industrial IoT;industrial infrastructure modernization;digitalization;modern IIoT system;security mechanisms;emerging blockchain technology;IIoT nature;distributed deployment;blockchain construction mechanisms;hardware based trusted execution environment","","3","","28","IEEE","9 Feb 2021","","","IEEE","IEEE Conferences"
"A Survey of Real-Time Social-Based Traffic Detection","H. Abu-gellban","Department of Computer Science, Texas Tech University, USA","2020 IEEE International Conference on Intelligence and Security Informatics (ISI)","8 Dec 2020","2020","","","1","6","Online traffic news web sites do not always announce traffic events in areas in real-time. There is a capability to employ text mining and machine learning techniques on the Twitter stream to perform event detection, to develop a real-time traffic detection system. In this present survey paper, we deliberate the current state-of-art techniques in detecting traffic events in real-time focusing on seven papers [1], [2], [3], [4], [5], [6], [7]. Lastly, applying effective text mining techniques and the SVM classifier in paper [2] gave the best results (i.e., 95.75% accuracy and 95.8% F1-score).","","978-1-7281-8800-3","10.1109/ISI49825.2020.9280534","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9280534","Traffic detection;monitoring social networks;Twitter data stream","Text mining;Support vector machines;Social networking (online);Blogs;Machine learning;Real-time systems;Web sites","data mining;learning (artificial intelligence);pattern classification;real-time systems;social networking (online);support vector machines;text analysis;traffic information systems","traffic event detection;text mining;Twitter stream;real time social based traffic detection;online traffic news Web sites;SVM classifier;machine learning","","4","","23","IEEE","8 Dec 2020","","","IEEE","IEEE Conferences"
"Ensemble Learning With Attention-Integrated Convolutional Recurrent Neural Network for Imbalanced Speech Emotion Recognition","X. Ai; V. S. Sheng; W. Fang; C. X. Ling; C. Li","Software and Service Outsourcing College, Suzhou Vocational Institute of Industrial Technology, Suzhou, China; Department of Computer Science, Texas Tech University, Lubbock, TX, USA; Jiangsu Engineering Center of Network Monitoring, School of Computer and Software, Nanjing University of Information Science and Technology, Nanjing, China; Department of Computer Science, Western University, London, ON, Canada; Software and Service Outsourcing College, Suzhou Vocational Institute of Industrial Technology, Suzhou, China","IEEE Access","10 Nov 2020","2020","8","","199909","199919","This article addresses observation duplication and lack of whole picture problems for ensemble learning with the attention model integrated convolutional recurrent neural network (ACRNN) in imbalanced speech emotion recognition. Firstly, we introduce Bagging with ACRNN and the observation duplication problem. Then Redagging is devised and proved to address the observation duplication problem by generating bootstrap samples from permutations of observations. Moreover, Augagging is proposed to get oversampling learner to participate in majority voting for addressing the lack of whole picture problem. Finally, Extensive experiments on IEMOCAP and Emo-DB samples demonstrate the superiority of our proposed methods (i.e., Redagging and Augagging).","2169-3536","","10.1109/ACCESS.2020.3035910","National Natural Science Foundation of China(grant numbers:61702351); Natural Science Foundation of the Jiangsu Higher Education Institutions of China(grant numbers:17KJB520036); Key Laboratory in Science and Technology Development Project of Suzhou(grant numbers:SZS201609); Suzhou Science and Technology Plan Project(grant numbers:SYG201903); Computer Basic Education Teaching Research Project(grant numbers:2018-AFCEC-328,2019-AFCEC-288); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9247946","Imbalance learning;ensemble learning;convolutional neural network;recurrent neural network;speech emotion recognition","Emotion recognition;Recurrent neural networks;Speech recognition;Bagging","convolutional neural nets;emotion recognition;learning (artificial intelligence);recurrent neural nets;speech recognition","attention model integrated convolutional recurrent neural network;ACRNN;observation duplication problem;ensemble learning;speech emotion recognition;Redagging;Augagging","","7","","38","CCBY","4 Nov 2020","","","IEEE","IEEE Journals"
"Context-Aware Autonomous Security Assertion for Industrial IoT","U. Tariq; A. O. Aseeri; M. S. Alkatheiri; Y. Zhuang","Department of Information Systems, College of Computer Engineering and Sciences, Prince Sattam Bin Abdulaziz University, Al-Kharj, Saudi Arabia; Department of Computer Science, College of Computer Engineering and Sciences, Prince Sattam Bin Abdulaziz University, Al-Kharj, Saudi Arabia; College of Computer Science and Engineering, University of Jeddah, Jeddah, Saudi Arabia; Department of Computer Science, Texas Tech University, Lubbock, TX, USA","IEEE Access","27 Oct 2020","2020","8","","191785","191794","The Industrial Internet of Things (IIoT) platform consists of purpose-driven communication controllers, enterprise-grade modems (routers and gateways), and edge computing systems that require integrated software and sensing capability in mission-critical environments. Extensible purpose-built industrial supervisory control and data acquisition networks are prone to numerous cybersecurity threats. In this paper, the historical databased qualitative threat assessment was part of the comprehensive risk breakdown (i.e., to quantify assessment and remediation) of the practicing industry (i.e., systems that rely on robotics, big data & analytics). Furthermore, a risk and operability (HAZOP & convolution neural-network) evaluation was proved to be the paramount study for autonomous vulnerability assessment. Through autonomous network management, continuous software monitoring, data-driven device insights, and integrated content filtering, the proposed endpoint protection scheme shows significant improvement in preventing data breaches, denial of service (DoS), and malware detection. A distinctive computational methodology to determine the cyber risk for industrial structures with IoT-explicit control factors has been programmed and elucidated in the perspective of IIoT systems. Firmware driven emulation (integrated and optimized) outcome aided to reduce breach ratio, better incident detection, and enhanced protection of confidential data.","2169-3536","","10.1109/ACCESS.2020.3032436","Deputyship for Research and Innovation, Ministry of Education, Saudi Arabia(grant numbers:383); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9233419","Industrial Internet of Things;HAZOP analysis;hybrid integrity model;multicriteria decision analysis;convolution neural-network","Peer-to-peer computing;Logic gates;Sensors;Computer security;Software","Big Data;computer network management;computer network security;convolutional neural nets;firmware;Internet of Things;invasive software;mobile robots;SCADA systems","context-aware autonomous security assertion;industrial IoT;communication controllers;edge computing systems;integrated software;sensing capability;mission-critical environments;supervisory control and data acquisition networks;comprehensive risk breakdown;big data;HAZOP;autonomous vulnerability assessment;autonomous network management;continuous software monitoring;data-driven device insights;integrated content filtering;data breaches;distinctive computational methodology;cyber risk;industrial structures;IoT-explicit control factors;IIoT systems;confidential data protection;convolution neural-network;databased qualitative threat assessment;cybersecurity threats;industrial Internet of Things platform;enterprise-grade modems;endpoint protection scheme;denial of service;malware detection;firmware driven emulation","","8","","24","CCBY","20 Oct 2020","","","IEEE","IEEE Journals"
"Towards Variability-Aware Legal-GRL Framework for Modeling Compliance Requirements","S. Sartoli; S. Ghanavati; A. Siami Namin","Department of Computer Science, University of North Georgia, Dahlonega, Georgia, USA; School of Computing and Information Science, University of Maine, Orono, ME, USA; Department of Computer Science, Texas Tech University, Lubbock, TX, USA","2020 IEEE 7th International Workshop on Evolving Security & Privacy Requirements Engineering (ESPRE)","14 Oct 2020","2020","","","7","12","The increasing adoption of cloud computing is making operating environments highly dynamic and changing. Once an operating environment condition (e.g., geographical location of data) changes, the compliance requirements might alsochange. To ensure that compliance requirements are continuouslymet, there is a need for frameworks that not only support modeling regulations, but also capture the potential environment variabilities and conditions in a systematic way. This paper introduces Variability-Aware Legal-GRL (Goal-oriented Requirements Language) framework for modeling compliance requirements in the presence of runtime changes. We extend the Goal-oriented Requirements Language (GRL) with new elements and model construction rules to model context-aware privacy policies for dynamic multi-jurisdictional domains as well as features for monitoring changes that trigger adaptation. We motivate and illustrate the proposed framework using Health Insurance Portability and Accountability Act (HIPAA) and Personal Health Information Protection Act (PHIPA) statements. The proposed modeling framework allows software engineers to automatically quantify and analyze satisfaction level of security and privacy related top level goals for multiple software design alternatives and thus, choose the best set of privacy measures.","","978-1-7281-8346-6","10.1109/ESPRE51200.2020.00007","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9223990","Adaptive Software;Adaptive Privacy;Legal Requirements;Goal Modeling;Dynamic Access Control","Adaptation models;Privacy;Systematics;Software design;Runtime;Software;Regulation","cloud computing;data privacy;formal specification;security of data;software engineering;ubiquitous computing","environment variabilities;modeling compliance requirements;runtime changes;model construction rules;context-aware privacy policies;monitoring changes;operating environments;operating environment condition;modeling regulations;Variability-Aware Legal-GRL framework","","","","22","IEEE","14 Oct 2020","","","IEEE","IEEE Conferences"
"Security Fault Tolerance for Access Control","D. Jang; M. Shin; D. Pathirage","Department of Computer Science, Texas Tech University, Lubbock, Texas, USA; Department of Computer Science, Texas Tech University, Lubbock, Texas, USA; Department of Computer Science, Texas Tech University, Lubbock, Texas, USA","2020 IEEE International Conference on Autonomic Computing and Self-Organizing Systems Companion (ACSOS-C)","15 Sep 2020","2020","","","212","217","This paper describes an approach to the security fault tolerance of access control in which the security breaches of an access control are tolerated by means of a security fault tolerant (SFT) access control. Though an access control is securely designed and implemented, it can contain faults in development or be contaminated in operation. The threats to an access control are analyzed to identify possible security breaches. To tolerate the security breaches, an SFT access control is made to be semantically identical to an access control. Our approach is described using role-based access control (RBAC) and extended access control list (EACL). A healthcare system is used to demonstrate our approach.","","978-1-7281-8414-2","10.1109/ACSOS-C51401.2020.00058","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9196330","security fault tolerance;access control;RBAC;EACL","Fault tolerance;Fault tolerant systems;Permission;Authorization;Authentication","authorisation;fault tolerant computing;health care","security fault tolerance;security fault tolerant access control;security breaches;SFT access control;role-based access control;extended access control list;EACL;RBAC;healthcare system","","","","22","IEEE","15 Sep 2020","","","IEEE","IEEE Conferences"
"A Roadmap to Domain Knowledge Integration in Machine Learning","H. D. Gupta; V. S. Sheng","Department of Computer Science, Texas Tech University, Lubbock, TX; Department of Computer Science, Texas Tech University, Lubbock, TX","2020 IEEE International Conference on Knowledge Graph (ICKG)","11 Sep 2020","2020","","","145","151","Many machine learning algorithms have been developed in recent years to enhance the performance of a model in different aspects of artificial intelligence. But the problem persists due to inadequate data and resource. Integrating knowledge in a machine learning model can help to overcome these obstacles up to a certain degree. Incorporating knowledge is a complex task though because of various forms of knowledge representation. In this paper, we will give a brief overview of these different forms of knowledge integration and their performance in certain machine learning tasks.","","978-1-7281-8156-1","10.1109/ICBK50248.2020.00030","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9194557","knowledge;domain;loss;constraint","Machine learning;Mathematical model;Task analysis;Neural networks;Machine learning algorithms;Knowledge representation","knowledge representation;learning (artificial intelligence)","artificial intelligence;knowledge representation;machine learning tasks;domain knowledge integration","","1","","23","IEEE","11 Sep 2020","","","IEEE","IEEE Conferences"
"Detecting host location attacks in SDN-based networks","S. S. Baidya; R. Hewett","Department of Computer Science, Texas Tech University, Lubbock, USA; Department of Computer Science, Texas Tech University, Lubbock, USA","2020 29th Wireless and Optical Communications Conference (WOCC)","11 Jun 2020","2020","","","1","6","Software Defined Networking (SDN) is an emerging technology that has increasingly become popular for implementing modern infrastructures. SDN offers advantages of programmable and flexible network management over the traditional practice. As more and more SDN-based networks are being implemented, it is necessary to consider security issues especially those that are inherent from SDN. This paper addresses an important SDN specific security issue, namely a host location (tracking) attack, where an attacker compromises a host and captures its location information to manipulate the packets and trick the controller. Such an attack can potentially lead to many harmful effects including disruption of network traffic and denial of services. In particular, we introduce a new host location attack that exploits unused ports, along with its countermeasure for the controller to detect and take appropriate actions. We illustrate and evaluate the proposed detection mechanism by network simulations. The results obtained from our experiments are effective and promising.","2379-1276","978-1-7281-6124-2","10.1109/WOCC48579.2020.9114932","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9114932","Software-Defined Networking;SDN security;host discovery;host hijacking attacks","Ports (computers);Wireless communication;Toxicology;Network topology;Optical switches;Telecommunication traffic;Optical variables measurement","computer network management;computer network security;software defined networking;telecommunication traffic","host location attack detection;SDN specific security issue;network traffic;location information;flexible network management;software defined networking;SDN-based networks;network simulations","","","","16","IEEE","11 Jun 2020","","","IEEE","IEEE Conferences"
"Loss Functions of Generative Adversarial Networks (GANs): Opportunities and Challenges","Z. Pan; W. Yu; B. Wang; H. Xie; V. S. Sheng; J. Lei; S. Kwong","School of Computer and Software, Nanjing University of Information Science and Technology, Nanjing, China; School of Computer and Software, Nanjing University of Information Science and Technology, Nanjing, China; School of Computer and Software, Nanjing University of Information Science and Technology, Nanjing, China; Department of Computing and Decision Sciences, Lingnan University, Hong Kong, China; Department of Computer Science, Texas Tech University, Lubbock, USA; School of Electrical and Information Engineering, Tianjin University, Tianjin, China; Department of Computer Science, City University of Hong Kong, Hong Kong, China","IEEE Transactions on Emerging Topics in Computational Intelligence","21 Jul 2020","2020","4","4","500","522","Recently, the Generative Adversarial Networks (GANs) are fast becoming a key promising research direction in computational intelligence. To improve the modeling ability of GANs, loss functions are used to measure the differences between samples generated by the model and real samples, and make the model learn towards the goal. In this paper, we perform a survey for the loss functions used in GANs, and analyze the pros and cons of these loss functions. Firstly, the basic theory of GANs, and its training mechanism are introduced. Then, the loss functions used in GANs are summarized, including not only the objective functions of GANs, but also the application-oriented GANs' loss functions. Thirdly, the experiments and analyses of representative loss functions are discussed. Finally, several suggestions on how to choose appropriate loss functions in a specific task are given.","2471-285X","","10.1109/TETCI.2020.2991774","National Natural Science Foundation of China(grant numbers:61971232); Six Talent Peaks Project of Jiangsu Province(grant numbers:XYDXXJS-041); Natural Science Foundation of Tianjin City(grant numbers:18ZXZNGX00110,18JCJQJC45800); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9098081","Loss functions;generative adversarial networks (GANs);deep learning;machine learning;computational intelligence","Gallium nitride;Computational modeling;Generators;Generative adversarial networks;Training;Task analysis;Linear programming","learning (artificial intelligence);neural nets","generative adversarial networks;objective functions;loss functions;computational intelligence;deep learning","","22","","97","IEEE","21 May 2020","","","IEEE","IEEE Journals"
"Automated Performance Modeling of HPC Applications Using Machine Learning","J. Sun; G. Sun; S. Zhan; J. Zhang; Y. Chen","School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; Department of Computer Science, Texas Tech University, Lubbock, USA","IEEE Transactions on Computers","7 Apr 2020","2020","69","5","749","763","Automated performance modeling and performance prediction of parallel programs are highly valuable in many use cases, such as in guiding task management and job scheduling, offering insights of application behaviors, and assisting resource requirement estimation. The performance of parallel programs is affected by numerous factors, including but not limited to hardware, applications, algorithms, and input parameters, thus an accurate performance prediction is often a challenging and daunting task. In this article, we focus on automatically predicting the execution time of parallel programs (more specifically, MPI programs) with different inputs, at different scales, and without domain knowledge. We model the correlation between the execution time and domain-independent runtime features. These features include values of variables, counters of branches, loops, and MPI communications. Through automatically instrumenting an MPI program, each execution of the program will output a feature vector and its corresponding execution time. After collecting data from executions with different inputs, a random forest machine learning approach is used to build an empirical performance model, which can predict the execution time of the program given a new input. A transfer learning method is used to reuse an existing performance model and improve the prediction accuracy on a new platform that lacks historical execution data. Our experiments and analyses of three parallel applications, Graph500, GalaxSee, and SMG2000, on three different systems confirm that our method performs well, with less than 20 percent prediction error on average.","1557-9956","","10.1109/TC.2020.2964767","National Natural Science Foundation of China(grant numbers:61772485,61432016); State Key Laboratory of Computer Architecture, Institute of Computing Technology, CAS(grant numbers:CARCH201711); Youth Innovation Promotion Association of the Chinese Academy of Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8956059","Parallel computing;performance modeling;machine learning;model transferring","Instruments;Computational modeling;Predictive models;Runtime;Data models;Machine learning;Feature extraction","application program interfaces;learning (artificial intelligence);message passing;parallel programming;software performance evaluation","HPC applications;machine learning;automated performance modeling;parallel programs;task management;job scheduling;application behaviors;resource requirement estimation;MPI program;domain-independent runtime features;empirical performance model;transfer learning method;historical execution data","","16","","45","IEEE","10 Jan 2020","","","IEEE","IEEE Journals"
"PRS: A Pattern-Directed Replication Scheme for Heterogeneous Object-Based Storage","J. Zhou; Y. Chen; W. Xie; D. Dai; S. He; W. Wang","Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Department of Computer Science, Texas Tech University, Lubbock, USA; Department of Computer Science, Texas Tech University, Lubbock, USA; Department of Computer Science, College of Computing and Informatics, University of North Carolina at Charlotte, Charlotte, USA; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Computers","11 Mar 2020","2020","69","4","591","605","Data replication is a key technique to achieve high data availability, reliability, and optimized performance in distributed storage systems. In recent years, with emerged new storage devices, heterogeneous object-based storage systems, such as a storage system with a mix of hard disk drives, solid state drives, and other non-volatile memory devices have become increasingly attractive since they combine the merits of different storage devices to deliver better promises. However, existing data replication schemes do not well consider distinct characteristics of heterogeneous storage devices yet, which could lead to suboptimal performance. This article introduces a new data replication scheme called Pattern-directed Replication Scheme (PRS) to achieve efficient data replication for heterogeneous storage systems. Different from traditional schemes, the PRS selectively replicates data objects and distributes replicas to various storage devices based on their characteristics. It aggregates objects that have I/O correlation into object groups by calculating object distance and makes replication for grouped objects according to application's data access pattern identified. In addition, the PRS uses a pseudo random algorithm to optimize replica placement by considering the storage device performance and capacity features. We have evaluated the pattern-directed replication scheme with extensive tests in Sheepdog, a typical object-based storage system. The experimental results confirm that it is a highly efficient replication scheme for heterogeneous storage systems. For instance, the read performance was improved by 105 percent to nearly 10x compared with existing replication schemes.","1557-9956","","10.1109/TC.2019.2954089","National Science Foundation(grant numbers:CNS-1338078,CNS-1362134,CCF-1409946,CCF-1718336,OAC-1835892,CNS-1817094); Beijing Municipal Science and Technology Commission(grant numbers:Z191100007119002); National Natural Science Foundation of China(grant numbers:61572377); Natural Science Foundation of Hubei Province(grant numbers:2017CFC889); Fundamental Research Funds for the Central Universities(grant numbers:2018QNA5015); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8906026","Data replication;heterogeneous storage;object-based storage;access pattern;data distribution","Performance evaluation;Correlation;Distributed databases;Reliability;Pattern analysis;Hard disks;Nonvolatile memory","data handling;input-output programs;replicated databases;storage management","PRS;pattern-directed replication scheme;distributed storage systems;heterogeneous object-based storage systems;data replication scheme;data objects;object groups;object distance;Sheepdog;I/O correlation;data access pattern","","9","","61","IEEE","19 Nov 2019","","","IEEE","IEEE Journals"
"A Holistic Heterogeneity-Aware Data Placement Scheme for Hybrid Parallel I/O Systems","S. He; Z. Li; J. Zhou; Y. Yin; X. Xu; Y. Chen; X. -H. Sun","College of Computer Science and Technology, Zhejiang University, Hangzhou, China; Computer Science Program, School of Business, Stockton University, Galloway, USA; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Zhejiang Lab, Intelligent Computing System Research Center, Institute of Artificial Intelligence, Hangzhou, China; Department of Computer Science, Kennesaw State University, Kennesaw, USA; Department of Computer Science, Texas Tech University, Lubbock, USA; Department of Computer Science, Illinois Institute of Technology, Chicago, USA","IEEE Transactions on Parallel and Distributed Systems","16 Jan 2020","2020","31","4","830","842","We present H2DP, a holistic heterogeneity-aware data placement scheme for hybrid parallel I/O systems, which consist of HDD servers and SSD servers. Most of the existing approaches focus on server performance or application I/O pattern heterogeneity in data placement. H2DP considers three axes of heterogeneity: server performance, server space, and application I/O pattern. More specifically, H2DP determines the optimized stripe sizes on servers based on server performance, keeps only critical data on all hybrid servers and the rest data on HDD servers, and dynamically migrates data among different types of servers at run-time. This holistic heterogeneity-awareness enables H2DP to achieve high performance by alleviating server load imbalance, efficiently utilizing SSD space, and accommodating application pattern variation. We have implemented a prototype of H2DP under MPICH2 atop OrangeFS. Extensive experimental results demonstrate that H2DP significantly improve I/O system performance compared to existing data placement schemes.","1558-2183","","10.1109/TPDS.2019.2948901","National Natural Science Foundation of China(grant numbers:61572377); Natural Science Foundation of Hubei Province(grant numbers:2017CFC889); Fundamental Research Funds for the Central Universities(grant numbers:2018QNA5015); Zhejiang Lab Research Project(grant numbers:2019KC0AC01); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8880508","Parallel I/O system;parallel file system;hybrid parallel file system;data placement;solid state drive","Servers;System performance;Bandwidth;Computer science;Distributed databases;Sun;File systems","computer centres;disc drives;file organisation;file servers;input-output programs;message passing;parallel processing;storage management","hybrid parallel I/O systems;pattern heterogeneity;SSD servers;holistic heterogeneity-aware data placement scheme;server load imbalance;HDD servers;rest data;hybrid servers;critical data;server performance;H2DP;server space","","6","","50","IEEE","24 Oct 2019","","","IEEE","IEEE Journals"
"A Highly Reliable Metadata Service for Large-Scale Distributed File Systems","J. Zhou; Y. Chen; W. Wang; S. He; D. Meng","Chinese Academy of Sciences, Beijing, China; Department of Computer Science, Texas Tech University, Lubbock, USA; Chinese Academy of Sciences, Beijing, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","10 Jan 2020","2020","31","2","374","392","Many massive data processing applications nowadays often need long, continuous, and uninterrupted data accesses. Distributed file systems are used as the back-end storage to provide the global namespace management and reliability guarantee. Due to increasing hardware failures and software issues with the growing system scale, metadata service reliability has become a critical issue as it has a direct impact on file and directory operations. Existing metadata management mechanisms can provide fault tolerance capability to some level but are inadequate. They often have limitations in system availability, state consistence, and performance overhead and lack an effective mechanism to offer metadata reliability. This paper introduces a novel highly reliable metadata service to address these issues in large-scale file systems. Different from traditional strategies, this proposed reliable metadata service adopts a new active-standby architecture for fault tolerance and uses a holistic approach to improve file system availability. A new shared storage pool (SSP) is designed for transparent metadata synchronization and replication between active and standby servers. Based on the SSP, a new policy called multiple actives multiple standbys (MAMS) is presented to perform metadata service recovery in case of failures. A new global state recovery strategy and a smart client fault tolerance mechanism are achieved to maintain the continuity of metadata service. We have implemented such highly reliable metadata service in a prototype file system CFS (Clover file system) and conducted extensive tests to evaluate it. Experimental results confirm that it can significantly improve file system reliability with fast failover under different failure scenarios while having negligible influence on performance. Compared with typical reliability designs in Hadoop Avatar, Hadoop HA, and Boom-FS file systems, the mean-time-to-recovery (MTTR) with the highly reliable metadata service was reduced by 80.23, 65.46 and 28.13 percent, respectively.","1558-2183","","10.1109/TPDS.2019.2937492","Beijing Municipal Science and Technology Commission(grant numbers:Z191100007119002); National Science Foundation(grant numbers:CNS-1338078,CNS-1362134,CCF-1409946,CCF-1718336,OAC-1835892,CNS-1817094); National Natural Science Foundation of China(grant numbers:61572377); Natural Science Foundation of Hubei Province(grant numbers:2017CFC889); Fundamental Research Funds for the Central Universities(grant numbers:2018QNA5015); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812918","Distributed file systems;metadata service;metadata reliability;fault tolerance;shared metadata storage","Metadata;Servers;Protocols;Synchronization;Fault tolerance;Fault tolerant systems","back-up procedures;client-server systems;data handling;distributed databases;fault tolerance;fault tolerant computing;meta data;parallel processing;reliability;software fault tolerance;storage management;system recovery;telecommunication network reliability","hardware failures;software issues;growing system scale;metadata service reliability;directory operations;metadata reliability;large-scale file systems;file system availability;transparent metadata synchronization;metadata service recovery;smart client fault tolerance mechanism;prototype file system CFS;Clover file system;file system reliability;typical reliability designs;Boom-FS file systems;large-scale distributed file systems;long data accesses;uninterrupted data accesses;global namespace management;reliability guarantee;continuous data accesses;highly reliable metadata service;metadata management mechanisms;efficiency 28.13 percent;FS","","6","","61","IEEE","26 Aug 2019","","","IEEE","IEEE Journals"
"A Survey of Machine Learning algorithms in EEG","H. -T. -T. Vo; L. -N. -T. Dang; V. -T. -N. Nguyen; V. -T. Huynh","Department of Physics and Computer Science, University of Science, VNU-HCM, Vietnam; Department of Physics and Computer Science, University of Science, VNU-HCM, Vietnam; Department of Computer Science, Texas Tech University, Lubbock, TX, USA; Department of Physics and Computer Science, University of Science, VNU-HCM, Vietnam","2019 6th NAFOSTED Conference on Information and Computer Science (NICS)","5 Mar 2020","2019","","","500","505","This research focuses on surveying machine learning algorithms and providing a suitable model to match electroencephalogram (EEG) signal classification needs. Data is recorded by self-written an application, including 11 states: the states related to eye behavior, facial expression, and thinking signals. Feature extraction is an essential step in the process of signal, and the Wavelet transform is used in this phase. The feature matrix is built with components calculated by using the Wavelet method. This paper also gives a piece of information about classical machine learning such as Support Vector Machine (SVM) and Bagged Tree (BT) and modern algorithms as Artificial Neural Network (ANN) and Deep Learning (DL). There are four main algorithms used to classify signals. The proposed scheme yielded significantly higher classification performances using machine learning classifiers compared to extant quantitative feature extraction. These results suggest the proposed feature extraction method reliably classifies EEG signals recorded during thinking with a higher degree of accuracy.","","978-1-7281-5163-2","10.1109/NICS48868.2019.9023884","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9023884","Artificial Neural Network;Bagged Tree;Deep Learning;EEG;Support Vector Machine;Wavelet","Support vector machines;Electroencephalography;Training;Brain modeling;Machine learning;Biological neural networks;Wavelet transforms","electroencephalography;feature extraction;learning (artificial intelligence);medical signal processing;neural nets;signal classification;support vector machines;trees (mathematics);wavelet transforms","machine learning algorithms;electroencephalogram signal classification;eye behavior;facial expression;thinking signals;Wavelet transform;feature matrix;artificial neural network;deep learning;feature extraction;EEG signal classification;support vector machine;bagged tree","","2","","14","IEEE","5 Mar 2020","","","IEEE","IEEE Conferences"
"HackerNets: Visualizing Media Conversations on Internet of Things, Big Data, and Cybersecurity","H. Van; H. N. Nguyen; R. Hewett; T. Dang","Department of Computer Science, Texas Tech University, Lubbock, Texas, USA; Department of Computer Science, Texas Tech University, Lubbock, Texas, USA; Department of Computer Science, Texas Tech University, Lubbock, Texas, USA; Department of Computer Science, Texas Tech University, Lubbock, Texas, USA","2019 IEEE International Conference on Big Data (Big Data)","24 Feb 2020","2019","","","3293","3302","The giant network of Internet of Things establishes connections between smart devices and people, with protocols to collect and share data. While the data is expanding at a fast pace in this era of Big Data, there are growing concerns about security and privacy policies. In the current Internet of Things ecosystems, at the intersection of the Internet of Things, Big Data, and Cybersecurity lies the subject that attracts the most attention. In aiding users in getting an adequate understanding, this paper introduces HackerNets, an interactive visualization for emerging topics in the crossing of IoT, Big Data, and Cybersecurity over time. To demonstrate the effectiveness and usefulness of HackerNets, we apply and evaluate the technique on the dataset from the social media platform.","","978-1-7281-0858-2","10.1109/BigData47090.2019.9006417","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9006417","Internet of Things;Cybersecurity;Big Data;Interactive Visualization;Venn Diagram;Media User Network;Social Media Mining","Data visualization;Big Data;Computer security;Social network services;Internet of Things;Computer science","data privacy;Internet;Internet of Things;social networking (online)","HackerNets;big data;cybersecurity;smart devices;Internet of Things;media conversation visualization;social media platform;privacy policies","","2","","42","IEEE","24 Feb 2020","","","IEEE","IEEE Conferences"
"The Performance of LSTM and BiLSTM in Forecasting Time Series","S. Siami-Namini; N. Tavakoli; A. S. Namin","Department of Math and Statistics, Texas Tech University; Department of Computer Science, Georgia Institute of Technology; Department of Computer Science, Texas Tech University","2019 IEEE International Conference on Big Data (Big Data)","24 Feb 2020","2019","","","3285","3292","Machine and deep learning-based algorithms are the emerging approaches in addressing prediction problems in time series. These techniques have been shown to produce more accurate results than conventional regression-based modeling. It has been reported that artificial Recurrent Neural Networks (RNN) with memory, such as Long Short-Term Memory (LSTM), are superior compared to Autoregressive Integrated Moving Average (ARIMA) with a large margin. The LSTM-based models incorporate additional “gates” for the purpose of memorizing longer sequences of input data. The major question is that whether the gates incorporated in the LSTM architecture already offers a good prediction and whether additional training of data would be necessary to further improve the prediction. Bidirectional LSTMs (BiLSTMs) enable additional training by traversing the input data twice (i.e., 1) left-to-right, and 2) right-to-left). The research question of interest is then whether BiLSTM, with additional training capability, outperforms regular unidirectional LSTM. This paper reports a behavioral analysis and comparison of BiLSTM and LSTM models. The objective is to explore to what extend additional layers of training of data would be beneficial to tune the involved parameters. The results show that additional training of data and thus BiLSTM-based modeling offers better predictions than regular LSTM-based models. More specifically, it was observed that BiLSTM models provide better predictions compared to ARIMA and LSTM models. It was also observed that BiLSTM models reach the equilibrium much slower than LSTM-based models.","","978-1-7281-0858-2","10.1109/BigData47090.2019.9005997","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9005997","","Biological system modeling;Training;Data models;Logic gates;Time series analysis;Predictive models;Recurrent neural networks","autoregressive moving average processes;learning (artificial intelligence);mathematics computing;recurrent neural nets;regression analysis;time series","conventional regression-based modeling;ARIMA;BiLSTM-based modeling;regular unidirectional LSTM;LSTM architecture;autoregressive integrated moving average;long short-term memory;artificial recurrent neural networks;deep learning-based algorithms;time series forecasting","","263","","27","IEEE","24 Feb 2020","","","IEEE","IEEE Conferences"
"Can Machine/Deep Learning Classifiers Detect Zero-Day Malware with High Accuracy?","F. Abri; S. Siami-Namini; M. A. Khanghah; F. M. Soltani; A. S. Namin","Department of Computer Science, Texas Tech University, USA; Department of Mathematics and Statistics, Texas Tech University University, USA; Department of Computer Science, University of Debrecen, Hungary; Department of Computer Science, University of Debrecen, Hungary; Department of Computer Science, Texas Tech University, USA","2019 IEEE International Conference on Big Data (Big Data)","24 Feb 2020","2019","","","3252","3259","The detection of zero-day attacks and vulnerabilities is a challenging problem. It is of utmost importance for network administrators to identify them with high accuracy. The higher the accuracy is, the more robust the defense mechanism will be. In an ideal scenario (i.e., 100% accuracy) the system can detect zero-day malware without being concerned about mistakenly tagging benign files as malware or enabling disruptive malicious code running as none-malicious ones. This paper investigates different machine learning algorithms to find out how well they can detect zero-day malware. Through the examination of 34 machine/deep learning classifiers, we found that the random forest classifier offered the best accuracy. The paper poses several research questions regarding the performance of machine and deep learning algorithms when detecting zero-day malware with zero rates for false positive and false negative.","","978-1-7281-0858-2","10.1109/BigData47090.2019.9006514","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9006514","zero-day vulnerability;machine learning","Malware;Machine learning;Machine learning algorithms;Security;Buildings;Support vector machines;Predictive models","invasive software;learning (artificial intelligence);neural nets;pattern classification","random forest classifier;deep learning algorithms;zero-day malware;zero-day attacks;machine learning classifier;deep learning classifier","","22","","21","IEEE","24 Feb 2020","","","IEEE","IEEE Conferences"
"MTSAD: Multivariate Time Series Abnormality Detection and Visualization","V. Pham; N. Nguyen; J. Li; J. Hass; Y. Chen; T. Dang","Computer Science Department, Texas Tech University, Lubbock, TX, USA; Computer Science Department, Texas Tech University, Lubbock, TX, USA; Computer Science Department, Texas Tech University, Lubbock, TX, USA; Distributed Management Task Force Dell, Inc, Austin, TX, USA; Computer Science Department, Texas Tech University, Lubbock, TX, USA; Computer Science Department, Texas Tech University, Lubbock, TX, USA","2019 IEEE International Conference on Big Data (Big Data)","24 Feb 2020","2019","","","3267","3276","Detecting outliers is one of the fundamental tasks in visual analytics and valuable in many application domains, such as suspicious network cyberattack recognition. This paper introduces an approach to analyzing and visualizing high-dimensional time series, focusing on identifying multivariate observations that are significantly different from the others. We also propose a prototype, called MTSAD, to guide users when interactively exploring abnormalities in large time series. The prototype contains two views: the main window provides an overview of identified outliers overtime, the detail window investigates and explores the ranked temporal data entries based on their outlying contributions to the overall plots. The visual interface supports a full range of interactions, such as lensing, brushing and linking, ranking, and filtering. To validate the benefits and usefulness of our approach, we demonstrate MTSAD on real-world datasets of different numbers of attributes.","","978-1-7281-0858-2","10.1109/BigData47090.2019.9006559","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9006559","Multivariate Time Series Visualization;Outlier vs. Inlier;Abnormality Detection;Box Plot Rule;Radar Charts;Parallel Coordinates.","Data visualization;Time series analysis;Radar;Monitoring;Computer science;Task analysis;Visualization","approximation theory;data analysis;data visualisation;security of data;time series;user interfaces","time series abnormality detection;detecting outliers;visual analytics;application domains;suspicious network cyberattack recognition;visualizing high-dimensional time series;multivariate observations;detail window investigates;ranked temporal data entries;visual interface;MTSAD","","8","","40","IEEE","24 Feb 2020","","","IEEE","IEEE Conferences"
"Exploring Metadata Search Essentials for Scientific Data Management","W. Zhang; S. Byna; C. Niu; Y. Chen","Department of Computer Science, Texas Tech University; Computational Research Division, Lawrence Berkeley National Laboratory; Department of Computer Science, Texas Tech University; Department of Computer Science, Texas Tech University","2019 IEEE 26th International Conference on High Performance Computing, Data, and Analytics (HiPC)","13 Feb 2020","2019","","","83","92","Scientific experiments and observations store massive amounts of data in various scientific file formats. Metadata, which describes the characteristics of the data, is commonly used to sift through massive datasets in order to locate data of interest to scientists. Several indexing data structures (such as hash tables, trie, self-balancing search trees, sparse array, etc.) have been developed as part of efforts to provide an efficient method for locating target data. However, efficient determination of an indexing data structure remains unclear in the context of scientific data management, due to the lack of investigation on metadata, metadata queries, and corresponding data structures. In this study, we perform a systematic study of the metadata search essentials in the context of scientific data management. We study a real-world astronomy observation dataset and explore the characteristics of the metadata in the dataset. We also study possible metadata queries based on the discovery of the metadata characteristics and evaluate different data structures for various types of metadata attributes. Our evaluation on real-world dataset suggests that trie is a suitable data structure when prefix/suffix query is required, otherwise hash table should be used. We conclude our study with a summary of our findings. These findings provide a guideline and offers insights in developing metadata indexing methodologies for scientific applications.","2640-0316","978-1-7281-4535-8","10.1109/HiPC.2019.00021","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8990452","Metadata Indexing, Metadata Search, Data Management, HDF5","Metadata;Data structures;Indexing;Search problems;Complexity theory","astronomy computing;meta data;query processing;tree data structures","scientific data management;scientific file formats;indexing data structure;real-world astronomy observation dataset;metadata queries;metadata characteristics;metadata attributes;data structure;metadata indexing methodologies","","7","","37","IEEE","13 Feb 2020","","","IEEE","IEEE Conferences"
"EQSA: Earthquake Situational Analytics from Social Media","H. N. Nguyen; T. Dang","Department of Computer Science, Texas Tech University; Department of Computer Science, Texas Tech University","2019 IEEE Conference on Visual Analytics Science and Technology (VAST)","10 Feb 2020","2019","","","142","143","This paper introduces EQSA, an interactive exploratory tool for earthquake situational analytics using social media. EQSA is designed to support users to characterize the condition across the area around the earthquake zone, regarding related events, resources to be allocated, and responses from the community. On the general level, changes in the volume of messages from chosen categories are presented, assisting users in conveying a general idea of the condition. More in-depth analysis is provided with topic evolution, community visualization, and location representation. EQSA is developed with intuitive, interactive features and multiple linked views, visualizing social media data, and supporting users to gain a comprehensive insight into the situation. In this paper, we present the application of EQSA with the VAST Challenge 2019: Mini-Challenge 3 (MC3) dataset.","","978-1-7281-2284-7","10.1109/VAST47406.2019.8986947","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8986947","Earthquake Analysis;Social Media Data;Interactive Visualization;Visual Analytics;EQSA;WordStream;VAST Challenge 2019;Mini-Challenge 3","","data analysis;data visualisation;earthquakes;emergency management;geophysics computing;social networking (online)","EQSA;earthquake situational analytics;interactive exploratory tool;earthquake zone;community visualization;intuitive features;interactive features;social media data","","9","","2","IEEE","10 Feb 2020","","","IEEE","IEEE Conferences"
"Outliagnostics: Visualizing Temporal Discrepancy in Outlying Signatures of Data Entries","V. Pham; T. Dang","Computer Science Department, Texas Tech University; Computer Science Department, Texas Tech University","2019 IEEE Visualization in Data Science (VDS)","30 Jan 2020","2019","","","29","37","This paper presents an approach to analyzing two-dimensional temporal datasets focusing on identifying observations that are significant in calculating the outliers of a scatterplot. We also propose a prototype, called Outliagnostics, to guide users when interactively exploring abnormalities in large time series. Instead of focusing on detecting outliers at each time point, we monitor and display the discrepant temporal signatures of each data entry concerning the overall distributions. Our prototype is designed to handle these tasks in parallel to improve performance. To highlight the benefits and performance of our approach, we illustrate and validate the use of Outliagnostics on real-world datasets of various sizes in different parallelism configurations. This work also discusses how to extend these ideas to handle time series with a higher number of dimensions and provides a prototype for this type of datasets.","","978-1-7281-5047-5","10.1109/VDS48975.2019.8973379","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8973379","","","data analysis;data visualisation;parallel processing;time series","data entry;time series;outlying signatures;discrepant temporal signatures;Outliagnostics;parallelism;temporal discrepancy visualization;two-dimensional temporal dataset analysis;outlier detection;parallel computing","","3","","42","IEEE","30 Jan 2020","","","IEEE","IEEE Conferences"
"CloudTraceViz: A Visualization Tool for Tracing Dynamic Usage of Cloud Computing Resources","V. T. Nguyen; T. Dang","Dept. of Computer Science, Texas Tech University, Lubbock, TX, USA; Dept. of Computer Science, Texas Tech University, Texas, United States","2019 IEEE/ACM Industry/University Joint International Workshop on Data-center Automation, Analytics, and Control (DAAC)","6 Jan 2020","2019","","","1","6","This paper introduces CloudTraceViz, a visual analytic tool for analyzing the characteristics of modern cloud data centers. The goals of this tool are: 1) to fulfill a set of visual tasks on cloud computing retrieved from in-depth interviews with domain experts, 2) to visualize and monitor large real-world data in terms of both the number of profiles and number of time steps, and 3) to aid system administrator to trace and understand the causal relationship of multivariate data. To reach these goals, our system composes several interconnected visual components. The customized heatmap is used to capture the pattern of a single machine as well as a group of machines, the progressive rendering of parallel coordinate allows users to see the dynamic behavior of running task/job over time, the scatterplot matrices are used in conjunction with the parallel graph for anomaly extraction. The results on the Alibaba Cloud Trace dataset show that the visualization tools offer great support for users to have a high-level overview of a large dataset as well as understand the causal relations within multivariate data.","","978-1-7281-5991-1","10.1109/DAAC49578.2019.00006","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8948826","Continuous heatmap;Multivariate analysis;Cloud computing;Scatterplot matrix;Parallel Coordinates","","cloud computing;computer centres;data analysis;data visualisation;graph theory","CloudTraceViz;visualization tool;cloud computing resources;visual analytic tool;modern cloud data centers;visual tasks;in-depth interviews;domain experts;real-world data;time steps;system administrator;multivariate data;interconnected visual components;dynamic behavior;dynamic usage tracing;Alibaba Cloud Trace dataset;anomaly extraction","","","","34","IEEE","6 Jan 2020","","","IEEE","IEEE Conferences"
"HiperJobViz: Visualizing Resource Allocations in High-Performance Computing Center via Multivariate Health-Status Data","N. Nguyen; T. Dang; J. Hass; Y. Chen","Department of Computer Science, Texas Tech University, Lubbock, TX, USA; Department of Computer Science, Texas Tech University, Lubbock, TX, USA; Dell, Inc., Austin, TX, USA; Department of Computer Science, Texas Tech University, Lubbock, TX, USA","2019 IEEE/ACM Industry/University Joint International Workshop on Data-center Automation, Analytics, and Control (DAAC)","6 Jan 2020","2019","","","19","24","Scheduling, visualizing, and balancing resource allocations in High-Performance Computing Centers are complicated tasks due to a large amount of data and the dynamic natures of the resource allocation problem. This paper introduces HiperJobViz, a visual analytic tool for visualizing the resource allocations of data centers for jobs, users, and usage statistics. The goals of this tool are: 1) to provide an overview of the current resource usages, 2) to track changes of resource usages by users, jobs, and hosts and 3) to provide a detailed view of the resource usage via multi-dimensional representation of health metrics, such as CPU temperatures, memory usage, and power consumption. To support these goals, our visual analytics tool provides a full range of interactive features, including details on demands, brushing and links, and filtering.","","978-1-7281-5991-1","10.1109/DAAC49578.2019.00009","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8948738","Multivariate analysis;Parallel Coordinates;Customizable Radar Charts;Job Scheduling;Health Metrics;Power Consumption;High-Performance Computing Centers","Radar;Processor scheduling;Measurement;Data visualization;Resource management;Monitoring;Layout","computer centres;data visualisation;resource allocation;scheduling","HiperJobViz;visual analytics tool;memory usage;resource usage;usage statistics;data centers;visual analytic tool;resource allocation problem;high-performance computing centers;multivariate health-status data","","2","","33","IEEE","6 Jan 2020","","","IEEE","IEEE Conferences"
"Prying into Private Spaces Using Mobile Device Motion Sensors","Z. Fyke; I. Griswold-Steiner; A. Serwadda","Department of Computer Science, Texas Tech University; Department of Computer Science, Texas Tech University; Department of Computer Science, Texas Tech University","2019 17th International Conference on Privacy, Security and Trust (PST)","6 Jan 2020","2019","","","1","10","Human made structures are designed in a predictable manner, conforming to the expectations of those who use them. These underlying patterns lend themselves to repetition in the way people get between different locations. We investigated the feasibility of an attacker using motion sensor data against their target, with the objective of predicting where they move and what activities they engaged in. In this work, we show that the gyroscope and accelerometer can be used to drive a privacy attack that stealthily maps out a user's private space with high accuracy. In particular, we show that a mobile app with access to this data can leverage it to analyze a user's step execution dynamics, turn operations, and general body movement activities and then methodically combine this information to map out paths and landmarks in protected spaces, such as houses. Using a dataset of 26 users who executed a number of activities and a combination of classification, regression, and distance matching techniques, we show this privacy attack to generate maps whose Normalized Hausdorff Distance from the ground-truth is as low as 0.1159.","2643-4202","978-1-7281-3265-5","10.1109/PST47121.2019.8949056","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8949056","","","accelerometers;data privacy;mobile computing","normalized Hausdorff distance;motion sensor data;mobile device motion sensors;private spaces;protected spaces;mobile app;stealthily maps;privacy attack;accelerometer;gyroscope","","1","","14","IEEE","6 Jan 2020","","","IEEE","IEEE Conferences"
"Extensive Examination of XOR Arbiter PUFs as Security Primitives for Resource-Constrained IoT Devices","K. T. Mursi; Y. Zhuang; M. S. Alkatheiri; A. O. Aseeri","Department of Computer Science, Texas Tech University, Lubbock, Texas, USA; Department of Computer Science, Texas Tech University, Lubbock, Texas, USA; College of Computer Science and Engineering, University of Jeddah, Jeddah, Saudi Arabia; Department of Computer Science, Prince Sattam bin Abdulaziz University, Saudi Arabia","2019 17th International Conference on Privacy, Security and Trust (PST)","6 Jan 2020","2019","","","1","9","Communication security is essential for the proper functioning of the Internet of Things. Traditional approaches that rely on cryptographic keys are vulnerable to side-channel attacks. Physical Unclonable Functions (PUFs), leveraging unavoidable and irreproducible variations of integrated circuits to produce responses unique for individual PUF devices, are emerging as promising candidates as security primitives to provide keyless solutions. Before a PUF can be adopted for real applications, the PUF must be thoroughly examined to understand its various properties for its application feasibility. In this paper, we study XOR PUFs for broad ranges of values for circuit architecture parameters. XOR PUFs have been extensively studied, and have been shown to be unable to withstand machine learning attacks for 64-bit XOR PUFs with less than ten component PUFs. Attack methods employed in existing studies need a large number of challenge-response pairs (CRPs), which are obtainable only if the PUF has an open access interface. When PUF-embedded devices equipped with mutual authentication or response obfuscating techniques, it is difficult for attackers to accumulate large numbers of CRPs. With only a small number of accumulated CRPs available to attackers, small size PUFs, like XOR PUFs with a small number of component PUFs and stages, may become resistant to machine learning attacks. Since smaller sizes mean less resource-demanding, it is worthwhile to examine such PUFs which have usually been considered unsafe against attacks. Such are thoughts that have been motivating us in this paper to explore the PUF performances for a wide range of values of the PUF architecture parameters.","2643-4202","978-1-7281-3265-5","10.1109/PST47121.2019.8949070","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8949070","XOR Arbiter PUF;Security;IoT;FPGA;Machine Learning attack","","cryptography;Internet of Things;learning (artificial intelligence)","XOR arbiter PUFs;security primitives;resource-constrained IoT devices;individual PUF devices;machine learning attacks;component PUFs;PUF-embedded devices;size PUFs;PUF performances;PUF architecture parameters;communication security;cryptographic keys;side-channel attacks;physical unclonable functions;integrated circuits;circuit architecture parameters;open access interface;challenge-response pairs;CRPs;attack methods;mutual authentication;response obfuscating techniques;word length 64 bit","","19","","30","IEEE","6 Jan 2020","","","IEEE","IEEE Conferences"
"Gearbox Fault Diagnostics Using Deep Learning with Simulated Data","O. Gecgel; S. Ekwaro-Osire; J. P. Dias; A. Serwadda; F. M. Alemayehu; A. Nispel","Department of Mechanical Engineering, Texas Tech University, Lubbock, USA; Department of Mechanical Engineering, Texas Tech University, Lubbock, USA; Department of Mechanical Engineering, Texas Tech University, Lubbock, USA; Department of Computer Science, Texas Tech University, Lubbock, USA; School of Engineering, West Texas A&M University, Canyon, USA; Department of Mechanical Engineering, Texas Tech University, Lubbock, USA","2019 IEEE International Conference on Prognostics and Health Management (ICPHM)","29 Aug 2019","2019","","","1","8","Transmission components are prone to fatigue damage due to high and intermittent loading cycles, that cause premature failure of gearboxes. Recently, several vibration-based diagnostics approaches using Machine Learning (ML) and Deep Learning (DL) algorithms have been proposed to identify gearboxes faults. However, most of them rely on a large amount of training data collection from physical experiments, which is often associated with high costs. This paper offers an ML and DL classification performance comparison of several algorithms to diagnose faults in a gearbox based on realistic simulated vibration data. A dynamic model of a single-stage gearbox was developed to generate data for different health conditions. Generated datasets were fed to ML and DL algorithms and accuracy results were compared. Results revealed the superiority of Convolutional Neural Network compared to other classifiers. This research contributes to the prevention of catastrophic failures in gearboxes by early crack detection and maintenance schedule optimization.","","978-1-5386-8357-6","10.1109/ICPHM.2019.8819423","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8819423","dynamic model;vibration-based diagnostics;deep learning;image encoding;tooth profile error","Vibrations;Classification algorithms;Gears;Time series analysis;Feature extraction;Mathematical model;Heuristic algorithms","condition monitoring;convolutional neural nets;crack detection;fault diagnosis;gears;learning (artificial intelligence);maintenance engineering;mechanical engineering computing;vibrations","data collection;classification performance comparison;single-stage gearbox;gearbox fault diagnostics;transmission components;fatigue damage;vibration-based diagnostics;machine learning;ML algorithm;deep learning algorithm;vibration data;gearboxes fault identification;convolutional neural network;DL algorithm","","14","","34","IEEE","29 Aug 2019","","","IEEE","IEEE Conferences"
"Twitter vs News: Concern Analysis of the 2018 California Wildfire Event","H. Du; L. Nguyen; Z. Yang; H. Abu-Gellban; X. Zhou; W. Xing; G. Cao; F. Jin","Department of Computer Science, Texas Tech University; Department of Computer Science, Texas Tech University; Department of Computer Science, Texas Tech University; Department of Computer Science, Texas Tech University; Department of Computer Science, Texas Tech University; College of Education, Texas Tech University; Department of Geosciences, Texas Tech University; Department of Computer Science, Texas Tech University","2019 IEEE 43rd Annual Computer Software and Applications Conference (COMPSAC)","9 Jul 2019","2019","2","","207","212","During disasters, discover people's concerns dynamically is crucial to disaster rescue and relief. In this paper, we propose a social media based framework to analyze people's concerns, to access the importance and to track the dynamic changes of these concerns. To better understand people's concerns across platforms and to monitor the dynamics, we make comparisons between Tweets and news on the mentioned aspects and disclosed some interesting findings. Specifically, we take 2018 Camp Fire, the most destructive wildfire on record in history of California as a case study. We find that despite their keen attentions towards the disaster, social media and news media focus on different aspects of the disaster, so are the contents and dynamic changes of their concerns.","0730-3157","978-1-7281-2607-4","10.1109/COMPSAC.2019.10208","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8754045","Concern Flow, Concern Analysis, Sentiment Analysis, Disaster Rescue, California Wildfire","Twitter;Media;Data mining;Resource management;Data models;Large scale integration","disasters;emergency management;social networking (online);wildfires","concern analysis;disaster rescue;social media based framework;dynamic changes;2018 Camp Fire;2018 California wildfire Event;Twitter;news","","16","","16","IEEE","9 Jul 2019","","","IEEE","IEEE Conferences"
"SpacePhaser: Phase Space Embedding Visual Analytics","T. Dang; N. Nguyen","Department of Computer Science, Texas Tech University, Lubbock, USA; Department of Computer Science, Texas Tech University, Lubbock, USA","2019 IEEE 43rd Annual Computer Software and Applications Conference (COMPSAC)","9 Jul 2019","2019","2","","239","244","In the paper, we focus on the statistical detection and understanding of signaling behavior via various media outlets that are associated with a violent hate crime. Following a body of theory that identifies the links between hate rhetoric with violence, we explore textual patterns of hate speech in traditional and non-traditional media outlets, including television news shows, social media, political blogs, and political forums, to detect an association with hate crime events per time and place. Applying the dynamical system theory, we can characterize the dynamic behaviors of essential keywords. Grouping keyword with similar temporal signatures enables us to highlight important information/topics corresponding to emerging events. We also introduce SpacePhaser, an interactive visual tool for textual pattern analysis. To show the usefulness of SpacePhaser, we demonstrate the results of its applications on various real-world datasets.","0730-3157","978-1-7281-2607-4","10.1109/COMPSAC.2019.10213","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8753913","Phase Space, Dynamical System Theory, Time Series Visualization, Visual Analytics, Dynamic Graphs, Social Media, Entity Recognition, Topic Modeling","Tag clouds;Space vehicles;Shape;Data visualization;Time-frequency analysis;Organizations;Visualization","data analysis;data visualisation;Internet;politics;social networking (online);text analysis","SpacePhaser;interactive visual tool;textual pattern analysis;statistical detection;violent hate crime;nontraditional media outlets;social media;political blogs;political forums;hate crime events;dynamical system theory;temporal signatures;phase space embedding visual analytics","","","","42","IEEE","9 Jul 2019","","","IEEE","IEEE Conferences"
"Wearables-Driven Freeform Handwriting Authentication","I. Griswold-Steiner; R. Matovu; A. Serwadda","Department of Computer Science, Texas Tech University, Lubbock, TX, USA; Department of Computer Science, Texas Tech University, Lubbock, TX, USA; Department of Computer Science, Texas Tech University, Lubbock, TX, USA","IEEE Transactions on Biometrics, Behavior, and Identity Science","6 Aug 2019","2019","1","3","152","164","With the ubiquity of handwriting in everyday tasks, it is surprising that existing avenues for handwriting authentication remain largely out of reach for the average individual or organization. Current solutions often rely on expensive or specialized equipment, and most existing research focuses on signatures rather than freeform handwriting. This limits the applicability of such technology to a narrow range of scenarios. In this paper, we argue that wearable devices might make handwriting authentication scalable and affordable. We design and evaluate two wearables-driven freeform handwriting authentication systems, one centered on a deep neural network and the other using human-engineered features. Our authentication systems are thoroughly tested across three writing experiments (involving 53 participants) that were carefully mapped to typical writing scenarios. We show the best performing configuration to attain an equal error rate of 5.51%, suggesting the potential of this modality for use in a multi-modal authentication system. To evaluate how our authentication systems perform against attacks by determined attackers, we developed and evaluated two impostor attacks that correspond to highly likely attack vectors. We then show that certain authentication system configurations are resistant to the attack. This paper represents an important step toward consumer ready wearables-driven freeform handwriting authentication.","2637-6407","","10.1109/TBIOM.2019.2912401","National Science Foundation(grant numbers:1527795); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8698222","Behavioral biometrics;wearables;handwriting;authentication;impersonation attacks","Authentication;Writing;Forgery;Biometrics (access control);Biomedical monitoring;Performance evaluation;Neural networks","biometrics (access control);feature extraction;handwriting recognition;message authentication;mobile computing;wearable computers","wearable devices;wearables-driven freeform handwriting authentication systems;typical writing scenarios;multimodal authentication system;authentication system configurations","","13","","35","IEEE","24 Apr 2019","","","IEEE","IEEE Journals"
"Managing Rich Metadata in High-Performance Computing Systems Using a Graph Model","D. Dai; Y. Chen; P. Carns; J. Jenkins; W. Zhang; R. Ross","Department of Computer Science, University of North Carolina at Charlotte, Charlotte, NC, USA; Department of Computer Science, Texas Tech University, Lubbock, TX, USA; Argonne National Laboratory, Lemont, IL, USA; Argonne National Laboratory, Lemont, IL, USA; Department of Computer Science, Texas Tech University, Lubbock, TX, USA; Argonne National Laboratory, Lemont, IL, USA","IEEE Transactions on Parallel and Distributed Systems","13 Jun 2019","2019","30","7","1613","1627","High-performance computing (HPC) systems generate huge amounts of metadata about different entities such as jobs, users, and files. Existing systems can efficiently record and manage part of these metadata, mainly the POSIX metadata of data files (e.g., file size, name, and permissions mode). But another important set of metadata, referred to as “rich” metadata in this study, which record not only wider range of entities (e.g., running processes and jobs) but also more complex relationships between them, are mostly missing in current HPC systems. Yet such rich metadata are critical for supporting many advanced data management functions such as identifying data sources and parameters behind a given result; auditing data usage; or understanding details about how inputs are transformed into outputs. To uniformly and efficiently manage the rich metadata generated in HPC systems, We propose to utilize a graph model in this study. We identify the key challenges of implementing such a graph-based HPC rich metadata management system and present GraphMeta, a graph-based rich metadata management system designed and optimized for HPC platforms, to tackle these challenges. Extensive evaluations on both synthetic and real HPC metadata workloads show its advantages in both performance and scalability compared with existing solutions.","1558-2183","","10.1109/TPDS.2018.2887380","U.S. Department of Energy, Office of Science(grant numbers:DE-AC02-06CH11357); National Science Foundation(grant numbers:CNS-1852815,CNS-1162488,CNS-1338078,IIP-1362134,CCF-1409946); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8580412","Data models;metadata;high performance computing;graph partitioning","Metadata;Computational modeling;Runtime;History;Task analysis;Computer science;Engines","graph theory;meta data;parallel processing","high-performance computing systems;graph model;POSIX metadata;advanced data management functions;graph-based HPC rich metadata management system;graph-based rich metadata management system;synthetic HPC metadata workloads;real HPC metadata workloads;GraphMeta","","3","","48","IEEE","18 Dec 2018","","","IEEE","IEEE Journals"
"Improving Nighttime Light Imagery With Location-Based Social Media Data","N. Zhao; W. Zhang; Y. Liu; E. L. Samson; Y. Chen; G. Cao","Department of Geosciences, Texas Tech University, Lubbock, TX, USA; Department of Computer Science, Texas Tech University, Lubbock, TX, USA; Department of Geosciences, Texas Tech University, Lubbock, TX, USA; Mayan Esteem Project, Farmington, CT, USA; Department of Computer Science, Texas Tech University, Lubbock, TX, USA; Department of Geosciences, Texas Tech University, Lubbock, TX, USA","IEEE Transactions on Geoscience and Remote Sensing","26 Mar 2019","2019","57","4","2161","2172","Location-based social media have been extensively utilized in the concept of “social sensing” to exploit dynamic information about human activities, yet joint uses of social sensing and remote sensing images are underdeveloped at present. In this paper, the close relationship between the number of Twitter users and brightness of nighttime lights (NTL) over the contiguous United States is calculated and geotagged tweets are then used to upsample a stable light image for 2013. An associated outcome of the upsampling process is the solution of two major problems existing in the NTL image, pixel saturation, and blooming effects. Compared with the original stable light image, digital number (DN) values of the upsampled stable light image have larger correlation coefficients with gridded population (0.47 versus 0.09) and DN values of the new generation NTL image product (0.56 versus 0.52), i.e., the Visible Infrared Imaging Radiometer Suite day/night band image composite. In addition, total personal incomes of states are disaggregated to each pixel in proportion to the DN value of the pixel in the NTL images and then aggregate by counties. Personal incomes distributed by the upsampled NTL image are closer to the official demographic data than those distributed by the original stable light image. All of these results explore the potential of geotagged tweets to improve the quality of NTL images for more accurately estimating or mapping socioeconomic factors.","1558-0644","","10.1109/TGRS.2018.2871788","Texas Tech University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8506604","Location-based social media;nighttime light (NTL) imagery;United States demographics;upsampling","Spatial resolution;Sociology;Statistics;Brightness;Twitter;Sensors","demography;geophysical image processing;infrared imaging;lighting;radiometry;remote sensing;social networking (online)","location-based social media data;remote sensing images;nighttime lights;contiguous United States;upsampling process;upsampled stable light image;generation NTL image product;Visible Infrared Imaging Radiometer Suite day/night band image composite;upsampled NTL image;nighttime light imagery;stable light image","","14","","74","IEEE","24 Oct 2018","","","IEEE","IEEE Journals"
"TimeMatrix: Visual Representation for Temporal Pattern Detection in Dynamic Networks, VAST 2018 Mini-Challenge 3","T. Dang; V. V. Pham","Computer Science Department, Texas Tech University; Computer Science Department, Texas Tech University","2018 IEEE Conference on Visual Analytics Science and Technology (VAST)","19 Aug 2019","2018","","","108","109","This work proposes a visual analytic technique for visualizing temporal pattern detection in networks. The force-directed layout is a popular way to highlight structures of a network however it does not allow to present the dynamic features (how communities change over time). We target this problem by forcing nodes to display vertically and using the horizontal space for representing the timeline of entities. To show the connection between nodes, the edges are drawn vertically to avoid edge-crossings. This work also presents the result of applying the solution to the VAST 2018 - Mini Challenge 3 dataset, which led to the Honorable Mention: Representation of Small-Scale Temporal Patterns for the challenge.","","978-1-5386-6861-0","10.1109/VAST.2018.8802457","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8802457","","Visual analytics;Layout;Computer science;Dynamics;Image color analysis;Bioinformatics","data analysis;data visualisation;graph theory","visual representation;temporal pattern detection;dynamic networks;visual analytic technique;force-directed layout;dynamic features;communities change;small-scale temporal patterns;VAST 2018 mini-challenge 3","","1","","5","IEEE","19 Aug 2019","","","IEEE","IEEE Conferences"
"MTDES: Multi-dimensional Temporal Data Exploration System","V. V. Pham; T. Dang","Computer Science Department, Texas Tech University; Computer Science Department, Texas Tech University","2018 IEEE Conference on Visual Analytics Science and Technology (VAST)","19 Aug 2019","2018","","","100","101","This work proposes a visual analytic solution which is well-designed to provide investigative functions with fluent interactions to analyze multi-dimensional temporal data. The solution allows users to view different dimensions of the data at different levels of details with a well-designed mixture of different visualizations and smooth interactions. At the general/overview level, various aggregation strategies are used to reduce data to be visualized, and different sorting procedures are used to cluster correlated data together to help discover patterns. Detail views are provided to explore and confirm/reject the identified patterns. Interaction and smooth transition between views are implemented to enable natural actions while performing analysis tasks. This work also presents the result of applying the solution to the VAST 2018 - Mini-Challenge (MC) 2 dataset, which led to the Strong Support for Exploratory Analysis award for the challenge.","","978-1-5386-6861-0","10.1109/VAST.2018.8802440","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8802440","Multi-dimensional;temporal data analysis—Heat-map—Line-graph—Interaction Design;VAST Challenge 2018—Mini-Challenge 2—Award:Strong Support for Exploratory Analysis—","Heating systems;Data visualization;Sorting;Image color analysis;Frequency measurement;Calcium;Magnesium","data aggregation;data analysis;data visualisation;sorting","exploratory analysis award;multidimensional temporal data exploration system;visual analytic solution;data clustering;VAST 2018 - Mini-Challenge 2 dataset;sorting procedures;aggregation strategies","","4","","4","IEEE","19 Aug 2019","","","IEEE","IEEE Conferences"
"Morph-a-Dope: Using Pupil Manipulation to Spoof Eye Movement Biometrics","I. Griswold-Steiner; Z. Fyke; M. Ahmed; A. Serwadda","Department of Computer Science, Texas Tech University, Lubbock, TX; Department of Computer Science, Texas Tech University, Lubbock, TX; Department of Computer Science, Texas Tech University, Lubbock, TX; Department of Computer Science, Texas Tech University, Lubbock, TX","2018 9th IEEE Annual Ubiquitous Computing, Electronics & Mobile Communication Conference (UEMCON)","15 Aug 2019","2018","","","543","552","Eye Tracking Authentication - a mechanism where eye movement patterns are used to verify a user's identity - is increasingly being explored for use as a layer of security in computing systems. Despite being widely studied, there is barely any research investigating how these systems could be attacked by a determined attacker. In particular, the relationship between pupil characteristics and lighting is one that could lead to vulnerabilities in improperly secured systems.This paper presents Morph-a-Dope, an attack that leverages lighting manipulations to defeat eye tracking authentication systems that heavily rely on features derived from pupil sizes. Across 20 attacker-victim pairs, the attack increased the EER by an average of over 50% as compared to the zero-effort attack by the overall population, and as much as 500% for individual victims. Our research calls for a greater emphasis on manipulation-resistant pupil size features or system designs that otherwise avoid such vulnerabilities.","","978-1-5386-7693-6","10.1109/UEMCON.2018.8796625","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8796625","eye tracking biometrics;behavioral biometrics;spoof attack;continuous authentication;machine learning","","authorisation;eye;feature extraction;gaze tracking;iris recognition;object tracking","zero-effort attack;manipulation-resistant pupil size features;system designs;Morph-a-Dope;pupil manipulation;spoof eye movement biometrics;eye movement patterns;computing systems;determined attacker;pupil characteristics;eye tracking authentication systems;attacker-victim pairs","","2","","24","IEEE","15 Aug 2019","","","IEEE","IEEE Conferences"
"Mitigating Suppression Attack in Multicast Protocol for Low Power and Lossy Networks","C. Pu; X. Zhou; S. Lim","Weisberg Division of Computer Science, Marshall University, Huntington, WV, USA; MS Graduate of Computer Science, Marshall University, Huntington, WV, USA; Department of Computer Science, Texas Tech University, Lubbock, TX, USA","2018 IEEE 43rd Conference on Local Computer Networks (LCN)","10 Feb 2019","2018","","","251","254","The following topics are dealt with: telecommunication traffic; telecommunication network routing; Internet; protocols; wireless sensor networks; transport protocols; Internet of Things; computer network security; resource allocation; learning (artificial intelligence).","0742-1303","978-1-5386-4413-3","10.1109/LCN.2018.8638238","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8638238","Denial-of-Service attack;low power and lossy networks;multicast protocol;suppression attack","Multicast communication;Multiprotocol label switching;Microsoft Windows;Wireless sensor networks;Multicast protocols;Wireless communication;Conferences","Internet;protocols;telecommunication network routing;telecommunication traffic;wireless sensor networks","telecommunication traffic;telecommunication network routing;Internet;protocols;wireless sensor networks;transport protocols;Internet of Things;computer network security;resource allocation;learning (artificial intelligence)","","7","","14","IEEE","10 Feb 2019","","","IEEE","IEEE Conferences"
"Predicting customer behaviors on energy consumption: Why past usage data are not enough?","S. Puangpontip; R. Hewett","Department of Computer Science, Texas Tech University, Lubbock, USA; Department of Computer Science, Texas Tech University, Lubbock, USA","2018 IEEE International Conference on Big Data (Big Data)","24 Jan 2019","2018","","","4577","4581","Smart Grid is an important cyber-physical infrastructure for power generation, transmission and distribution. As energy becomes a necessity for our modern living, effective management of Smart Grid can have great impact on power services. Recent smart meter technology has provided an opportunity that allows an economic model of utility to be more flexible and optimal. Construction of such a model requires better understanding of consumer behaviors in response to a given pricing. Most existing research uses mathematical models that tend to rely on assumptions that may not be realistically realizable. Majority of empirical work aims to predict or optimize utility loads. To the best of our knowledge, no empirical published work has addressed this issue. Our research aims to find an appropriate Big data analytic approach to learning consumption pattern of each household at appliance level. Particularly, this paper investigates how machine learning can be used to predict the change of customer's consumption habits for lower pricing. The problem is non-trivial due to conflicting dependent factors that are dynamically changing. Our preliminary results, obtained from three popular machine-learning models using synthesized data, show relatively high accuracy ranging from 61.5% to 99.4% for various appliances. Although the results are encouraging, we provide insights towards the validity and improvement of these predictive models.","","978-1-5386-5035-6","10.1109/BigData.2018.8622034","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8622034","demand response;cyber-physical system;smart grid;machine learning;big data analytics","Home appliances;Pricing;Mathematical model;Smart grids;Machine learning;Predictive models;Big Data","Big Data;consumer behaviour;data analysis;energy consumption;learning (artificial intelligence);power engineering computing;pricing;smart meters;smart power grids","customer behaviors;energy consumption;usage data;power generation;power services;economic model;consumer behaviors;mathematical models;utility loads;consumption pattern;machine learning;synthesized data;smart grid;cyber-physical infrastructure;power transmission;power distribution;smart meter technology;pricing;big data analytic approach","","2","","18","IEEE","24 Jan 2019","","","IEEE","IEEE Conferences"
"Unleashing the Power of Hashtags in Tweet Analytics with Distributed Framework on Apache Storm","V. Gupta; R. Hewett","Department of Computer Science, Texas Tech University, Lubbock, TX; Department of Computer Science, Texas Tech University, Lubbock, TX","2018 IEEE International Conference on Big Data (Big Data)","24 Jan 2019","2018","","","4554","4558","Twitter is a popular social network platform where users can interact and post texts of up to 280 characters called tweets. Hashtags, hyperlinked words in tweets, have increasingly become crucial for tweet retrieval and search. Using hashtags for tweet topic classification is a challenging problem because of context dependent among words, slangs, abbreviation and emoticons in a short tweet along with evolving use of hashtags. Since Twitter generates millions of tweets daily, tweet analytics is a fundamental problem of Big data stream that often requires a real-time Distributed processing. This paper proposes a distributed online approach to tweet topic classification with hashtags. Being implemented on Apache Storm, a distributed real time framework, our approach incrementally identifies and updates a set of strong predictors in the Naïve Bayes model for classifying each incoming tweet instance. Preliminary experiments show promising results with up to 97% accuracy and 37% increase in throughput on eight processors.","","978-1-5386-5035-6","10.1109/BigData.2018.8622302","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8622302","Twitter;Hashtags;Social Media;Big Data Stream;Ontology;Apache Storm","Twitter;Tagging;Storms;Big Data;Program processors;Real-time systems;Throughput","Bayes methods;classification;distributed processing;file organisation;indexing;real-time systems;social networking (online);text analysis","hashtags;tweet analytics;Apache Storm;hyperlinked words;tweet retrieval;tweet topic classification;social network platform;real-time distributed processing;Big Data;Naïve Bayes model;Twitter","","2","","23","IEEE","24 Jan 2019","","","IEEE","IEEE Conferences"
"FinanViz: Visualizing Emerging Topics in Financial News","N. V. T. Nguyen; V. T. Nguyen; V. Pham; T. Dang","Computer Science Department, Texas Tech University, Lubbock, TX, USA; Computer Science Department, Texas Tech University, Lubbock, TX, USA; Computer Science Department, Texas Tech University, Lubbock, TX, USA; Computer Science Department, Texas Tech University, Lubbock, TX, USA","2018 IEEE International Conference on Big Data (Big Data)","24 Jan 2019","2018","","","4698","4704","The explosion of social media has paved a way for big data in which entrepreneurs use this data to find out potential customers, market demands, individual behavior, thereby to improve existing products, to create new products according to users' need, or to analyze and evaluate financial risks. The challenges of the heterogeneity and fragmentation of data make it difficult for analysts to fully exploit the benefit of deluge information. Available statistical software lacks customization and address unknown research questions. This paper proposes FinanViz, a visual analytics tool for analyzing financial news on social media. The principal aim of FinanViz is to observe the dynamic behavior of terms/words over time along with their proximity to other terms/words. The tool provides an intuitive, interactive exploration of the financial topics and what events are emerging in which we would argue that it will give hints for financial marketers in the decision making process.","","978-1-5386-5035-6","10.1109/BigData.2018.8622097","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8622097","Financial news;social media;coordinated multiple views;dynamic network;community detection;word clouds","Social network services;Data visualization;Correlation;Tools;Big Data;Tag clouds;Visualization","data analysis;data mining;data visualisation;decision making;financial data processing;financial management;product design;risk management;social networking (online);statistical analysis;stock markets;text analysis","FinanViz;financial news;social media;big data;market demands;individual behavior;financial risks;heterogeneity;fragmentation;deluge information;customization;visual analytics tool;dynamic behavior;financial topics;financial marketers;statistical software","","4","","31","IEEE","24 Jan 2019","","","IEEE","IEEE Conferences"
"A Survey of Privacy Concerns in Wearable Devices","P. Datta; A. S. Namin; M. Chatterjee","Department of Computer Science, Texas Tech University; Department of Computer Science, Texas Tech University; Department of Computer Science, Texas Tech University","2018 IEEE International Conference on Big Data (Big Data)","24 Jan 2019","2018","","","4549","4553","With the continued improvement and innovation, technology has become an integral part of our daily lives. The rapid adoption of technology and its affordability has given rise to the Internet-of-Things (IoT). IoT is an interconnected network of devices that are able to communicate and share information seamlessly. IoT encompasses a gamut of heterogeneous devices ranging from a small sensor to large industrial machines. One such domain of IoT that has seen a significant growth in the recent few years is that of the wearable devices. While the privacy issues for medical devices has been well-researched and documented in the literature, the threats to privacy arising from the use of consumer wearable devices have received very little attention from the research community. This paper presents a survey of the literature to understand the various privacy challenges, mitigation strategies, and future research directions as a result of the widespread adoption of wearable devices.","","978-1-5386-5035-6","10.1109/BigData.2018.8622110","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8622110","privacy;wearable devices;IoT","Privacy;Biomedical monitoring;Data privacy;Employment;Computer science;Monitoring;Automobiles","data privacy;Internet of Things","Internet-of-Things;IoT;heterogeneous devices;privacy issues;medical devices;consumer wearable devices;privacy challenges;privacy concerns","","10","","40","IEEE","24 Jan 2019","","","IEEE","IEEE Conferences"
"A Multi-variable Stacked Long-Short Term Memory Network for Wind Speed Forecasting","S. Liang; L. Nguyen; F. Jin","Department of Computer Science, Texas Tech University, Lubbock, Texas, USA; Department of Computer Science, Texas Tech University, Lubbock, Texas, USA; Department of Computer Science, Texas Tech University, Lubbock, Texas, USA","2018 IEEE International Conference on Big Data (Big Data)","24 Jan 2019","2018","","","4561","4564","Precisely forecasting wind speed is essential for wind power producers and grid operators. However, this task is challenging due to the stochasticity of wind speed. To accurately predict short-term wind speed under uncertainties, this paper proposed a multi-variable stacked LSTMs model (MSLSTM). The proposed method utilizes multiple historical meteorological variables, such as wind speed, temperature, humidity, pressure, dew point and solar radiation to accurately predict wind speeds. The prediction performance is extensively assessed using real data collected in West Texas, USA. The experimental results show that the proposed MSLSTM can preferably capture and learn uncertainties while output competitive performance.","","978-1-5386-5035-6","10.1109/BigData.2018.8622332","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8622332","Deep learning;Wind speed prediction;LSTM;Stacked LSTMs","Wind speed;Predictive models;Forecasting;Wind forecasting;Wind power generation;Humidity","learning (artificial intelligence);load forecasting;power engineering computing;recurrent neural nets;wind power","wind speed forecasting;short-term wind speed;multiple historical meteorological variables;multivariable stacked LSTMs model;multivariable stacked long-short term memory network","","31","","13","IEEE","24 Jan 2019","","","IEEE","IEEE Conferences"
"CVExplorer: Multidimensional Visualization for Common Vulnerabilities and Exposures","V. Pham; T. Dang","Dept. of Computer Science, Texas Tech University, Lubbock, TX, USA; Dept. of Computer Science, Texas Tech University, Lubbock, TX, USA","2018 IEEE International Conference on Big Data (Big Data)","24 Jan 2019","2018","","","1296","1301","Cyber attacks cause great damage to our national security, ranging from individual internet user to biggest governmental/industrial organizations, such as Equifax (Data Breach 145.5 Million Accounts, reported in July 2017) or Uber (Data Breach 57 Million Records, reported in November 2017). The cyber assault has significantly increased in breadth and depth. This paper introduces CVExplorer, a novel interactive system for visualizing cybersecurity threats reported in the National Vulnerability Database. The proposed system aims to work as a reporting and alerting tool that can help enhance the security against cyber attacks can potentially reduce network vulnerabilities. The CVExplorer system containing multiple linked views allows users to visualize the relationships of various dimensions in the large number of vulnerability reports, such as types and levels of vulnerability, vendors, and products. The CVExplorer provides an intuitive interface and supports a range of interactive features, such as filtering and ordering by vulnerability severity ratings, allowing users to narrow down topics of interest quickly. To demonstrate the effectiveness of the proposed system, we demonstrate the CVExplorer on two case studies of Common Vulnerabilities and Exposures retrieved from the National Vulnerability Database.","","978-1-5386-5035-6","10.1109/BigData.2018.8622092","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8622092","Common Vulnerabilities and Exposures;Common Vulnerability Scoring System;High-Dimensional Visual Analytics;Parallel coordinates;Force directed layouts.","Data visualization;Computer security;Databases;Standards;Tools;Visualization","data visualisation;interactive systems;security of data","governmental organization;interactive system;cybersecurity threat visualization;vulnerability reports;multiple linked views;CVExplorer system;network vulnerabilities;alerting tool;cybersecurity threats;cyber assault;individual internet user;national security;cyber attacks;multidimensional visualization;National Vulnerability Database;vulnerability severity ratings;interactive features","","18","","20","IEEE","24 Jan 2019","","","IEEE","IEEE Conferences"
"Detect Hidden Road Hazards combining Multiple Social Media Data","F. Jin; H. Liu","Department of Computer Science, Texas Tech University; Department of Civil, Environmental, and Construction Engineering, Texas Tech University","2018 IEEE International Conference on Big Data (Big Data)","24 Jan 2019","2018","","","4559","4560","It is imperatively important to spot hidden road hazards which cause a high proportion of traffic incidents. Social media data is featured by its innumerable and up-to-date information, and provides a promising approach to road hazard spotting. However, this research area is not well studied yet. In this paper, we present our view of the important research issues, including challenges of mining spatio-temporal dataset, road hazards reasoning and etc.","","978-1-5386-5035-6","10.1109/BigData.2018.8622256","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8622256","","Roads;Hazards;Social network services;Accidents;Cognition;Vehicles;Data mining","data analysis;social networking (online)","research area;road hazard spotting;up-to-date information;innumerable date information;traffic incidents;multiple social media data;detect hidden road hazards","","","","7","IEEE","24 Jan 2019","","","IEEE","IEEE Conferences"
"Towards Prediction of Security Attacks on Software Defined Networks: A Big Data Analytic Approach","E. Unal; S. Sen-Baidya; R. Hewett","Department of Computer Science, Texas Tech University, Lubbock, TX, USA; Department of Computer Science, Texas Tech University, Lubbock, TX, USA; Department of Computer Science, Texas Tech University, Lubbock, TX, USA","2018 IEEE International Conference on Big Data (Big Data)","24 Jan 2019","2018","","","4582","4588","Cyber-physical systems (CPS) tightly integrate physical and computing processes by monitoring and control data interacting between them via underlying networks. Software Defined Network (SDN) Technology has increasingly become essential in many advanced computer networks, including those in modern CPS, to provide flexible and agile network development. Despite many benefits that SDN offers, malicious attacks that can eventually prevent network services are unavoidable. Among the most predominant attacks on SDN controller layer, Link Discovery Attack and ARP (Address Resolution Protocol) Spoofing Attack are fundamental in that they are the gateways of many other SDN threats and attacks. To defend these attacks, most existing techniques either rely on relatively complex data validation techniques or use thresholds that can be subjective and unable to detect more than one type of attacks at a time if one deciding factor is used. While Big data technology, particularly machine learning, has been widely used for intrusion/anomaly detection, little has been done in SDN. This paper explores how well this technology can be used to predict these SDN attacks. By employing typical machine learning algorithms on simulated data of routing in SDN when attacks occur, preliminary results, obtained from four machine learning models, show the average area under ROC curve of over 96% and 92% for sample size 50,970 (12 switches) and 60,000 (20 switches), respectively. Further experiments show near-linear scaling in training time for the best performing algorithm when sample size grows up to 100,000.","","978-1-5386-5035-6","10.1109/BigData.2018.8622524","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8622524","Software-Defined Networking;SDN-specific security;Link Discovery attack;ARP Spoofing attack;Machine Learning;Data Analytic Applications","Control systems;Protocols;Software defined networking;Security;Big Data;Machine learning;Computer architecture","Big Data;computer network security;cyber-physical systems;data analysis;IP networks;learning (artificial intelligence);software defined networking","security attacks;Big data analytic approach;cyber-physical systems;flexible network development;agile network development;malicious attacks;SDN controller layer;SDN attacks;software defined network technology;computer networks;CPS;ARP spoofing attack;link discovery attack;address resolution protocol spoofing attack;machine learning;intrusion-anomaly detection","","6","","23","IEEE","24 Jan 2019","","","IEEE","IEEE Conferences"
"Web-Based Virtual Reality Development in Classroom: From Learner's Perspectives","V. T. Nguyen; R. Hite; T. Dang","Computer Science Department, Texas Tech University, Lubbock, TX, USA; Department of Curriculum and Instruction, Texas Tech University, Lubbock, TX, USA; Computer Science Department, Texas Tech University, Lubbock, TX, USA","2018 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR)","17 Jan 2019","2018","","","11","18","Virtual Reality (VR) content development tools are in continuous production by both enthusiastic researchers and software development companies. Yet, learners could benefit from participating in this development, not only for learning vital programming skills, but also skills in creativity and collaboration. Web-based VR (WebVR) has emerged as a platform-independent framework that permits individuals (with little to no prior programming experience) to create immersive and interactive VR applications. Yet, the success of WebVR relies on students' technological acceptance, the intersectionality of perceived utility and ease of use. In order to determine the effectiveness of the emerging tool for learners of varied experience levels, this paper presents a case study of 38 students who were tasked with developing WebVR 'dream' houses. Results showed that students were accepting of the technology by not only learning and implementing WebVR in a short time (one month), but were also capable of demonstrating creativity and problem-solving skills with classroom supports (i.e., pre-project presentations, online discussions, exemplary projects, and TA support). Results as well as recommendations, lessons learned, and further research are addressed.","","978-1-5386-9269-1","10.1109/AIVR.2018.00010","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8613629","Web-based virtual reality, technology acceptance model, course design, student creativity, A-frame","Education;Programming profession;Solid modeling;Three-dimensional displays;Software;Virtual reality","computer aided instruction;computer science education;Internet;software engineering;technology acceptance model;virtual reality","student technological acceptance;WebVR dream houses;varied experience levels;perceived utility;interactive VR applications;immersive VR applications;prior programming experience;platform-independent framework;web-based VR;vital programming skills;software development companies;continuous production;Virtual Reality content development tools;web-based virtual reality development;classroom supports;problem-solving skills;creativity","","17","","24","IEEE","17 Jan 2019","","","IEEE","IEEE Conferences"
"A Comparison of ARIMA and LSTM in Forecasting Time Series","S. Siami-Namini; N. Tavakoli; A. Siami Namin","Department of Applied Economics, Texas Tech University; Department of Computer Science, Georgia Institute of Technology; Department of Computer Science, Texas Tech University","2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA)","17 Jan 2019","2018","","","1394","1401","Forecasting time series data is an important subject in economics, business, and finance. Traditionally, there are several techniques to effectively forecast the next lag of time series data such as univariate Autoregressive (AR), univariate Moving Average (MA), Simple Exponential Smoothing (SES), and more notably Autoregressive Integrated Moving Average (ARIMA) with its many variations. In particular, ARIMA model has demonstrated its outperformance in precision and accuracy of predicting the next lags of time series. With the recent advancement in computational power of computers and more importantly development of more advanced machine learning algorithms and approaches such as deep learning, new algorithms are developed to analyze and forecast time series data. The research question investigated in this article is that whether and how the newly developed deep learning-based algorithms for forecasting time series data, such as ""Long Short-Term Memory (LSTM)"", are superior to the traditional algorithms. The empirical studies conducted and reported in this article show that deep learning-based algorithms such as LSTM outperform traditional-based algorithms such as ARIMA model. More specifically, the average reduction in error rates obtained by LSTM was between 84 - 87 percent when compared to ARIMA indicating the superiority of LSTM to ARIMA. Furthermore, it was noticed that the number of training times, known as ""epoch"" in deep learning, had no effect on the performance of the trained forecast model and it exhibited a truly random behavior.","","978-1-5386-6805-4","10.1109/ICMLA.2018.00227","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8614252","Deep Learning, Long Short-Term Memory (LSTM), Autoregressive Integrated Moving Average (ARIMA), Forecasting, Time Series Data","Time series analysis;Forecasting;Predictive models;Autoregressive processes;Economics;Deep learning;Data models","autoregressive moving average processes;data analysis;forecasting theory;learning (artificial intelligence);recurrent neural nets;time series","LSTM;Autoregressive Integrated Moving Average;ARIMA model;data processing;long short-term memory;forecasting time series data analysis;deep learning-based algorithms;machine learning algorithms","","304","","22","IEEE","17 Jan 2019","","","IEEE","IEEE Conferences"
"Smart and Connected Water Resource Management Via Social Media and Community Engagement","L. H. Nguyen; R. Hewett; A. S. Namin; N. Alvarez; C. Bradatan; F. Jin","Department of Computer Science, Texas Tech University; Department of Computer Science, Texas Tech University; Department of Computer Science, Texas Tech University; Department of Computer Science, Texas Tech University; Department of Sociology, Anthropology, and Social Work, Texas Tech University; Department of Computer Science, Texas Tech University","2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)","25 Oct 2018","2018","","","613","616","Water is a critical natural resource that has significant impacts on human living and society. Growing population and energy consumption exacerbate the scarcity of water and our ability to manage this resource. This demonstration paper presents WaterScope, a smart and connected platform for water resource management, which integrates multiple data sources such as water level data, social media data, and water related articles. Furthermore, the tool enables forecasting underground water levels, identifying water concerns, sharing knowledge and expertise among stakeholders, and thus bringing new insights to our understanding and insights of the water supplies and resource management. The prototype engages water stakeholders who face problems of similar nature but deal with the problem in an ad-hoc and isolated manner. The interactive WaterScope platform targets creating an interconnected virtual community that aims to improve water supply resilience.","2473-991X","978-1-5386-6051-5","10.1109/ASONAM.2018.8508602","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8508602","","Water resources;Water conservation;Stakeholders;Forecasting;Time series analysis;Social network services;Predictive models","energy consumption;natural resources;social networking (online);water resources;water supply","water supply resilience;community engagement;critical natural resource;human living;smart platform;connected platform;water resource management;water level data;social media data;water related articles;underground water levels;water concerns;water stakeholders;energy consumption;data sources;WaterScope platform;virtual community","","5","","5","IEEE","25 Oct 2018","","","IEEE","IEEE Conferences"
"The FOL-Based Legal-GRL (FLG) Framework: Towards an Automated Goal Modeling Approach for Regulations","A. Rabinia; S. Ghanavati","Department of Computer Science, Texas Tech University, Lubbock, TX, USA; Department of Computer Science, Texas Tech University, Lubbock, TX, USA","2018 IEEE 8th International Model-Driven Requirements Engineering Workshop (MoDRE)","21 Oct 2018","2018","","","58","67","In recent years, several goal modeling approaches have been used and extended to capture the complexity of legal requirements and help modeling them in notations familiar to the requirements engineers and analysts. Legal-GRL, which is an extension of the Goal-oriented Requirements Language (GRL), is used for modeling and analyzing legal requirements. However, creating Legal-GRL models is still a manual process, which limits its effectiveness and scalability. In this paper, we propose a new goal modeling framework based on GRL to facilitate the automation of the legal requirements modeling process. Our FOL-based Legal-GRL (FLG) framework uses a legal ontology, which entails a modal theory and First-order Logic (FOL) approach, for the purpose of extraction, refinement, and representation of legal requirements. Our FLG framework consists of a database design and a set of methods for automating the modeling process. We evaluate our work by modeling several statements from HIPAA, PHIPA, the EU GDPR and EU-US Privacy Shield.","","978-1-5386-8406-1","10.1109/MoDRE.2018.00014","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8501495","Goal Modeling, Legal Requirements, First Order Logic, GRL","Law;Ontologies;Analytical models;Complexity theory;Computational modeling;Databases","data privacy;database management systems;formal logic;formal specification;formal verification;ontologies (artificial intelligence);systems analysis","FOL-based Legal-GRL framework;legal requirements modeling process;legal ontology;FLG framework;first-order logic approach;goal-oriented requirements language;database design;modeling process;EU-US Privacy Shield;EU GDPR;PHIPA;HIPAA","","3","","37","IEEE","21 Oct 2018","","","IEEE","IEEE Conferences"
"I/O Characteristics Discovery in Cloud Storage Systems","J. Zhou; D. Dai; Y. Mao; X. Chen; Y. Zhuang; Y. Chen","Department of Computer Science, Texas Tech University, USA; Department of Computer Science, Texas Tech University, USA; Department of Computer Science, Texas Tech University, USA; Department of Computer Science, Texas Tech University, USA; Department of Computer Science, Texas Tech University, USA; Department of Computer Science, Texas Tech University, USA","2018 IEEE 11th International Conference on Cloud Computing (CLOUD)","11 Sep 2018","2018","","","170","177","The data growth from many applications in clouds poses significant challenges to cloud storage systems. To deliver the best storage and I/O performance possible, it is often required to understand and leverage the I/O characteristics based on data accesses. A number of research studies have been carried out on this topic. However, most of them either utilize a limited number of data-access attributes, restricting the general applicability of the method for different applications, or heavily rely on the domain knowledge or expertise about applications' I/O behaviors to select the best representative features, introducing bias for certain workloads. To overcome these limitations, in this study, we present a new I/O characteristic discovery methodology. This method enables capturing data-access features as many as possible to eliminate human bias. It utilizes a machine-learning based strategy to derive the most important set of features automatically, and groups data objects with a clustering algorithm (DBSCAN) to reveal I/O characteristics discovered. These I/O characteristics revealed can direct I/O performance optimizations in numerous scenarios, such as in data prefeteching and data reorganization optimizations in cloud storage systems.","2159-6190","978-1-5386-7235-8","10.1109/CLOUD.2018.00029","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8457797","Cloud storage systems, file systems, I/O characteristics discovery","Correlation;Cloud computing;Clustering algorithms;Optimization;Semantics;Principal component analysis;Machine learning","cloud computing;data mining;learning (artificial intelligence);pattern clustering;storage management","data reorganization optimizations;cloud storage systems;characteristics discovery;I/O characteristics;data-access attributes;characteristic discovery methodology;data-access features;groups data objects;data prefeteching;I/O performance optimizations","","6","","28","IEEE","11 Sep 2018","","","IEEE","IEEE Conferences"
"AKIN: A Streaming Graph Partitioning Algorithm for Distributed Graph Storage Systems","W. Zhang; Y. Chen; D. Dai","Department of Computer Science, Texas Tech University, Lubbock, TX, USA; Department of Computer Science, Texas Tech University, Lubbock, TX, USA; Department of Computer Science, Texas Tech University, Lubbock, TX, USA","2018 18th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID)","16 Jul 2018","2018","","","183","192","Many graph-related applications face the challenge of managing excessive and ever-growing graph data in a distributed environment. Therefore, it is necessary to consider a graph partitioning algorithm to distribute graph data onto multiple machines as the data comes in. Balancing data distribution and minimizing edge-cut ratio are two basic pursuits of the graph partitioning problem. While achieving balanced partitions for streaming graphs is easy, existing graph partitioning algorithms either fail to work on streaming workloads, or leave edge-cut ratio to be further improved. Our research aims to provide a better solution that fits the need of streaming graph partitioning in a distributed system, which further reduces the edge-cut ratio while maintaining rough balance among all partitions. We exploit the similarity measure on the degree of vertices to gather structuralrelated vertices in the same partition as much as possible, this reduces the edge-cut ratio even further as compared to the state-of-the-art streaming graph partitioning algorithm - FENNEL. Our evaluation shows that our streaming graph partitioning algorithm is able to achieve better partitioning quality in terms of edge-cut ratio (up to 20% reduction as compared to FENNEL) while maintaining decent balance between all partitions, and such improvement applies to various real-life graphs.","","978-1-5386-5815-4","10.1109/CCGRID.2018.00033","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8411022","graph partitioning;graph database;distributed system;graph storage","Partitioning algorithms;Heuristic algorithms;Distributed databases;Measurement;Computer science;Electronic mail;Standards","data analysis;graph theory;storage management","distributed graph storage systems;graph-related applications;graph data;distributed environment;balancing data distribution;graph partitioning problem;balanced partitions;distributed system;partitioning quality;edge-cut ratio minimization;streaming graph partitioning algorithm;FENNEL;AKIN","","13","","29","IEEE","16 Jul 2018","","","IEEE","IEEE Conferences"
"Detecting Web Spams Using Evidence Theory","M. Chatterjee; A. S. Namin","Department of Computer Science, Texas Tech University, Lubbock, TX, USA; Department of Computer Science, Texas Tech University, Lubbock, TX, USA","2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC)","22 Jun 2018","2018","02","","695","700","Search engines are the major instruments on the Web. The determination of the liability of the results returned by a typical search engine is a daunting challenge mainly due to the presence of Web spams. New types of Web spams are continuously introduced every now and then, which makes it drastically challenging to decide about the accuracy of the results. The problem looks like a reasoning problem in the presence of uncertainty. This paper presents a methodology for predicting Web spam where the spamicity of hosts is formulated as a reasoning problem. The approach is based on evidence theory, a mathematical prediction model based on Dempster-Shafer Theory (DST). The key benefit of our approach for Web spam is DST's ability to deal with the uncertainty. When a new spam is introduced in the system, the system lacks a reasonable prior knowledge. This is where DST provides more liable solution to detect spams without any prior information. The paper presents detailed statistical evaluations of the proposed approach where an accuracy of 99.27% in detecting Web spams is reported.","0730-3157","978-1-5386-2667-2","10.1109/COMPSAC.2018.10321","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8377949","Dempster-Shafer Theory;Basic Probability Assignment;Belief;Plausibility;Web Spam","Uncertainty;Cognition;Mathematical model;Magnetic heads;Search engines;Classification algorithms;Computer science","Bayes methods;inference mechanisms;Internet;search engines;uncertainty handling;unsolicited e-mail","evidence theory;search engines;reasoning problem;Dempster-Shafer Theory;Web spams;mathematical prediction model;DST ability","","4","","14","IEEE","22 Jun 2018","","","IEEE","IEEE Conferences"
"Reducing Network Vulnerability to Malicious Attacks","M. Rayatidamavandi; F. Conlon; M. Rahnamay-Naeini","Computer Science Department, Texas Tech University, Lubbock, TX, USA; Computer Science Department, Texas Tech University, Lubbock, TX, USA; Electrical Engineering Department, University of South Florida, Tampa, FL, USA","2018 International Conference on Computing, Networking and Communications (ICNC)","21 Jun 2018","2018","","","718","723","The underlying networks of many physical systems, including critical infrastructures, exhibit properties of complex networks such as scale-free properties. It is well-known that scale-free networks can tolerate random failures but show high vulnerabilities to targeted attacks, particularly compared to random networks of the same scale. In recent years many efforts have been made to reduce the vulnerability of such networks to targeted attacks by adding and/or rewiring links. However, to the best of our knowledge, the existing methods do not take attack patterns into consideration when determining how to improve or evaluate the resilience of a network. In this paper, we define an attack-dependent measure of vulnerability for networks and show that the problem of minimizing network vulnerability by adding a limited number of links while considering a potential attacks is NP-hard. Since the problem is intractable, we suggest a heuristic algorithm to reduces the vulnerability of a network with respect to specific attacks in polynomial time. By experimental results, we show that our approach outperforms the existing methods of network resiliency enhancement.","","978-1-5386-3652-7","10.1109/ICCNC.2018.8390313","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8390313","","Robustness;Resilience;Performance evaluation;Critical infrastructure;Complex networks;Vehicle-to-grid;Electronic mail","security of data","targeted attacks;attack patterns;attack-dependent measure;network vulnerability;network resiliency enhancement;malicious attacks;complex networks;scale-free properties;scale-free networks","","1","","12","IEEE","21 Jun 2018","","","IEEE","IEEE Conferences"
"IEEE Access Special Section Editorial: Artificial Intelligence in Parallel and Distributed Computing","S. Pei; J. Wu; T. Li; Y. Chen; S. Zuckerman","Department of Computer Science and Engineering, University of Shanghai for Science and Technology, Shanghai, China; College of Computer, National University of Defense Technology, Changsha, China; College of Computer Science, Nankai University, Tianjin, China; Department of Computer Science, Texas Tech University, Lubbock, TX, USA; University of Cergy-Pontoise, Cergy, France","IEEE Access","18 Jan 2021","2021","9","","9535","9538","Traditional computation is driven by parallel accelerators or distributed computation nodes in order to improve computing performance, save energy, and decrease delays in accessing memory. Recently, artificial intelligent algorithms, frameworks, and computing models are growing to help with high computational performance.","2169-3536","","10.1109/ACCESS.2020.3048598","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9328372","","","","","","","","0","CCBY","18 Jan 2021","","","IEEE","IEEE Journals"
"The IEEE ICDM 2020 Workshops","G. Di Fatta; V. Sheng; A. Cuzzocrea","Department of Computer Science, University of Reading, Reading, United Kingdom; Department of Computer Science, Texas Tech University, Lubbock, Texas, USA; iDEA Lab University of Calabria, Cosenza, Italy","2020 International Conference on Data Mining Workshops (ICDMW)","16 Feb 2021","2020","","","26","29","The 20th IEEE International Conference on Data Mining (ICDM) hosts many co-located workshops, whose papers are traditionally published in a dedicated IEEE CS press proceedings. The purpose of these workshops is to give exposure to the current trends in data mining research, which do not find space or sufficient attention in the main conference tracks either because of the specialised application domain or because of the emerging nature of the field. The aim is to disseminate advances in these emerging fields and to attract more attention and research effort from the community. The quality of the papers and of the final program is guaranteed by an initial selection process of the workshop proposals and by a rigorous peer-review process within each workshop. This volume contains all the papers accepted for publication in the ICDM 2020 workshops and represents an interesting snapshot of data mining methods and applications of emerging and innovative areas of interest. This editorial provides an overview of the workshops included in the final program of ICDM 2020.","2375-9259","978-1-7281-9012-9","10.1109/ICDMW51313.2020.00009","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9346471","Data-Mining","","","","","1","","0","IEEE","16 Feb 2021","","","IEEE","IEEE Conferences"
